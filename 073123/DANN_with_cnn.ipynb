{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data_util\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)\n",
    "device = torch.device('cuda:0' if CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = pd.read_csv('elec_merge.csv')\n",
    "nelec = pd.read_csv('n_elec_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_nelec = MinMaxScaler()\n",
    "scaler_elec = MinMaxScaler()\n",
    "\n",
    "scaler_nelec.fit(nelec)\n",
    "scaler_elec.fit(elec)\n",
    "\n",
    "scaled_nelec = scaler_nelec.transform(nelec)\n",
    "scaled_elec = scaler_elec.transform(elec)\n",
    "\n",
    "new_nelec = pd.DataFrame(scaled_nelec, index=nelec.index, columns=nelec.columns)\n",
    "new_elec = pd.DataFrame(scaled_elec, index=elec.index, columns=elec.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nelec = new_nelec.iloc[:-1,].reset_index(drop=True)\n",
    "output_nelec = new_nelec[['n_elec']].iloc[1:].reset_index(drop=True)\n",
    "output_nelec.columns = ['nelec']\n",
    "\n",
    "input_elec = new_elec.iloc[:-1,].reset_index(drop=True)\n",
    "output_elec = new_elec[['ELEC']].iloc[1:].reset_index(drop=True)\n",
    "output_elec.columns = ['elec']\n",
    "\n",
    "nelec = pd.concat([input_nelec, output_nelec], axis=1)\n",
    "elec = pd.concat([input_elec, output_elec], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_nelec = int(len(nelec) * 0.7)\n",
    "standard_elec = int(len(elec) * 0.7)\n",
    "\n",
    "nelec_train = nelec.iloc[:standard_nelec]\n",
    "nelec_test = nelec.iloc[standard_nelec:].reset_index(drop=True)\n",
    "\n",
    "elec_train = elec.iloc[:standard_elec]\n",
    "elec_test = elec.iloc[standard_elec:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_trainx = nelec_train.drop(['nelec'], axis=1)\n",
    "nelec_trainy = nelec_train[['nelec']]\n",
    "\n",
    "nelec_testx = nelec_test.drop(['nelec'], axis=1)\n",
    "nelec_testy = nelec_test[['nelec']]\n",
    "\n",
    "elec_trainx = elec_train.drop(['elec'], axis=1)\n",
    "elec_trainy = elec_train[['elec']]\n",
    "\n",
    "elec_testx = elec_test.drop(['elec'], axis=1)\n",
    "elec_testy = elec_test[['elec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)\n",
    "\n",
    "nelec_trainx, nelec_trainy = buildDataSet(nelec_trainx, nelec_trainy, 3)\n",
    "nelec_testx, nelec_testy = buildDataSet(nelec_testx, nelec_testy, 3)\n",
    "\n",
    "elec_trainx, elec_trainy = buildDataSet(elec_trainx, elec_trainy, 3)\n",
    "elec_testx, elec_testy = buildDataSet(elec_testx, elec_testy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "nelec_trainx_tensor = torch.FloatTensor(nelec_trainx)\n",
    "nelec_trainy_tensor = torch.FloatTensor(nelec_trainy)\n",
    "\n",
    "nelec_testx_tensor = torch.FloatTensor(nelec_testx)\n",
    "nelec_testy_tensor = torch.FloatTensor(nelec_testy)\n",
    "\n",
    "elec_trainx_tensor = torch.FloatTensor(elec_trainx)\n",
    "elec_trainy_tensor = torch.FloatTensor(elec_trainy)\n",
    "\n",
    "elec_testx_tensor = torch.FloatTensor(elec_testx)\n",
    "elec_testy_tensor = torch.FloatTensor(elec_testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 형태로 데이터 정의\n",
    "nelec_train = TensorDataset(nelec_trainx_tensor, nelec_trainy_tensor)\n",
    "nelec_test = TensorDataset(nelec_testx_tensor, nelec_testy_tensor)\n",
    "elec_train = TensorDataset(elec_trainx_tensor, elec_trainy_tensor)\n",
    "elec_test = TensorDataset(elec_testx_tensor, elec_testy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "source_train = DataLoader(nelec_train,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_test = DataLoader(nelec_test,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_train = DataLoader(elec_train,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_test = DataLoader(elec_test,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9352844e8cb247bdb265d9548e7608d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n",
      "torch.Size([64, 3, 13])\n"
     ]
    }
   ],
   "source": [
    "#for step, (source_data, target_data) in enumerate(zip(source_train, target_train)):\n",
    "for step in tqdm(range(len(source_train))):\n",
    "    # source_data = next(iter(source_train))\n",
    "    # target_data = next(iter(target_train))\n",
    "    \n",
    "    # 각 batch 내 데이터 : 0번은 이미지 픽셀 값, 1번은 0~9 class 라벨 값\n",
    "    source_x = source_data[0].to(device)\n",
    "    source_y = source_data[1].to(device)\n",
    "    \n",
    "    target_x = target_data[0].to(device)\n",
    "    target_y = target_data[1].to(device)\n",
    "\n",
    "    #print(source_x.shape) # 0: reg_logits torch.Size([64, 1])\n",
    "    #print(source_y.shape) # 1: cla_logits torch.Size([64, 10])\n",
    "    print(target_x.shape) # 0: reg_logits torch.Size([64, 1])\n",
    "    #print(target_y.shape) # 1: cla_logits torch.Size([64, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구조 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 128, kernel_size=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(128, 64, kernel_size=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=1, padding=0)\n",
    "        self.fc = nn.Linear(64, 64, bias=False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=11)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(-1,64)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "hoxy = cnn()\n",
    "cnnx = torch.randn(64,3,13)\n",
    "hoxy2 = hoxy(cnnx)\n",
    "print(hoxy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradientReversalLayer(torch.autograd.Function):\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "\n",
    "#     def backward(self, grad_output):\n",
    "#         return (grad_output * -1)\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return (grad_output * -1), None\n",
    "\n",
    "class GradientReversalLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_regression, self).__init__()\n",
    "        self.regression_layer1 = nn.Linear(64, 64)\n",
    "        self.regression_layer2 = nn.Linear(64, 32)\n",
    "        self.regression_layer3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.regression_layer1(x))\n",
    "        #x = F.relu(self.regression_layer2(x))\n",
    "        #x = self.regression_layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "hoxy3 = domain_regression()\n",
    "cnnx = torch.randn(64,64)\n",
    "hoxy4 = hoxy3(cnnx)\n",
    "print(hoxy4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_classfication(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classfication, self).__init__()\n",
    "        self.classification_layer0 = GradientReversalLayer()\n",
    "        self.classification_layer1 = nn.Linear(64, 64)\n",
    "        self.classification_layer2 = nn.Linear(64, 32)\n",
    "        self.classification_layer3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classification_layer0(x)\n",
    "        x = F.relu(self.classification_layer1(x))\n",
    "        x = F.relu(self.classification_layer2(x))\n",
    "        x = torch.sigmoid(self.classification_layer3(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "hoxy5 = domain_classfication()\n",
    "cnnx = torch.randn(64,64)\n",
    "hoxy6 = hoxy5(cnnx)\n",
    "print(hoxy6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann(nn.Module):\n",
    "    def __init__(self, cnn):\n",
    "        super(dann, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.regression = domain_regression()\n",
    "        self.classification = domain_classfication()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.cnn(x)\n",
    "        reg_output = self.regression(feature)\n",
    "        cla_output = self.classification(feature)\n",
    "        \n",
    "        return reg_output, cla_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "hoxy7 = dann(cnn)\n",
    "cnnx = torch.randn(64,64)\n",
    "hoxy6 = hoxy5(cnnx)\n",
    "print(hoxy6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dann_loss, self).__init__()\n",
    "        self.reg = nn.L1Loss() # 회귀 L1 손실 == MAE\n",
    "        self.cla = nn.BCEWithLogitsLoss() # 도메인 분류\n",
    "        \n",
    "    def forward(self, result, reg_real, domain_num, alpha=1):\n",
    "        reg_output, cla_output = result\n",
    "        #print(reg_output.shape)\n",
    "        #print(cla_output.shape)\n",
    "        batch_size = reg_output.shape[0]\n",
    "        #print(batch_size)\n",
    "        cla_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        \n",
    "        reg_loss = self.reg(reg_output, reg_real)\n",
    "        cla_loss = self.cla(cla_output, cla_target)\n",
    "        loss = reg_loss + cla_loss * alpha\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "dann                                     [1, 64]                   --\n",
       "├─cnn: 1-1                               [1, 64]                   --\n",
       "│    └─Conv1d: 2-1                       [128, 13]                 512\n",
       "│    └─MaxPool1d: 2-2                    [128, 12]                 --\n",
       "│    └─GELU: 2-3                         [128, 12]                 --\n",
       "│    └─Conv1d: 2-4                       [64, 12]                  8,256\n",
       "│    └─MaxPool1d: 2-5                    [64, 11]                  --\n",
       "│    └─GELU: 2-6                         [64, 11]                  --\n",
       "│    └─AvgPool1d: 2-7                    [64, 1]                   --\n",
       "│    └─Linear: 2-8                       [1, 64]                   4,096\n",
       "├─domain_regression: 1-2                 [1, 64]                   2,113\n",
       "│    └─Linear: 2-9                       [1, 64]                   4,160\n",
       "├─domain_classfication: 1-3              [1, 1]                    --\n",
       "│    └─GradientReversalLayer: 2-10       [1, 64]                   --\n",
       "│    └─Linear: 2-11                      [1, 64]                   4,160\n",
       "│    └─Linear: 2-12                      [1, 32]                   2,080\n",
       "│    └─Linear: 2-13                      [1, 1]                    33\n",
       "==========================================================================================\n",
       "Total params: 25,410\n",
       "Trainable params: 25,410\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.61\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.09\n",
       "Estimated Total Size (MB): 0.11\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, (3,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Avg Loss : 0.5073\n",
      "Epoch : 2, Avg Loss : 0.4906\n",
      "Epoch : 3, Avg Loss : 0.4869\n",
      "Epoch : 4, Avg Loss : 0.4844\n",
      "Epoch : 5, Avg Loss : 0.4813\n",
      "Epoch : 6, Avg Loss : 0.4782\n",
      "Epoch : 7, Avg Loss : 0.4761\n",
      "Epoch : 8, Avg Loss : 0.4749\n",
      "Epoch : 9, Avg Loss : 0.4739\n",
      "Epoch : 10, Avg Loss : 0.4733\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#batch = 64\n",
    "\n",
    "my_cnn = cnn()\n",
    "model = dann(my_cnn).to(device)\n",
    "loss_fn = dann_loss().to(device)\n",
    "\n",
    "epochs = 10\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=epochs * len(source_train))\n",
    "\n",
    "alpha = 2.0\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    for step, (source_data, target_data) in enumerate(zip(source_train, target_train)):\n",
    "        # mnist, svhn에서 1 batch씩 가져오기\n",
    "        # source_data = next(iter(source_train))\n",
    "        # target_data = next(iter(target_train))\n",
    "        \n",
    "        # 각 batch 내 데이터 : 0번은 이미지 픽셀 값, 1번은 0~9 class 라벨 값\n",
    "        mnist_data = source_data[0].to(device)\n",
    "        mnist_target = source_data[1].to(device)\n",
    "        \n",
    "        svhn_data = target_data[0].to(device)\n",
    "        svhn_target = target_data[1].to(device)\n",
    "        \n",
    "        # 순전파 결과 구하기\n",
    "        source_result = model(mnist_data)\n",
    "        target_result = model(svhn_data)\n",
    "        #print(source_result[0].shape) # 0: reg_logits torch.Size([64, 1])\n",
    "        #print(source_result[1].shape) # 1: cla_logits torch.Size([64, 10])\n",
    "        #print(target_result[0].shape) # 0: reg_logits torch.Size([64, 1])\n",
    "        #print(target_result[1].shape) # 1: cla_logits torch.Size([64, 10])\n",
    "        # 순전파 결과, class_label, domain_label, alpha 순\n",
    "        source_loss = loss_fn(source_result, mnist_target, 0 , alpha = alpha)\n",
    "        target_loss = loss_fn(target_result, svhn_target, 1, alpha = alpha)\n",
    "        \n",
    "        loss = source_loss + target_loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    print('Epoch : %d, Avg Loss : %.4f'%(i, total_loss / len(source_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS 분류 정확도 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model.eval()  # 테스트 모드로 전환\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "\n",
    "#for step in tqdm(range(len(source_train))):\n",
    "for step, (source_data, target_data) in enumerate(zip(source_train, target_train)):\n",
    "    # source_data = next(iter(source_train))\n",
    "    # target_data = next(iter(target_train))\n",
    "\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "    # domain 분류와 관련된 logits은 사용하지 않기에 _로 받아서 처리\n",
    "    _, source_pred = model(sourcex)\n",
    "    _, target_pred = model(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Embedding Space 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 앞 batch의 250개씩의 데이터만 샘플링\n",
    "source_tsne = DataLoader(nelec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_tsne = DataLoader(elec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_tsne2 = next(iter(source_tsne))\n",
    "target_tsne2 = next(iter(target_tsne))\n",
    "\n",
    "source_x_TNSE = source_tsne2[0].to(device)\n",
    "source_y_TNSE = source_tsne2[1].to(device)\n",
    "\n",
    "target_x_TNSE = target_tsne2[0].to(device)\n",
    "target_y_TNSE = target_tsne2[1].to(device)\n",
    "\n",
    "# 학습된 모델의 LSTM 부분만 활용 (100차원 임베딩 벡터를 받아오는 과정)\n",
    "source_vector = model.cnn(source_x_TNSE)\n",
    "target_vector = model.cnn(target_x_TNSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([source_vector.cpu().detach().numpy(), target_vector.cpu().detach().numpy()]))\n",
    "\n",
    "tsne_np = TSNE(n_components=2).fit_transform(df)\n",
    "tsne_df = pd.DataFrame(tsne_np, columns=['component 0', 'component 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_0 = tsne_df.loc[:1000]\n",
    "tsne_df_1 = tsne_df.loc[1000:]\n",
    "\n",
    "plt.scatter(tsne_df_0['component 0'], tsne_df_0['component 1'], color='blue', label='NELEC', alpha=0.5)\n",
    "plt.scatter(tsne_df_1['component 0'], tsne_df_1['component 1'], color='green', label='ELEC', alpha=0.5)\n",
    "\n",
    "plt.title('alpha = '+ str(alpha))\n",
    "plt.xlabel('component 0')\n",
    "plt.ylabel('component 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
