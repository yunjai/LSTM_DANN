{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data as data_util\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = pd.read_csv('elec_merge.csv')\n",
    "nelec = pd.read_csv('n_elec_merge.csv')\n",
    "dhw = pd.read_csv('dhw_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4274, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_nelec = MinMaxScaler()\n",
    "scaler_elec = MinMaxScaler()\n",
    "scaler_dhw = MinMaxScaler()\n",
    "\n",
    "scaler_nelec.fit(nelec)\n",
    "scaler_elec.fit(elec)\n",
    "scaler_dhw.fit(dhw)\n",
    "\n",
    "scaled_nelec = scaler_nelec.transform(nelec)\n",
    "scaled_elec = scaler_elec.transform(elec)\n",
    "scaled_dhw = scaler_dhw.transform(dhw)\n",
    "\n",
    "new_nelec = pd.DataFrame(scaled_nelec, index=nelec.index, columns=nelec.columns)\n",
    "new_elec = pd.DataFrame(scaled_elec, index=elec.index, columns=elec.columns)\n",
    "new_dhw = pd.DataFrame(scaled_dhw, index=dhw.index, columns=dhw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nelec = new_nelec.iloc[:-1,].reset_index(drop=True)\n",
    "output_nelec = new_nelec[['n_elec']].iloc[1:].reset_index(drop=True)\n",
    "output_nelec.columns = ['nelec']\n",
    "\n",
    "input_elec = new_elec.iloc[:-1,].reset_index(drop=True)\n",
    "output_elec = new_elec[['ELEC']].iloc[1:].reset_index(drop=True)\n",
    "output_elec.columns = ['elec']\n",
    "\n",
    "input_dhw = new_dhw.iloc[:-1,].reset_index(drop=True)\n",
    "output_dhw = new_dhw[['DHW']].iloc[1:].reset_index(drop=True)\n",
    "output_dhw.columns = ['dhw']\n",
    "\n",
    "nelec = pd.concat([input_nelec, output_nelec], axis=1)\n",
    "elec = pd.concat([input_elec, output_elec], axis=1)\n",
    "dhw = pd.concat([input_dhw, output_dhw], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_nelec = int(len(nelec) * 0.7)\n",
    "standard_elec = int(len(elec) * 0.7)\n",
    "standard_dhw = int(len(dhw) * 0.7)\n",
    "\n",
    "nelec_train = nelec.iloc[:standard_nelec]\n",
    "nelec_test = nelec.iloc[standard_nelec:].reset_index(drop=True)\n",
    "\n",
    "elec_train = elec.iloc[:standard_elec]\n",
    "elec_test = elec.iloc[standard_elec:].reset_index(drop=True)\n",
    "\n",
    "dhw_train = dhw.iloc[:standard_dhw]\n",
    "dhw_test = dhw.iloc[standard_dhw:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_trainx = nelec_train.drop(['nelec'], axis=1)\n",
    "nelec_trainy = nelec_train[['nelec']]\n",
    "\n",
    "nelec_testx = nelec_test.drop(['nelec'], axis=1)\n",
    "nelec_testy = nelec_test[['nelec']]\n",
    "\n",
    "elec_trainx = elec_train.drop(['elec'], axis=1)\n",
    "elec_trainy = elec_train[['elec']]\n",
    "\n",
    "elec_testx = elec_test.drop(['elec'], axis=1)\n",
    "elec_testy = elec_test[['elec']]\n",
    "\n",
    "dhw_trainx = dhw_train.drop(['dhw'], axis=1)\n",
    "dhw_trainy = dhw_train[['dhw']]\n",
    "\n",
    "dhw_testx = dhw_test.drop(['dhw'], axis=1)\n",
    "dhw_testy = dhw_test[['dhw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)\n",
    "\n",
    "nelec_trainx, nelec_trainy = buildDataSet(nelec_trainx, nelec_trainy, 3)\n",
    "nelec_testx, nelec_testy = buildDataSet(nelec_testx, nelec_testy, 3)\n",
    "\n",
    "elec_trainx, elec_trainy = buildDataSet(elec_trainx, elec_trainy, 3)\n",
    "elec_testx, elec_testy = buildDataSet(elec_testx, elec_testy, 3)\n",
    "\n",
    "dhw_trainx, dhw_trainy = buildDataSet(dhw_trainx, dhw_trainy, 3)\n",
    "dhw_testx, dhw_testy = buildDataSet(dhw_testx, dhw_testy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20413, 3, 13)\n",
      "(20413, 1)\n",
      "(8748, 3, 13)\n",
      "(8748, 1)\n",
      "(2988, 3, 13)\n",
      "(2988, 1)\n",
      "(1279, 3, 13)\n",
      "(1279, 1)\n"
     ]
    }
   ],
   "source": [
    "print(nelec_trainx.shape)\n",
    "print(nelec_trainy.shape)\n",
    "print(nelec_testx.shape)\n",
    "print(nelec_testy.shape)\n",
    "print(elec_trainx.shape)\n",
    "print(elec_trainy.shape)\n",
    "print(elec_testx.shape)\n",
    "print(elec_testy.shape)\n",
    "#print(dhw_trainx.shape)\n",
    "#print(dhw_trainy.shape)\n",
    "#print(dhw_testx.shape)\n",
    "#print(dhw_testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "nelec_trainx_tensor = torch.FloatTensor(nelec_trainx)\n",
    "nelec_trainy_tensor = torch.FloatTensor(nelec_trainy)\n",
    "\n",
    "nelec_testx_tensor = torch.FloatTensor(nelec_testx)\n",
    "nelec_testy_tensor = torch.FloatTensor(nelec_testy)\n",
    "\n",
    "elec_trainx_tensor = torch.FloatTensor(elec_trainx)\n",
    "elec_trainy_tensor = torch.FloatTensor(elec_trainy)\n",
    "\n",
    "elec_testx_tensor = torch.FloatTensor(elec_testx)\n",
    "elec_testy_tensor = torch.FloatTensor(elec_testy)\n",
    "\n",
    "dhw_trainx_tensor = torch.FloatTensor(dhw_trainx)\n",
    "dhw_trainy_tensor = torch.FloatTensor(dhw_trainy)\n",
    "\n",
    "dhw_testx_tensor = torch.FloatTensor(dhw_testx)\n",
    "dhw_testy_tensor = torch.FloatTensor(dhw_testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 형태로 데이터 정의\n",
    "nelec_train = TensorDataset(nelec_trainx_tensor, nelec_trainy_tensor)\n",
    "nelec_test = TensorDataset(nelec_testx_tensor, nelec_testy_tensor)\n",
    "elec_train = TensorDataset(elec_trainx_tensor, elec_trainy_tensor)\n",
    "elec_test = TensorDataset(elec_testx_tensor, elec_testy_tensor)\n",
    "dhw_train = TensorDataset(dhw_trainx_tensor, dhw_trainy_tensor)\n",
    "dhw_test = TensorDataset(dhw_testx_tensor, dhw_testy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_for_source_train = len(nelec_trainx)\n",
    "batch_for_source_test = 8748\n",
    "batch_for_target_train = 2988\n",
    "batch_for_target_test = 1279\n",
    "source_train = DataLoader(nelec_train,\n",
    "                        batch_size=batch_for_source_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_test = DataLoader(nelec_test,\n",
    "                        batch_size=batch_for_source_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_train = DataLoader(elec_train,\n",
    "                        batch_size=batch_for_target_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_test = DataLoader(elec_test,\n",
    "                        batch_size=batch_for_target_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20413\n"
     ]
    }
   ],
   "source": [
    "nelec_train_size = len(nelec_train)\n",
    "print(nelec_train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_train batches: 1\n",
      "source_test batches: 1\n",
      "target_train batches: 1\n",
      "target_test batches: 1\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 개수를 가정하여 변수에 저장\n",
    "nelec_train_size = len(nelec_train)\n",
    "nelec_test_size = len(nelec_test)\n",
    "elec_train_size = len(elec_train)\n",
    "elec_test_size = len(elec_test)\n",
    "\n",
    "# 각 DataLoader에서 생성되는 배치의 개수 계산\n",
    "source_train_batches = nelec_train_size // batch_for_source_train\n",
    "source_test_batches = nelec_test_size // batch_for_source_test\n",
    "target_train_batches = elec_train_size // batch_for_target_train\n",
    "target_test_batches = elec_test_size // batch_for_target_test\n",
    "\n",
    "# 결과 출력\n",
    "print(\"source_train batches:\", source_train_batches)\n",
    "print(\"source_test batches:\", source_test_batches)\n",
    "print(\"target_train batches:\", target_train_batches)\n",
    "print(\"target_test batches:\", target_test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientReversalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradientReversalLayer(torch.autograd.Function):\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "\n",
    "#     def backward(self, grad_output):\n",
    "#         return (grad_output * -1)\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return (grad_output * -1), None\n",
    "\n",
    "class GradientReversalLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(13, 64, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(3)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(64, 64, batch_first=True)\n",
    "        self.bn2 = nn.BatchNorm1d(3)\n",
    "        \n",
    "        self.fc = nn.Linear(64, 100, bias=False)\n",
    "        self.fc2 = nn.Linear(100, 64, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.fc2(x[:,-1,:])\n",
    "        x = self.dropout(x)\n",
    "        #x = x.view(-1, 100)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoxy = lstm()\n",
    "cnnx = torch.randn(512,3,13)\n",
    "hoxy2 = hoxy(cnnx)\n",
    "print(hoxy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_regression, self).__init__()\n",
    "        self.regression_layer1 = nn.Linear(64, 128)\n",
    "        self.regression_layer2 = nn.Linear(128, 128)\n",
    "        self.regression_layer3 = nn.Linear(128, 128)\n",
    "        self.regression_layer4 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.regression_layer1(x))\n",
    "        x = F.relu(self.regression_layer2(x))\n",
    "        x = F.relu(self.regression_layer3(x))\n",
    "        x = F.relu(self.regression_layer4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_classfication(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classfication, self).__init__()\n",
    "        self.classification_layer0 = GradientReversalLayer()\n",
    "        self.classification_layer1 = nn.Linear(64, 100)\n",
    "        self.classification_layer2 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classification_layer0(x)\n",
    "        x = F.relu(self.classification_layer1(x))\n",
    "        x = torch.sigmoid(self.classification_layer2(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super(dann, self).__init__()\n",
    "        self.lstm = lstm\n",
    "        self.regression = domain_regression()\n",
    "        self.classification = domain_classfication()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.lstm(x)\n",
    "        reg_output = self.regression(feature)\n",
    "        cla_output = self.classification(feature)\n",
    "        \n",
    "        return reg_output, cla_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dann_loss, self).__init__()\n",
    "        self.reg = nn.L1Loss() # 회귀 L1 손실 == MAE\n",
    "        self.cla = nn.BCEWithLogitsLoss() # 도메인 분류\n",
    "        \n",
    "    def forward(self, result, reg_real, domain_num, alpha=1):\n",
    "        reg_output, cla_output = result\n",
    "        batch_size = reg_output.shape[0]\n",
    "        cla_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        \n",
    "        reg_loss = self.reg(reg_output, reg_real)\n",
    "        cla_loss = self.cla(cla_output, cla_target)\n",
    "        loss = reg_loss + cla_loss * alpha\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "my_lstm = lstm().to(device)\n",
    "model = dann(my_lstm).to(device)\n",
    "loss_fn = dann_loss().to(device)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "model.train() # 훈련 모드 설정\n",
    "\n",
    "optimizer= torch.optim.Adamax(model.parameters(), lr = 0.001, weight_decay=0.01)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters = epochs * len(source_train))\n",
    "\n",
    "best_source_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    source_total_loss = 0\n",
    "    target_total_loss = 0\n",
    "\n",
    "    for step, (source_data, target_data) in enumerate(zip(source_train, target_train)):\n",
    "        source_x = source_data[0].to(device)\n",
    "        source_y = source_data[1].to(device)\n",
    "        target_x = target_data[0].to(device)\n",
    "        target_y = target_data[1].to(device)\n",
    "        #print(source_x.shape)\n",
    "        #print(source_y.shape)\n",
    "        #print(target_x.shape)\n",
    "        #print(target_y.shape)\n",
    "        # 순전파\n",
    "        source_result = model(source_x)\n",
    "        target_result = model(target_x)\n",
    "        #print(source_result[0].shape)\n",
    "        #print(target_result[0].shape)\n",
    "        #print(source_result[1].shape)\n",
    "        #print(target_result[1].shape)\n",
    "        # 순전파 loss\n",
    "        source_loss = loss_fn(source_result, source_y, 0, alpha=alpha) # 소스 도메인 레이블 0\n",
    "        target_loss = loss_fn(target_result, target_y, 1, alpha=alpha) # 타겟 도메인 레이블 1\n",
    "        loss = source_loss + target_loss\n",
    "        \n",
    "        source_total_loss += source_loss.item()\n",
    "        target_total_loss += target_loss.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 그래디언트 계산 및 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Epoch : %d, Total Avg Loss : %.4f' % (i, total_loss / len(source_train)))     \n",
    "    print('Source Avg Loss : %.4f' % (source_total_loss / len(source_train)))\n",
    "    print('Target Avg Loss : %.4f' % (target_total_loss / len(source_train)))\n",
    "    print('')\n",
    "    \n",
    "    # source_loss가 가장 낮은 경우 모델 가중치 저장\n",
    "    if source_total_loss / len(source_train) < best_source_loss:\n",
    "        best_source_loss = source_total_loss / len(source_train)\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "# 최상의 모델을 파일로 저장\n",
    "if best_model_state_dict is not None:\n",
    "    torch.save(best_model_state_dict, 'best_source_model6.pth')\n",
    "    #torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, (64,3,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('best_source_model6.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(source_train, target_train)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    _, source_pred = best_source_model2(sourcex)\n",
    "    _, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Embedding Space 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 앞 batch의 250개씩의 데이터만 샘플링\n",
    "source_tsne = DataLoader(nelec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_tsne = DataLoader(elec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_tsne2 = next(iter(source_tsne))\n",
    "target_tsne2 = next(iter(target_tsne))\n",
    "\n",
    "source_x_TNSE = source_tsne2[0].to(device)\n",
    "source_y_TNSE = source_tsne2[1].to(device)\n",
    "\n",
    "target_x_TNSE = target_tsne2[0].to(device)\n",
    "target_y_TNSE = target_tsne2[1].to(device)\n",
    "\n",
    "# 학습된 모델의 LSTM 부분만 활용 (100차원 임베딩 벡터를 받아오는 과정)\n",
    "source_vector = model.lstm(source_x_TNSE)\n",
    "target_vector = model.lstm(target_x_TNSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([source_vector.cpu().detach().numpy(), target_vector.cpu().detach().numpy()]))\n",
    "\n",
    "tsne_np = TSNE(n_components=2).fit_transform(df)\n",
    "tsne_df = pd.DataFrame(tsne_np, columns=['component 0', 'component 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_0 = tsne_df.loc[:1000]\n",
    "tsne_df_1 = tsne_df.loc[1000:]\n",
    "\n",
    "plt.scatter(tsne_df_0['component 0'], tsne_df_0['component 1'], color='blue', label='NELEC', alpha=0.5)\n",
    "plt.scatter(tsne_df_1['component 0'], tsne_df_1['component 1'], color='green', label='ELEC', alpha=0.5)\n",
    "\n",
    "plt.title('alpha = '+ str(alpha))\n",
    "plt.xlabel('component 0')\n",
    "plt.ylabel('component 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
