{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data as data_util\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = pd.read_csv('elec_merge.csv')\n",
    "nelec = pd.read_csv('n_elec_merge.csv')\n",
    "dhw = pd.read_csv('dhw_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = elec.drop(['YEAR'], axis=1)\n",
    "nelec = nelec.drop(['YEAR'], axis=1)\n",
    "dhw = dhw.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nelec = nelec.iloc[:-1,].reset_index(drop=True)\n",
    "output_nelec = nelec[['n_elec']].iloc[1:].reset_index(drop=True)\n",
    "output_nelec.columns = ['nelec']\n",
    "\n",
    "input_elec = elec.iloc[:-1,].reset_index(drop=True)\n",
    "output_elec = elec[['ELEC']].iloc[1:].reset_index(drop=True)\n",
    "output_elec.columns = ['elec']\n",
    "\n",
    "input_dhw = dhw.iloc[:-1,].reset_index(drop=True)\n",
    "output_dhw = dhw[['DHW']].iloc[1:].reset_index(drop=True)\n",
    "output_dhw.columns = ['dhw']\n",
    "\n",
    "nelec = pd.concat([input_nelec, output_nelec], axis=1)\n",
    "elec = pd.concat([input_elec, output_elec], axis=1)\n",
    "dhw = pd.concat([input_dhw, output_dhw], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_nelec = MinMaxScaler()\n",
    "scaler_elec = MinMaxScaler()\n",
    "scaler_dhw = MinMaxScaler()\n",
    "\n",
    "scaler_nelec.fit(nelec)\n",
    "scaler_elec.fit(elec)\n",
    "scaler_dhw.fit(dhw)\n",
    "\n",
    "scaled_nelec = scaler_nelec.transform(nelec)\n",
    "scaled_elec = scaler_elec.transform(elec)\n",
    "scaled_dhw = scaler_dhw.transform(dhw)\n",
    "\n",
    "nelec = pd.DataFrame(scaled_nelec, index=nelec.index, columns=nelec.columns)\n",
    "elec = pd.DataFrame(scaled_elec, index=elec.index, columns=elec.columns)\n",
    "dhw = pd.DataFrame(scaled_dhw, index=dhw.index, columns=dhw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_nelec = int(len(nelec) * 0.7)\n",
    "standard_elec = int(len(elec) * 0.7)\n",
    "standard_dhw = int(len(dhw) * 0.7)\n",
    "\n",
    "nelec_train = nelec.iloc[:standard_nelec]\n",
    "nelec_test = nelec.iloc[standard_nelec:].reset_index(drop=True)\n",
    "\n",
    "elec_train = elec.iloc[:standard_elec]\n",
    "elec_test = elec.iloc[standard_elec:].reset_index(drop=True)\n",
    "\n",
    "dhw_train = dhw.iloc[:standard_dhw]\n",
    "dhw_test = dhw.iloc[standard_dhw:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_trainx = nelec_train.drop(['nelec'], axis=1)\n",
    "nelec_trainy = nelec_train[['nelec']]\n",
    "\n",
    "nelec_testx = nelec_test.drop(['nelec'], axis=1)\n",
    "nelec_testy = nelec_test[['nelec']]\n",
    "\n",
    "elec_trainx = elec_train.drop(['elec'], axis=1)\n",
    "elec_trainy = elec_train[['elec']]\n",
    "\n",
    "elec_testx = elec_test.drop(['elec'], axis=1)\n",
    "elec_testy = elec_test[['elec']]\n",
    "\n",
    "dhw_trainx = dhw_train.drop(['dhw'], axis=1)\n",
    "dhw_trainy = dhw_train[['dhw']]\n",
    "\n",
    "dhw_testx = dhw_test.drop(['dhw'], axis=1)\n",
    "dhw_testy = dhw_test[['dhw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)\n",
    "\n",
    "nelec_trainx, nelec_trainy = buildDataSet(nelec_trainx, nelec_trainy, 3)\n",
    "nelec_testx, nelec_testy = buildDataSet(nelec_testx, nelec_testy, 3)\n",
    "\n",
    "elec_trainx, elec_trainy = buildDataSet(elec_trainx, elec_trainy, 3)\n",
    "elec_testx, elec_testy = buildDataSet(elec_testx, elec_testy, 3)\n",
    "\n",
    "dhw_trainx, dhw_trainy = buildDataSet(dhw_trainx, dhw_trainy, 3)\n",
    "dhw_testx, dhw_testy = buildDataSet(dhw_testx, dhw_testy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20413, 3, 12)\n",
      "(20413, 1)\n",
      "(8748, 3, 12)\n",
      "(8748, 1)\n",
      "(2988, 3, 12)\n",
      "(2988, 1)\n",
      "(1279, 3, 12)\n",
      "(1279, 1)\n",
      "(20413, 3, 12)\n",
      "(20413, 1)\n",
      "(8748, 3, 12)\n",
      "(8748, 1)\n"
     ]
    }
   ],
   "source": [
    "print(nelec_trainx.shape)\n",
    "print(nelec_trainy.shape)\n",
    "print(nelec_testx.shape)\n",
    "print(nelec_testy.shape)\n",
    "print(elec_trainx.shape)\n",
    "print(elec_trainy.shape)\n",
    "print(elec_testx.shape)\n",
    "print(elec_testy.shape)\n",
    "print(dhw_trainx.shape)\n",
    "print(dhw_trainy.shape)\n",
    "print(dhw_testx.shape)\n",
    "print(dhw_testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "nelec_trainx_tensor = torch.FloatTensor(nelec_trainx)\n",
    "nelec_trainy_tensor = torch.FloatTensor(nelec_trainy)\n",
    "\n",
    "nelec_testx_tensor = torch.FloatTensor(nelec_testx)\n",
    "nelec_testy_tensor = torch.FloatTensor(nelec_testy)\n",
    "\n",
    "elec_trainx_tensor = torch.FloatTensor(elec_trainx)\n",
    "elec_trainy_tensor = torch.FloatTensor(elec_trainy)\n",
    "\n",
    "elec_testx_tensor = torch.FloatTensor(elec_testx)\n",
    "elec_testy_tensor = torch.FloatTensor(elec_testy)\n",
    "\n",
    "dhw_trainx_tensor = torch.FloatTensor(dhw_trainx)\n",
    "dhw_trainy_tensor = torch.FloatTensor(dhw_trainy)\n",
    "\n",
    "dhw_testx_tensor = torch.FloatTensor(dhw_testx)\n",
    "dhw_testy_tensor = torch.FloatTensor(dhw_testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 형태로 데이터 정의\n",
    "nelec_train = TensorDataset(nelec_trainx_tensor, nelec_trainy_tensor)\n",
    "nelec_test = TensorDataset(nelec_testx_tensor, nelec_testy_tensor)\n",
    "\n",
    "elec_train = TensorDataset(elec_trainx_tensor, elec_trainy_tensor)\n",
    "elec_test = TensorDataset(elec_testx_tensor, elec_testy_tensor)\n",
    "\n",
    "dhw_train = TensorDataset(dhw_trainx_tensor, dhw_trainy_tensor)\n",
    "dhw_test = TensorDataset(dhw_testx_tensor, dhw_testy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_for_nelec_train = len(nelec_trainx)\n",
    "batch_for_nelec_test = len(nelec_testx)\n",
    "batch_for_elec_train = len(elec_trainx)\n",
    "batch_for_elec_test = len(elec_testx)\n",
    "batch_for_dhw_train = len(dhw_trainx)\n",
    "batch_for_dhw_test = len(dhw_testx)\n",
    "\n",
    "NELEC_train = DataLoader(nelec_train,\n",
    "                        batch_size=batch_for_nelec_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "NELEC_test = DataLoader(nelec_test,\n",
    "                        batch_size=batch_for_nelec_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "ELEC_train = DataLoader(elec_train,\n",
    "                        batch_size=batch_for_elec_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "ELEC_test = DataLoader(elec_test,\n",
    "                        batch_size=batch_for_elec_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "DHW_train = DataLoader(dhw_train,\n",
    "                        batch_size=batch_for_dhw_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "DHW_test = DataLoader(dhw_test,\n",
    "                        batch_size=batch_for_dhw_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋의 개수를 가정하여 변수에 저장\n",
    "# nelec_train_size = len(nelec_train)\n",
    "# nelec_test_size = len(nelec_test)\n",
    "# elec_train_size = len(elec_train)\n",
    "# elec_test_size = len(elec_test)\n",
    "# dhw_train_size = len(dhw_train)\n",
    "# dhw_test_size = len(dhw_test)\n",
    "\n",
    "# # 각 DataLoader에서 생성되는 배치의 개수 계산\n",
    "# nelec_train_batches = nelec_train_size // batch_for_nelec_train\n",
    "# nelec_test_batches = nelec_test_size // batch_for_nelec_test\n",
    "# elec_train_batches = elec_train_size // batch_for_elec_train\n",
    "# elec_test_batches = elec_test_size // batch_for_elec_test\n",
    "# dhw_train_batches = dhw_train_size // batch_for_dhw_train\n",
    "# dhw_test_batches = dhw_test_size // batch_for_dhw_test\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"nelec_train batches:\", nelec_train_batches)\n",
    "# print(\"nelec_test batches:\", nelec_test_batches)\n",
    "# print(\"elec_train batches:\", elec_train_batches)\n",
    "# print(\"elec_test batches:\", elec_test_batches)\n",
    "# print(\"dhw_train batches:\", dhw_train_batches)\n",
    "# print(\"dhw_test batches:\", dhw_test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientReversalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return (grad_output * -1), None\n",
    "\n",
    "class GradientReversalLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(3, 32, kernel_size=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=1, padding=0)\n",
    "        \n",
    "        self.encoder_lstm1 = nn.LSTM(10, 32, batch_first=True)\n",
    "        self.encoder_lstm2 = nn.LSTM(32, 64, batch_first=True)\n",
    "        \n",
    "        self.decoder_lstm1 = nn.LSTM(64, 64, batch_first=True)\n",
    "        self.decoder_lstm2 = nn.LSTM(64, 32, batch_first=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc = nn.Linear(32,64, bias=False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        # Encoder\n",
    "        x, _ = self.encoder_lstm1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.tanh(x)\n",
    "        x, (h_n, c_n) = self.encoder_lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x, _ = self.decoder_lstm1(x, (h_n, c_n))\n",
    "        x = self.bn2(x)\n",
    "        x = self.tanh(x)\n",
    "        x, _ = self.decoder_lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        x = self.fc(x[:,-1,:])\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hoxy = lstm()\n",
    "# cnnx = torch.randn(512,24,12)\n",
    "# hoxy2 = hoxy(cnnx)\n",
    "# print(hoxy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_regression, self).__init__()\n",
    "        self.regression_layer1 = nn.Linear(64, 128)\n",
    "        self.regression_layer2 = nn.Linear(128, 128)\n",
    "        self.regression_layer3 = nn.Linear(128, 128)\n",
    "        self.regression_layer4 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.regression_layer1(x))\n",
    "        x = F.relu(self.regression_layer2(x))\n",
    "        x = F.relu(self.regression_layer3(x))\n",
    "        x = F.relu(self.regression_layer4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_classfication(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classfication, self).__init__()\n",
    "        self.classification_layer0 = GradientReversalLayer()\n",
    "        self.classification_layer1 = nn.Linear(64, 128)\n",
    "        self.classification_layer2 = nn.Linear(128,128)\n",
    "        self.classification_layer4 = nn.Linear(128, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.classification_layer0(x)\n",
    "        x = F.relu(self.classification_layer1(x))\n",
    "        x = F.relu(self.classification_layer2(x))\n",
    "        x = torch.sigmoid(self.classification_layer4(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super(dann, self).__init__()\n",
    "        self.lstm = lstm\n",
    "        self.regression = domain_regression()\n",
    "        self.classification = domain_classfication()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.lstm(x)\n",
    "        reg_output = self.regression(feature)\n",
    "        cla_output = self.classification(feature)\n",
    "        \n",
    "        return reg_output, cla_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dann_loss, self).__init__()\n",
    "        self.reg = nn.L1Loss() # 회귀 L1 손실 == MAE\n",
    "        self.cla = nn.BCEWithLogitsLoss() # 도메인 분류\n",
    "        \n",
    "    def forward(self, result, reg_real, domain_num, alpha=1):\n",
    "        reg_output, cla_output = result\n",
    "        batch_size = reg_output.shape[0]\n",
    "        cla_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        \n",
    "        reg_loss = self.reg(reg_output, reg_real)\n",
    "        cla_loss = self.cla(cla_output, cla_target)\n",
    "    \n",
    "        loss = reg_loss + cla_loss * alpha\n",
    "        \n",
    "        return loss, reg_loss, cla_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lstm = lstm().to(device)\n",
    "model = dann(my_lstm).to(device)\n",
    "loss_fn = dann_loss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "# alpha = 1.0\n",
    "# epochs = 200\n",
    "\n",
    "# model.train() # 훈련 모드 설정\n",
    "\n",
    "# optimizer= torch.optim.Adam(model.parameters(), lr=0.01)#lr = 0.001, weight_decay=0.01)\n",
    "# scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters = epochs * len(NELEC_train))\n",
    "\n",
    "# best_source_loss = float('inf')\n",
    "# best_model_state_dict = None\n",
    "\n",
    "# for i in range(1, epochs + 1):\n",
    "#     total_loss = 0\n",
    "#     source_total_loss = 0\n",
    "#     target_total_loss = 0\n",
    "#     src_reg_total = 0\n",
    "#     src_cla_total = 0\n",
    "#     tar_reg_total = 0\n",
    "#     tar_cal_total = 0\n",
    "#     for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "#         source_x = source_data[0].to(device)\n",
    "#         source_y = source_data[1].to(device)\n",
    "#         target_x = target_data[0].to(device)\n",
    "#         target_y = target_data[1].to(device)\n",
    "#         #print(source_x.shape)\n",
    "#         #print(source_y.shape)\n",
    "#         #print(target_x.shape)\n",
    "#         #print(target_y.shape)\n",
    "#         # 순전파\n",
    "#         source_result = model(source_x)\n",
    "#         target_result = model(target_x)\n",
    "#         #print(source_result[0].shape)\n",
    "#         #print(target_result[0].shape)\n",
    "#         #print(source_result[1].shape)\n",
    "#         #print(target_result[1].shape)\n",
    "#         # 순전파 loss\n",
    "#         source_loss, source_reg_loss, source_cla_loss = loss_fn(source_result, source_y, 1, alpha=alpha) # 소스 도메인 레이블 1\n",
    "#         target_loss, target_reg_loss, target_cla_loss = loss_fn(target_result, target_y, 0, alpha=alpha) # 타겟 도메인 레이블 0\n",
    "#         loss = source_loss + target_loss\n",
    "        \n",
    "#         src_reg_total += source_reg_loss.item()\n",
    "#         src_cla_total += source_cla_loss.item()\n",
    "#         tar_reg_total += target_reg_loss.item()\n",
    "#         tar_cal_total += target_cla_loss.item()\n",
    "        \n",
    "#         source_total_loss += source_loss.item()\n",
    "#         target_total_loss += target_loss.item()\n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#         # 그래디언트 계산 및 업데이트\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#     print('Epoch : %d, Total Avg Loss : %.4f' % (i, total_loss / len(NELEC_train)))     \n",
    "#     #print('Source Avg Loss : %.4f' % (source_total_loss / len(NELEC_train)))\n",
    "#     #print('Target Avg Loss : %.4f' % (target_total_loss / len(NELEC_train)))\n",
    "#     print('Source Avg reg Loss : %.4f' % (src_reg_total / len(NELEC_train)))\n",
    "#     print('Target Avg reg Loss : %.4f' % (tar_reg_total / len(NELEC_train)))\n",
    "#     print('Source Avg cla Loss : %.4f' % (src_cla_total / len(NELEC_train)))\n",
    "#     print('Target Avg cla Loss : %.4f' % (tar_cal_total / len(NELEC_train)))\n",
    "#     print('')\n",
    "    \n",
    "#     # source_loss가 가장 낮은 경우 모델 가중치 저장\n",
    "#     if total_loss / len(NELEC_train) < best_source_loss:\n",
    "#         best_source_loss = total_loss / len(NELEC_train)\n",
    "#         best_model_state_dict = model.state_dict()\n",
    "\n",
    "# # 최상의 모델을 파일로 저장\n",
    "# if best_model_state_dict is not None:\n",
    "#     torch.save(best_model_state_dict, '0809hoxy3.pth')\n",
    "#     #torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, (512,7,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for dann:\n\tsize mismatch for lstm.conv1.weight: copying a param with shape torch.Size([32, 7, 1]) from checkpoint, the shape in current model is torch.Size([32, 3, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 저장된 최상의 소스 모델 불러오기\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_source_model2 \u001b[39m=\u001b[39m dann(my_lstm)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m best_source_model2\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m0809hoxy.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m best_source_model2\u001b[39m.\u001b[39meval()  \u001b[39m# 평가 모드로 설정\u001b[39;00m\n\u001b[0;32m      6\u001b[0m s_pred \u001b[39m=\u001b[39m []  \u001b[39m# MNIST 데이터셋의 예측값을 저장할 리스트\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for dann:\n\tsize mismatch for lstm.conv1.weight: copying a param with shape torch.Size([32, 7, 1]) from checkpoint, the shape in current model is torch.Size([32, 3, 1])."
     ]
    }
   ],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('0809hoxy.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "s_d_pred = []\n",
    "t_d_pred = []\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    source_domain_label_pred, source_pred = best_source_model2(sourcex)\n",
    "    target_domain_label_pred, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "    s_d_pred.extend(source_domain_label_pred.detach().cpu().numpy())\n",
    "    t_d_pred.extend(target_domain_label_pred.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('0809hoxy1.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_test, DHW_test)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    _, source_pred = best_source_model2(sourcex)\n",
    "    _, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Embedding Space 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 앞 batch의 250개씩의 데이터만 샘플링\n",
    "source_tsne = DataLoader(nelec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_tsne = DataLoader(dhw_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_tsne2 = next(iter(source_tsne))\n",
    "target_tsne2 = next(iter(target_tsne))\n",
    "\n",
    "source_x_TNSE = source_tsne2[0].to(device)\n",
    "source_y_TNSE = source_tsne2[1].to(device)\n",
    "\n",
    "target_x_TNSE = target_tsne2[0].to(device)\n",
    "target_y_TNSE = target_tsne2[1].to(device)\n",
    "\n",
    "# 학습된 모델의 LSTM 부분만 활용 (100차원 임베딩 벡터를 받아오는 과정)\n",
    "source_vector = model.lstm(source_x_TNSE)\n",
    "target_vector = model.lstm(target_x_TNSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([source_vector.cpu().detach().numpy(), target_vector.cpu().detach().numpy()]))\n",
    "\n",
    "tsne_np = TSNE(n_components=2).fit_transform(df)\n",
    "tsne_df = pd.DataFrame(tsne_np, columns=['component 0', 'component 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_0 = tsne_df.loc[:1000]\n",
    "tsne_df_1 = tsne_df.loc[1000:]\n",
    "\n",
    "plt.scatter(tsne_df_0['component 0'], tsne_df_0['component 1'], color='green', label='NELEC', alpha=0.5)\n",
    "plt.scatter(tsne_df_1['component 0'], tsne_df_1['component 1'], color='black', label='DHW', alpha=0.5)\n",
    "\n",
    "plt.title('alpha = '+ str(alpha))\n",
    "plt.xlabel('component 0')\n",
    "plt.ylabel('component 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
