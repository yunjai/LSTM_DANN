{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data as data_util\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = pd.read_csv('elec_merge.csv')\n",
    "nelec = pd.read_csv('n_elec_merge.csv')\n",
    "dhw = pd.read_csv('dhw_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = elec.drop(['YEAR'], axis=1)\n",
    "nelec = nelec.drop(['YEAR'], axis=1)\n",
    "dhw = dhw.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nelec = nelec.iloc[:-1,].reset_index(drop=True)\n",
    "output_nelec = nelec[['n_elec']].iloc[1:].reset_index(drop=True)\n",
    "output_nelec.columns = ['nelec']\n",
    "\n",
    "input_elec = elec.iloc[:-1,].reset_index(drop=True)\n",
    "output_elec = elec[['ELEC']].iloc[1:].reset_index(drop=True)\n",
    "output_elec.columns = ['elec']\n",
    "\n",
    "input_dhw = dhw.iloc[:-1,].reset_index(drop=True)\n",
    "output_dhw = dhw[['DHW']].iloc[1:].reset_index(drop=True)\n",
    "output_dhw.columns = ['dhw']\n",
    "\n",
    "nelec = pd.concat([input_nelec, output_nelec], axis=1)\n",
    "elec = pd.concat([input_elec, output_elec], axis=1)\n",
    "dhw = pd.concat([input_dhw, output_dhw], axis=1)\n",
    "\n",
    "# input_nelec = nelec.iloc[:-1,:-1].reset_index(drop=True)\n",
    "# output_nelec = nelec[['n_elec']].iloc[:-1].reset_index(drop=True)\n",
    "# output_nelec2 = nelec['n_elec'][1:].reset_index(drop=True)\n",
    "# output_nelec['nelec'] = output_nelec2\n",
    "\n",
    "# input_elec = elec.iloc[:-1,:-1].reset_index(drop=True)\n",
    "# output_elec = elec[['ELEC']].iloc[:-1].reset_index(drop=True)\n",
    "# output_elec2= elec['ELEC'][1:].reset_index(drop=True)\n",
    "# output_elec['elec'] = output_elec2\n",
    "\n",
    "# input_dhw = dhw.iloc[:-1,:-1].reset_index(drop=True)\n",
    "# output_dhw = dhw[['DHW']].iloc[:-1].reset_index(drop=True)\n",
    "# output_dhw2 = dhw['DHW'][1:].reset_index(drop=True)\n",
    "# output_dhw['dhw'] = output_dhw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_nelec = MinMaxScaler()\n",
    "scaler_elec = MinMaxScaler()\n",
    "scaler_dhw = MinMaxScaler()\n",
    "\n",
    "scaler_nelec.fit(nelec)\n",
    "scaler_elec.fit(elec)\n",
    "scaler_dhw.fit(dhw)\n",
    "\n",
    "scaled_nelec = scaler_nelec.transform(nelec)\n",
    "scaled_elec = scaler_elec.transform(elec)\n",
    "scaled_dhw = scaler_dhw.transform(dhw)\n",
    "\n",
    "nelec = pd.DataFrame(scaled_nelec, index=nelec.index, columns=nelec.columns)\n",
    "elec = pd.DataFrame(scaled_elec, index=elec.index, columns=elec.columns)\n",
    "dhw = pd.DataFrame(scaled_dhw, index=dhw.index, columns=dhw.columns)\n",
    "\n",
    "# scaler_nelec = MinMaxScaler()\n",
    "# scaler_elec = MinMaxScaler()\n",
    "# scaler_dhw = MinMaxScaler()\n",
    "\n",
    "# scaler_nelec.fit(input_nelec)\n",
    "# scaler_elec.fit(input_elec)\n",
    "# scaler_dhw.fit(input_dhw)\n",
    "\n",
    "# scaled_nelec = scaler_nelec.transform(input_nelec)\n",
    "# scaled_elec = scaler_elec.transform(input_elec)\n",
    "# scaled_dhw = scaler_dhw.transform(input_dhw)\n",
    "\n",
    "# nelec_x = pd.DataFrame(scaled_nelec, index=input_nelec.index, columns=input_nelec.columns)\n",
    "# elec_x = pd.DataFrame(scaled_elec, index=input_elec.index, columns=input_elec.columns)\n",
    "# dhw_x = pd.DataFrame(scaled_dhw, index=input_dhw.index, columns=input_dhw.columns)\n",
    "\n",
    "# nelec = pd.concat([nelec_x, output_nelec], axis=1)\n",
    "# elec = pd.concat([elec_x, output_elec], axis=1)\n",
    "# dhw = pd.concat([dhw_x, output_dhw], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_nelec = int(len(nelec) * 0.7)\n",
    "standard_elec = int(len(elec) * 0.7)\n",
    "standard_dhw = int(len(dhw) * 0.7)\n",
    "\n",
    "nelec_train = nelec.iloc[:standard_nelec]\n",
    "nelec_test = nelec.iloc[standard_nelec:].reset_index(drop=True)\n",
    "\n",
    "elec_train = elec.iloc[:standard_elec]\n",
    "elec_test = elec.iloc[standard_elec:].reset_index(drop=True)\n",
    "\n",
    "dhw_train = dhw.iloc[:standard_dhw]\n",
    "dhw_test = dhw.iloc[standard_dhw:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_trainx = nelec_train.drop(['nelec'], axis=1)\n",
    "nelec_trainy = nelec_train[['nelec']]\n",
    "\n",
    "nelec_testx = nelec_test.drop(['nelec'], axis=1)\n",
    "nelec_testy = nelec_test[['nelec']]\n",
    "\n",
    "elec_trainx = elec_train.drop(['elec'], axis=1)\n",
    "elec_trainy = elec_train[['elec']]\n",
    "\n",
    "elec_testx = elec_test.drop(['elec'], axis=1)\n",
    "elec_testy = elec_test[['elec']]\n",
    "\n",
    "dhw_trainx = dhw_train.drop(['dhw'], axis=1)\n",
    "dhw_trainy = dhw_train[['dhw']]\n",
    "\n",
    "dhw_testx = dhw_test.drop(['dhw'], axis=1)\n",
    "dhw_testy = dhw_test[['dhw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-7]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)\n",
    "\n",
    "nelec_trainx, nelec_trainy = buildDataSet(nelec_trainx, nelec_trainy, 7)\n",
    "nelec_testx, nelec_testy = buildDataSet(nelec_testx, nelec_testy, 7)\n",
    "\n",
    "elec_trainx, elec_trainy = buildDataSet(elec_trainx, elec_trainy, 7)\n",
    "elec_testx, elec_testy = buildDataSet(elec_testx, elec_testy, 7)\n",
    "\n",
    "dhw_trainx, dhw_trainy = buildDataSet(dhw_trainx, dhw_trainy, 7)\n",
    "dhw_testx, dhw_testy = buildDataSet(dhw_testx, dhw_testy, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20409, 7, 12)\n",
      "(20409, 1)\n",
      "(8744, 7, 12)\n",
      "(8744, 1)\n",
      "(2984, 7, 12)\n",
      "(2984, 1)\n",
      "(1275, 7, 12)\n",
      "(1275, 1)\n",
      "(20409, 7, 12)\n",
      "(20409, 1)\n",
      "(8744, 7, 12)\n",
      "(8744, 1)\n"
     ]
    }
   ],
   "source": [
    "print(nelec_trainx.shape)\n",
    "print(nelec_trainy.shape)\n",
    "print(nelec_testx.shape)\n",
    "print(nelec_testy.shape)\n",
    "print(elec_trainx.shape)\n",
    "print(elec_trainy.shape)\n",
    "print(elec_testx.shape)\n",
    "print(elec_testy.shape)\n",
    "print(dhw_trainx.shape)\n",
    "print(dhw_trainy.shape)\n",
    "print(dhw_testx.shape)\n",
    "print(dhw_testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "nelec_trainx_tensor = torch.FloatTensor(nelec_trainx)\n",
    "nelec_trainy_tensor = torch.FloatTensor(nelec_trainy)\n",
    "\n",
    "nelec_testx_tensor = torch.FloatTensor(nelec_testx)\n",
    "nelec_testy_tensor = torch.FloatTensor(nelec_testy)\n",
    "\n",
    "elec_trainx_tensor = torch.FloatTensor(elec_trainx)\n",
    "elec_trainy_tensor = torch.FloatTensor(elec_trainy)\n",
    "\n",
    "elec_testx_tensor = torch.FloatTensor(elec_testx)\n",
    "elec_testy_tensor = torch.FloatTensor(elec_testy)\n",
    "\n",
    "dhw_trainx_tensor = torch.FloatTensor(dhw_trainx)\n",
    "dhw_trainy_tensor = torch.FloatTensor(dhw_trainy)\n",
    "\n",
    "dhw_testx_tensor = torch.FloatTensor(dhw_testx)\n",
    "dhw_testy_tensor = torch.FloatTensor(dhw_testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 형태로 데이터 정의\n",
    "nelec_train = TensorDataset(nelec_trainx_tensor, nelec_trainy_tensor)\n",
    "nelec_test = TensorDataset(nelec_testx_tensor, nelec_testy_tensor)\n",
    "\n",
    "elec_train = TensorDataset(elec_trainx_tensor, elec_trainy_tensor)\n",
    "elec_test = TensorDataset(elec_testx_tensor, elec_testy_tensor)\n",
    "\n",
    "dhw_train = TensorDataset(dhw_trainx_tensor, dhw_trainy_tensor)\n",
    "dhw_test = TensorDataset(dhw_testx_tensor, dhw_testy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_for_nelec_train = len(nelec_trainx)\n",
    "batch_for_nelec_test = len(nelec_testx)\n",
    "batch_for_elec_train = len(elec_trainx)\n",
    "batch_for_elec_test = len(elec_testx)\n",
    "batch_for_dhw_train = len(dhw_trainx)\n",
    "batch_for_dhw_test = len(dhw_testx)\n",
    "\n",
    "NELEC_train = DataLoader(nelec_train,\n",
    "                        batch_size=1024,#batch_for_nelec_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "NELEC_test = DataLoader(nelec_test,\n",
    "                        batch_size=1024,#batch_for_nelec_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "ELEC_train = DataLoader(elec_train,\n",
    "                        batch_size=1024,#batch_for_elec_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "ELEC_test = DataLoader(elec_test,\n",
    "                        batch_size=1024,#batch_for_elec_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "DHW_train = DataLoader(dhw_train,\n",
    "                        batch_size=1024,#batch_for_dhw_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "DHW_test = DataLoader(dhw_test,\n",
    "                        batch_size=1024,#batch_for_dhw_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋의 개수를 가정하여 변수에 저장\n",
    "# nelec_train_size = len(nelec_train)\n",
    "# nelec_test_size = len(nelec_test)\n",
    "# elec_train_size = len(elec_train)\n",
    "# elec_test_size = len(elec_test)\n",
    "# dhw_train_size = len(dhw_train)\n",
    "# dhw_test_size = len(dhw_test)\n",
    "\n",
    "# # 각 DataLoader에서 생성되는 배치의 개수 계산\n",
    "# nelec_train_batches = nelec_train_size // batch_for_nelec_train\n",
    "# nelec_test_batches = nelec_test_size // batch_for_nelec_test\n",
    "# elec_train_batches = elec_train_size // batch_for_elec_train\n",
    "# elec_test_batches = elec_test_size // batch_for_elec_test\n",
    "# dhw_train_batches = dhw_train_size // batch_for_dhw_train\n",
    "# dhw_test_batches = dhw_test_size // batch_for_dhw_test\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"nelec_train batches:\", nelec_train_batches)\n",
    "# print(\"nelec_test batches:\", nelec_test_batches)\n",
    "# print(\"elec_train batches:\", elec_train_batches)\n",
    "# print(\"elec_test batches:\", elec_test_batches)\n",
    "# print(\"dhw_train batches:\", dhw_train_batches)\n",
    "# print(\"dhw_test batches:\", dhw_test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientReversalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return (grad_output * -1), None\n",
    "\n",
    "class GradientReversalLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(12, 64, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(7)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(64, 64, batch_first=True)\n",
    "        self.bn2 = nn.BatchNorm1d(7)\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(64,64, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(64, 64, bias=False)\n",
    "        self.fc2 = nn.Linear(64, 100, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(7)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.swish = Swish()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (h,c) = self.lstm1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        x, (h, c) = self.lstm2(x, (h, c))\n",
    "        x = self.bn2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        x, _ = self.lstm3(x, (h, c))\n",
    "        x = self.bn2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.fc2(x[:,-1,:])\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 100])\n"
     ]
    }
   ],
   "source": [
    "hoxy = lstm()\n",
    "inputx = torch.randn(1024,7,12)\n",
    "hoxy2 = hoxy(inputx)\n",
    "print(hoxy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_regression, self).__init__()\n",
    "        self.regression_layer1 = nn.Linear(100, 100)\n",
    "        self.regression_layer2 = nn.Linear(100, 100)\n",
    "        self.regression_layer3 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.regression_layer1(x))\n",
    "        x = F.relu(self.regression_layer2(x))\n",
    "        x = F.relu(self.regression_layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_classfication(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classfication, self).__init__()\n",
    "        self.classification_layer0 = GradientReversalLayer()\n",
    "        self.classification_layer1 = nn.Linear(100, 100)\n",
    "        self.classification_layer2 = nn.Linear(100 ,1)\n",
    "        #self.classification_layer3 = nn.Linear(128,128)\n",
    "        #self.classification_layer4 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classification_layer0(x)\n",
    "        x = F.relu(self.classification_layer1(x))\n",
    "        x = torch.sigmoid(self.classification_layer2(x))\n",
    "        #x = F.relu(self.classification_layer3(x))\n",
    "        #x = torch.sigmoid(self.classification_layer4(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super(dann, self).__init__()\n",
    "        self.lstm = lstm\n",
    "        self.regression = domain_regression()\n",
    "        self.classification = domain_classfication()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.lstm(x)\n",
    "        reg_output = self.regression(feature)\n",
    "        cla_output = self.classification(feature)\n",
    "        \n",
    "        return reg_output, cla_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dann_loss, self).__init__()\n",
    "        self.reg = nn.L1Loss() # 회귀 L1 손실 == MAE\n",
    "        self.cla = nn.BCEWithLogitsLoss() # 도메인 분류\n",
    "        \n",
    "    def forward(self, result, reg_real, domain_num, alpha=1):\n",
    "        reg_output, cla_output = result\n",
    "        batch_size = reg_output.shape[0]\n",
    "        cla_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        \n",
    "        reg_loss = self.reg(reg_output, reg_real)\n",
    "        cla_loss = self.cla(cla_output, cla_target)\n",
    "        cla_loss2 = cla_loss * alpha\n",
    "        \n",
    "        loss = reg_loss + cla_loss2\n",
    "        \n",
    "        return loss, reg_loss, cla_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lstm = lstm().to(device)\n",
    "model = dann(my_lstm).to(device)\n",
    "loss_fn = dann_loss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Total Avg Loss : 1.9634\n",
      "Source Avg Loss : 0.8766\n",
      "Target Avg Loss : 1.0867\n",
      "Source Avg reg Loss : 0.2757\n",
      "Target Avg reg Loss : 0.2843\n",
      "Source Avg cla Loss : 0.6009\n",
      "Target Avg cla Loss : 0.8024\n",
      "Avg Regression Loss : 0.5601\n",
      "Avg Classification Loss : 1.4033\n",
      "\n",
      "Epoch : 2, Total Avg Loss : 1.8678\n",
      "Source Avg Loss : 0.9145\n",
      "Target Avg Loss : 0.9534\n",
      "Source Avg reg Loss : 0.2227\n",
      "Target Avg reg Loss : 0.2588\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.4815\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 3, Total Avg Loss : 1.8251\n",
      "Source Avg Loss : 0.8604\n",
      "Target Avg Loss : 0.9647\n",
      "Source Avg reg Loss : 0.1675\n",
      "Target Avg reg Loss : 0.2713\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.4388\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 4, Total Avg Loss : 1.8021\n",
      "Source Avg Loss : 0.8515\n",
      "Target Avg Loss : 0.9506\n",
      "Source Avg reg Loss : 0.1586\n",
      "Target Avg reg Loss : 0.2573\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.4159\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 5, Total Avg Loss : 1.7952\n",
      "Source Avg Loss : 0.8468\n",
      "Target Avg Loss : 0.9484\n",
      "Source Avg reg Loss : 0.1539\n",
      "Target Avg reg Loss : 0.2550\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.4089\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 6, Total Avg Loss : 1.7936\n",
      "Source Avg Loss : 0.8453\n",
      "Target Avg Loss : 0.9482\n",
      "Source Avg reg Loss : 0.1525\n",
      "Target Avg reg Loss : 0.2549\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.4073\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 7, Total Avg Loss : 1.7913\n",
      "Source Avg Loss : 0.8445\n",
      "Target Avg Loss : 0.9468\n",
      "Source Avg reg Loss : 0.1516\n",
      "Target Avg reg Loss : 0.2534\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.4050\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 8, Total Avg Loss : 1.7897\n",
      "Source Avg Loss : 0.8434\n",
      "Target Avg Loss : 0.9463\n",
      "Source Avg reg Loss : 0.1506\n",
      "Target Avg reg Loss : 0.2528\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.4034\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 9, Total Avg Loss : 1.7891\n",
      "Source Avg Loss : 0.8431\n",
      "Target Avg Loss : 0.9460\n",
      "Source Avg reg Loss : 0.1503\n",
      "Target Avg reg Loss : 0.2525\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.4028\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 10, Total Avg Loss : 1.7877\n",
      "Source Avg Loss : 0.8427\n",
      "Target Avg Loss : 0.9451\n",
      "Source Avg reg Loss : 0.1499\n",
      "Target Avg reg Loss : 0.2515\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.4014\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 11, Total Avg Loss : 1.7869\n",
      "Source Avg Loss : 0.8419\n",
      "Target Avg Loss : 0.9451\n",
      "Source Avg reg Loss : 0.1492\n",
      "Target Avg reg Loss : 0.2514\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.4007\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 12, Total Avg Loss : 1.7861\n",
      "Source Avg Loss : 0.8417\n",
      "Target Avg Loss : 0.9444\n",
      "Source Avg reg Loss : 0.1492\n",
      "Target Avg reg Loss : 0.2506\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.3998\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 13, Total Avg Loss : 1.7854\n",
      "Source Avg Loss : 0.8412\n",
      "Target Avg Loss : 0.9442\n",
      "Source Avg reg Loss : 0.1487\n",
      "Target Avg reg Loss : 0.2504\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.3991\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 14, Total Avg Loss : 1.7844\n",
      "Source Avg Loss : 0.8403\n",
      "Target Avg Loss : 0.9441\n",
      "Source Avg reg Loss : 0.1479\n",
      "Target Avg reg Loss : 0.2502\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.3981\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 15, Total Avg Loss : 1.7837\n",
      "Source Avg Loss : 0.8401\n",
      "Target Avg Loss : 0.9435\n",
      "Source Avg reg Loss : 0.1478\n",
      "Target Avg reg Loss : 0.2496\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.3974\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 16, Total Avg Loss : 1.7834\n",
      "Source Avg Loss : 0.8399\n",
      "Target Avg Loss : 0.9435\n",
      "Source Avg reg Loss : 0.1477\n",
      "Target Avg reg Loss : 0.2494\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.3971\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 17, Total Avg Loss : 1.7830\n",
      "Source Avg Loss : 0.8396\n",
      "Target Avg Loss : 0.9434\n",
      "Source Avg reg Loss : 0.1475\n",
      "Target Avg reg Loss : 0.2492\n",
      "Source Avg cla Loss : 0.6921\n",
      "Target Avg cla Loss : 0.6942\n",
      "Avg Regression Loss : 0.3967\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 18, Total Avg Loss : 1.7828\n",
      "Source Avg Loss : 0.8393\n",
      "Target Avg Loss : 0.9435\n",
      "Source Avg reg Loss : 0.1473\n",
      "Target Avg reg Loss : 0.2493\n",
      "Source Avg cla Loss : 0.6921\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.3965\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 19, Total Avg Loss : 1.7825\n",
      "Source Avg Loss : 0.8397\n",
      "Target Avg Loss : 0.9428\n",
      "Source Avg reg Loss : 0.1477\n",
      "Target Avg reg Loss : 0.2484\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3961\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 20, Total Avg Loss : 1.7820\n",
      "Source Avg Loss : 0.8392\n",
      "Target Avg Loss : 0.9427\n",
      "Source Avg reg Loss : 0.1473\n",
      "Target Avg reg Loss : 0.2483\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3957\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 21, Total Avg Loss : 1.7818\n",
      "Source Avg Loss : 0.8391\n",
      "Target Avg Loss : 0.9427\n",
      "Source Avg reg Loss : 0.1471\n",
      "Target Avg reg Loss : 0.2483\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3954\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 22, Total Avg Loss : 1.7814\n",
      "Source Avg Loss : 0.8388\n",
      "Target Avg Loss : 0.9426\n",
      "Source Avg reg Loss : 0.1469\n",
      "Target Avg reg Loss : 0.2482\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.3951\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 23, Total Avg Loss : 1.7810\n",
      "Source Avg Loss : 0.8387\n",
      "Target Avg Loss : 0.9423\n",
      "Source Avg reg Loss : 0.1468\n",
      "Target Avg reg Loss : 0.2479\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3947\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 24, Total Avg Loss : 1.7808\n",
      "Source Avg Loss : 0.8384\n",
      "Target Avg Loss : 0.9425\n",
      "Source Avg reg Loss : 0.1466\n",
      "Target Avg reg Loss : 0.2479\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3945\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 25, Total Avg Loss : 1.7805\n",
      "Source Avg Loss : 0.8386\n",
      "Target Avg Loss : 0.9419\n",
      "Source Avg reg Loss : 0.1468\n",
      "Target Avg reg Loss : 0.2474\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3942\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 26, Total Avg Loss : 1.7804\n",
      "Source Avg Loss : 0.8383\n",
      "Target Avg Loss : 0.9421\n",
      "Source Avg reg Loss : 0.1466\n",
      "Target Avg reg Loss : 0.2475\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.3941\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 27, Total Avg Loss : 1.7802\n",
      "Source Avg Loss : 0.8382\n",
      "Target Avg Loss : 0.9420\n",
      "Source Avg reg Loss : 0.1465\n",
      "Target Avg reg Loss : 0.2474\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.3939\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 28, Total Avg Loss : 1.7800\n",
      "Source Avg Loss : 0.8380\n",
      "Target Avg Loss : 0.9419\n",
      "Source Avg reg Loss : 0.1465\n",
      "Target Avg reg Loss : 0.2473\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.3937\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 29, Total Avg Loss : 1.7800\n",
      "Source Avg Loss : 0.8377\n",
      "Target Avg Loss : 0.9423\n",
      "Source Avg reg Loss : 0.1462\n",
      "Target Avg reg Loss : 0.2474\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.3937\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 30, Total Avg Loss : 1.7797\n",
      "Source Avg Loss : 0.8375\n",
      "Target Avg Loss : 0.9422\n",
      "Source Avg reg Loss : 0.1462\n",
      "Target Avg reg Loss : 0.2472\n",
      "Source Avg cla Loss : 0.6913\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.3934\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 31, Total Avg Loss : 1.7796\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9422\n",
      "Source Avg reg Loss : 0.1461\n",
      "Target Avg reg Loss : 0.2472\n",
      "Source Avg cla Loss : 0.6913\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.3933\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 32, Total Avg Loss : 1.7794\n",
      "Source Avg Loss : 0.8372\n",
      "Target Avg Loss : 0.9422\n",
      "Source Avg reg Loss : 0.1459\n",
      "Target Avg reg Loss : 0.2472\n",
      "Source Avg cla Loss : 0.6913\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.3931\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 33, Total Avg Loss : 1.7794\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9420\n",
      "Source Avg reg Loss : 0.1461\n",
      "Target Avg reg Loss : 0.2469\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.3931\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 34, Total Avg Loss : 1.7794\n",
      "Source Avg Loss : 0.8372\n",
      "Target Avg Loss : 0.9422\n",
      "Source Avg reg Loss : 0.1460\n",
      "Target Avg reg Loss : 0.2470\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6952\n",
      "Avg Regression Loss : 0.3930\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 35, Total Avg Loss : 1.7792\n",
      "Source Avg Loss : 0.8371\n",
      "Target Avg Loss : 0.9421\n",
      "Source Avg reg Loss : 0.1459\n",
      "Target Avg reg Loss : 0.2470\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.3929\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 36, Total Avg Loss : 1.7791\n",
      "Source Avg Loss : 0.8370\n",
      "Target Avg Loss : 0.9420\n",
      "Source Avg reg Loss : 0.1459\n",
      "Target Avg reg Loss : 0.2469\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6952\n",
      "Avg Regression Loss : 0.3928\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 37, Total Avg Loss : 1.7790\n",
      "Source Avg Loss : 0.8371\n",
      "Target Avg Loss : 0.9419\n",
      "Source Avg reg Loss : 0.1459\n",
      "Target Avg reg Loss : 0.2468\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.3927\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 38, Total Avg Loss : 1.7790\n",
      "Source Avg Loss : 0.8371\n",
      "Target Avg Loss : 0.9419\n",
      "Source Avg reg Loss : 0.1459\n",
      "Target Avg reg Loss : 0.2468\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.3927\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 39, Total Avg Loss : 1.7788\n",
      "Source Avg Loss : 0.8370\n",
      "Target Avg Loss : 0.9419\n",
      "Source Avg reg Loss : 0.1458\n",
      "Target Avg reg Loss : 0.2468\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.3925\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 40, Total Avg Loss : 1.7788\n",
      "Source Avg Loss : 0.8368\n",
      "Target Avg Loss : 0.9420\n",
      "Source Avg reg Loss : 0.1458\n",
      "Target Avg reg Loss : 0.2467\n",
      "Source Avg cla Loss : 0.6910\n",
      "Target Avg cla Loss : 0.6953\n",
      "Avg Regression Loss : 0.3925\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 41, Total Avg Loss : 1.7788\n",
      "Source Avg Loss : 0.8367\n",
      "Target Avg Loss : 0.9420\n",
      "Source Avg reg Loss : 0.1457\n",
      "Target Avg reg Loss : 0.2467\n",
      "Source Avg cla Loss : 0.6910\n",
      "Target Avg cla Loss : 0.6953\n",
      "Avg Regression Loss : 0.3924\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 42, Total Avg Loss : 1.7787\n",
      "Source Avg Loss : 0.8369\n",
      "Target Avg Loss : 0.9417\n",
      "Source Avg reg Loss : 0.1458\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6911\n",
      "Target Avg cla Loss : 0.6952\n",
      "Avg Regression Loss : 0.3924\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 43, Total Avg Loss : 1.7787\n",
      "Source Avg Loss : 0.8370\n",
      "Target Avg Loss : 0.9417\n",
      "Source Avg reg Loss : 0.1458\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.3924\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 44, Total Avg Loss : 1.7787\n",
      "Source Avg Loss : 0.8370\n",
      "Target Avg Loss : 0.9416\n",
      "Source Avg reg Loss : 0.1457\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6913\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.3923\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 45, Total Avg Loss : 1.7786\n",
      "Source Avg Loss : 0.8371\n",
      "Target Avg Loss : 0.9415\n",
      "Source Avg reg Loss : 0.1457\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.3923\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 46, Total Avg Loss : 1.7786\n",
      "Source Avg Loss : 0.8372\n",
      "Target Avg Loss : 0.9414\n",
      "Source Avg reg Loss : 0.1457\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.3922\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 47, Total Avg Loss : 1.7786\n",
      "Source Avg Loss : 0.8372\n",
      "Target Avg Loss : 0.9414\n",
      "Source Avg reg Loss : 0.1457\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.3923\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 48, Total Avg Loss : 1.7785\n",
      "Source Avg Loss : 0.8372\n",
      "Target Avg Loss : 0.9413\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.3922\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 49, Total Avg Loss : 1.7785\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.3922\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 50, Total Avg Loss : 1.7785\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.3922\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 51, Total Avg Loss : 1.7785\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.3922\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 52, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 53, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.3922\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 54, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 55, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 56, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 57, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8372\n",
      "Target Avg Loss : 0.9412\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 58, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8373\n",
      "Target Avg Loss : 0.9411\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 59, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9410\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 60, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9410\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 61, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9409\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 62, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9410\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 63, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9410\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 64, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8375\n",
      "Target Avg Loss : 0.9409\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 65, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8374\n",
      "Target Avg Loss : 0.9410\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 66, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8376\n",
      "Target Avg Loss : 0.9408\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 67, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8376\n",
      "Target Avg Loss : 0.9407\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6921\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 68, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 69, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 70, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 71, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 72, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 73, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 74, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 75, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 76, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 77, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9406\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2466\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 78, Total Avg Loss : 1.7784\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 79, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 80, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3921\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 81, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.3920\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 82, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1456\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3920\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 83, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3920\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 84, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3920\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 85, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9404\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3920\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 86, Total Avg Loss : 1.7783\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9405\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3920\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 87, Total Avg Loss : 1.7782\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9404\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.3920\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 88, Total Avg Loss : 1.7782\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9404\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2465\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3919\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 89, Total Avg Loss : 1.7782\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9404\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3919\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 90, Total Avg Loss : 1.7782\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9404\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.3919\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 91, Total Avg Loss : 1.7782\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9404\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.3919\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 92, Total Avg Loss : 1.7782\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9403\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.3919\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 93, Total Avg Loss : 1.7782\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9403\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.3919\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 94, Total Avg Loss : 1.7781\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9402\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2463\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.3918\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 95, Total Avg Loss : 1.7781\n",
      "Source Avg Loss : 0.8380\n",
      "Target Avg Loss : 0.9402\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2463\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.3918\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 96, Total Avg Loss : 1.7781\n",
      "Source Avg Loss : 0.8379\n",
      "Target Avg Loss : 0.9401\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2463\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.3918\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 97, Total Avg Loss : 1.7781\n",
      "Source Avg Loss : 0.8380\n",
      "Target Avg Loss : 0.9401\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2463\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.3918\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 98, Total Avg Loss : 1.7781\n",
      "Source Avg Loss : 0.8380\n",
      "Target Avg Loss : 0.9401\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2463\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.3918\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 99, Total Avg Loss : 1.7780\n",
      "Source Avg Loss : 0.8380\n",
      "Target Avg Loss : 0.9400\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2463\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.3917\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 100, Total Avg Loss : 1.7780\n",
      "Source Avg Loss : 0.8380\n",
      "Target Avg Loss : 0.9400\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2463\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.3917\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 101, Total Avg Loss : 1.7781\n",
      "Source Avg Loss : 0.8378\n",
      "Target Avg Loss : 0.9403\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2462\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.3917\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 102, Total Avg Loss : 1.7785\n",
      "Source Avg Loss : 0.8376\n",
      "Target Avg Loss : 0.9409\n",
      "Source Avg reg Loss : 0.1455\n",
      "Target Avg reg Loss : 0.2464\n",
      "Source Avg cla Loss : 0.6921\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.3919\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 103, Total Avg Loss : 1.7751\n",
      "Source Avg Loss : 0.7818\n",
      "Target Avg Loss : 0.9933\n",
      "Source Avg reg Loss : 0.1496\n",
      "Target Avg reg Loss : 0.2107\n",
      "Source Avg cla Loss : 0.6323\n",
      "Target Avg cla Loss : 0.7825\n",
      "Avg Regression Loss : 0.3603\n",
      "Avg Classification Loss : 1.4148\n",
      "\n",
      "Epoch : 104, Total Avg Loss : 1.6991\n",
      "Source Avg Loss : 0.8228\n",
      "Target Avg Loss : 0.8763\n",
      "Source Avg reg Loss : 0.1393\n",
      "Target Avg reg Loss : 0.1587\n",
      "Source Avg cla Loss : 0.6835\n",
      "Target Avg cla Loss : 0.7176\n",
      "Avg Regression Loss : 0.2980\n",
      "Avg Classification Loss : 1.4011\n",
      "\n",
      "Epoch : 105, Total Avg Loss : 1.6204\n",
      "Source Avg Loss : 0.8333\n",
      "Target Avg Loss : 0.7871\n",
      "Source Avg reg Loss : 0.1414\n",
      "Target Avg reg Loss : 0.0914\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6957\n",
      "Avg Regression Loss : 0.2328\n",
      "Avg Classification Loss : 1.3876\n",
      "\n",
      "Epoch : 106, Total Avg Loss : 1.6077\n",
      "Source Avg Loss : 0.8229\n",
      "Target Avg Loss : 0.7848\n",
      "Source Avg reg Loss : 0.1313\n",
      "Target Avg reg Loss : 0.0896\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6952\n",
      "Avg Regression Loss : 0.2209\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 107, Total Avg Loss : 1.6011\n",
      "Source Avg Loss : 0.8203\n",
      "Target Avg Loss : 0.7809\n",
      "Source Avg reg Loss : 0.1285\n",
      "Target Avg reg Loss : 0.0863\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.2148\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 108, Total Avg Loss : 1.6053\n",
      "Source Avg Loss : 0.8094\n",
      "Target Avg Loss : 0.7959\n",
      "Source Avg reg Loss : 0.1191\n",
      "Target Avg reg Loss : 0.0815\n",
      "Source Avg cla Loss : 0.6903\n",
      "Target Avg cla Loss : 0.7144\n",
      "Avg Regression Loss : 0.2006\n",
      "Avg Classification Loss : 1.4047\n",
      "\n",
      "Epoch : 109, Total Avg Loss : 1.5370\n",
      "Source Avg Loss : 0.7774\n",
      "Target Avg Loss : 0.7596\n",
      "Source Avg reg Loss : 0.0912\n",
      "Target Avg reg Loss : 0.0586\n",
      "Source Avg cla Loss : 0.6862\n",
      "Target Avg cla Loss : 0.7010\n",
      "Avg Regression Loss : 0.1498\n",
      "Avg Classification Loss : 1.3872\n",
      "\n",
      "Epoch : 110, Total Avg Loss : 1.5366\n",
      "Source Avg Loss : 0.7745\n",
      "Target Avg Loss : 0.7621\n",
      "Source Avg reg Loss : 0.0917\n",
      "Target Avg reg Loss : 0.0588\n",
      "Source Avg cla Loss : 0.6828\n",
      "Target Avg cla Loss : 0.7033\n",
      "Avg Regression Loss : 0.1505\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 111, Total Avg Loss : 1.5225\n",
      "Source Avg Loss : 0.7666\n",
      "Target Avg Loss : 0.7559\n",
      "Source Avg reg Loss : 0.0811\n",
      "Target Avg reg Loss : 0.0554\n",
      "Source Avg cla Loss : 0.6855\n",
      "Target Avg cla Loss : 0.7005\n",
      "Avg Regression Loss : 0.1364\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 112, Total Avg Loss : 1.5136\n",
      "Source Avg Loss : 0.7592\n",
      "Target Avg Loss : 0.7544\n",
      "Source Avg reg Loss : 0.0790\n",
      "Target Avg reg Loss : 0.0488\n",
      "Source Avg cla Loss : 0.6802\n",
      "Target Avg cla Loss : 0.7056\n",
      "Avg Regression Loss : 0.1278\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 113, Total Avg Loss : 1.4880\n",
      "Source Avg Loss : 0.7366\n",
      "Target Avg Loss : 0.7515\n",
      "Source Avg reg Loss : 0.0586\n",
      "Target Avg reg Loss : 0.0430\n",
      "Source Avg cla Loss : 0.6780\n",
      "Target Avg cla Loss : 0.7084\n",
      "Avg Regression Loss : 0.1016\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 114, Total Avg Loss : 1.5080\n",
      "Source Avg Loss : 0.7442\n",
      "Target Avg Loss : 0.7638\n",
      "Source Avg reg Loss : 0.0741\n",
      "Target Avg reg Loss : 0.0516\n",
      "Source Avg cla Loss : 0.6700\n",
      "Target Avg cla Loss : 0.7123\n",
      "Avg Regression Loss : 0.1257\n",
      "Avg Classification Loss : 1.3823\n",
      "\n",
      "Epoch : 115, Total Avg Loss : 1.4910\n",
      "Source Avg Loss : 0.7015\n",
      "Target Avg Loss : 0.7895\n",
      "Source Avg reg Loss : 0.0741\n",
      "Target Avg reg Loss : 0.0480\n",
      "Source Avg cla Loss : 0.6273\n",
      "Target Avg cla Loss : 0.7415\n",
      "Avg Regression Loss : 0.1221\n",
      "Avg Classification Loss : 1.3689\n",
      "\n",
      "Epoch : 116, Total Avg Loss : 1.6885\n",
      "Source Avg Loss : 0.6705\n",
      "Target Avg Loss : 1.0180\n",
      "Source Avg reg Loss : 0.0812\n",
      "Target Avg reg Loss : 0.1499\n",
      "Source Avg cla Loss : 0.5893\n",
      "Target Avg cla Loss : 0.8681\n",
      "Avg Regression Loss : 0.2311\n",
      "Avg Classification Loss : 1.4574\n",
      "\n",
      "Epoch : 117, Total Avg Loss : 1.5610\n",
      "Source Avg Loss : 0.6741\n",
      "Target Avg Loss : 0.8869\n",
      "Source Avg reg Loss : 0.0856\n",
      "Target Avg reg Loss : 0.1043\n",
      "Source Avg cla Loss : 0.5885\n",
      "Target Avg cla Loss : 0.7826\n",
      "Avg Regression Loss : 0.1899\n",
      "Avg Classification Loss : 1.3711\n",
      "\n",
      "Epoch : 118, Total Avg Loss : 1.5004\n",
      "Source Avg Loss : 0.5803\n",
      "Target Avg Loss : 0.9201\n",
      "Source Avg reg Loss : 0.0997\n",
      "Target Avg reg Loss : 0.0653\n",
      "Source Avg cla Loss : 0.4806\n",
      "Target Avg cla Loss : 0.8548\n",
      "Avg Regression Loss : 0.1650\n",
      "Avg Classification Loss : 1.3354\n",
      "\n",
      "Epoch : 119, Total Avg Loss : 1.5066\n",
      "Source Avg Loss : 0.6127\n",
      "Target Avg Loss : 0.8938\n",
      "Source Avg reg Loss : 0.0887\n",
      "Target Avg reg Loss : 0.0531\n",
      "Source Avg cla Loss : 0.5240\n",
      "Target Avg cla Loss : 0.8407\n",
      "Avg Regression Loss : 0.1419\n",
      "Avg Classification Loss : 1.3647\n",
      "\n",
      "Epoch : 120, Total Avg Loss : 1.5121\n",
      "Source Avg Loss : 0.6179\n",
      "Target Avg Loss : 0.8943\n",
      "Source Avg reg Loss : 0.0853\n",
      "Target Avg reg Loss : 0.0445\n",
      "Source Avg cla Loss : 0.5325\n",
      "Target Avg cla Loss : 0.8497\n",
      "Avg Regression Loss : 0.1299\n",
      "Avg Classification Loss : 1.3823\n",
      "\n",
      "Epoch : 121, Total Avg Loss : 1.4979\n",
      "Source Avg Loss : 0.6010\n",
      "Target Avg Loss : 0.8969\n",
      "Source Avg reg Loss : 0.0793\n",
      "Target Avg reg Loss : 0.0469\n",
      "Source Avg cla Loss : 0.5217\n",
      "Target Avg cla Loss : 0.8500\n",
      "Avg Regression Loss : 0.1262\n",
      "Avg Classification Loss : 1.3717\n",
      "\n",
      "Epoch : 122, Total Avg Loss : 1.5442\n",
      "Source Avg Loss : 0.6270\n",
      "Target Avg Loss : 0.9172\n",
      "Source Avg reg Loss : 0.1087\n",
      "Target Avg reg Loss : 0.0638\n",
      "Source Avg cla Loss : 0.5183\n",
      "Target Avg cla Loss : 0.8534\n",
      "Avg Regression Loss : 0.1725\n",
      "Avg Classification Loss : 1.3717\n",
      "\n",
      "Epoch : 123, Total Avg Loss : 1.6216\n",
      "Source Avg Loss : 0.6076\n",
      "Target Avg Loss : 1.0139\n",
      "Source Avg reg Loss : 0.1071\n",
      "Target Avg reg Loss : 0.0951\n",
      "Source Avg cla Loss : 0.5005\n",
      "Target Avg cla Loss : 0.9188\n",
      "Avg Regression Loss : 0.2022\n",
      "Avg Classification Loss : 1.4193\n",
      "\n",
      "Epoch : 124, Total Avg Loss : 1.8053\n",
      "Source Avg Loss : 0.6128\n",
      "Target Avg Loss : 1.1925\n",
      "Source Avg reg Loss : 0.0987\n",
      "Target Avg reg Loss : 0.0654\n",
      "Source Avg cla Loss : 0.5141\n",
      "Target Avg cla Loss : 1.1271\n",
      "Avg Regression Loss : 0.1642\n",
      "Avg Classification Loss : 1.6412\n",
      "\n",
      "Epoch : 125, Total Avg Loss : 1.7286\n",
      "Source Avg Loss : 0.6299\n",
      "Target Avg Loss : 1.0987\n",
      "Source Avg reg Loss : 0.0986\n",
      "Target Avg reg Loss : 0.0772\n",
      "Source Avg cla Loss : 0.5313\n",
      "Target Avg cla Loss : 1.0215\n",
      "Avg Regression Loss : 0.1758\n",
      "Avg Classification Loss : 1.5528\n",
      "\n",
      "Epoch : 126, Total Avg Loss : 1.4905\n",
      "Source Avg Loss : 0.6805\n",
      "Target Avg Loss : 0.8100\n",
      "Source Avg reg Loss : 0.0755\n",
      "Target Avg reg Loss : 0.0498\n",
      "Source Avg cla Loss : 0.6050\n",
      "Target Avg cla Loss : 0.7602\n",
      "Avg Regression Loss : 0.1253\n",
      "Avg Classification Loss : 1.3652\n",
      "\n",
      "Epoch : 127, Total Avg Loss : 1.4765\n",
      "Source Avg Loss : 0.6714\n",
      "Target Avg Loss : 0.8051\n",
      "Source Avg reg Loss : 0.0644\n",
      "Target Avg reg Loss : 0.0433\n",
      "Source Avg cla Loss : 0.6070\n",
      "Target Avg cla Loss : 0.7618\n",
      "Avg Regression Loss : 0.1077\n",
      "Avg Classification Loss : 1.3688\n",
      "\n",
      "Epoch : 128, Total Avg Loss : 1.5726\n",
      "Source Avg Loss : 0.6991\n",
      "Target Avg Loss : 0.8735\n",
      "Source Avg reg Loss : 0.0643\n",
      "Target Avg reg Loss : 0.0763\n",
      "Source Avg cla Loss : 0.6348\n",
      "Target Avg cla Loss : 0.7972\n",
      "Avg Regression Loss : 0.1406\n",
      "Avg Classification Loss : 1.4320\n",
      "\n",
      "Epoch : 129, Total Avg Loss : 1.6796\n",
      "Source Avg Loss : 0.7032\n",
      "Target Avg Loss : 0.9764\n",
      "Source Avg reg Loss : 0.0576\n",
      "Target Avg reg Loss : 0.0897\n",
      "Source Avg cla Loss : 0.6456\n",
      "Target Avg cla Loss : 0.8868\n",
      "Avg Regression Loss : 0.1472\n",
      "Avg Classification Loss : 1.5324\n",
      "\n",
      "Epoch : 130, Total Avg Loss : 1.5820\n",
      "Source Avg Loss : 0.7447\n",
      "Target Avg Loss : 0.8373\n",
      "Source Avg reg Loss : 0.0732\n",
      "Target Avg reg Loss : 0.0777\n",
      "Source Avg cla Loss : 0.6715\n",
      "Target Avg cla Loss : 0.7596\n",
      "Avg Regression Loss : 0.1509\n",
      "Avg Classification Loss : 1.4310\n",
      "\n",
      "Epoch : 131, Total Avg Loss : 1.4967\n",
      "Source Avg Loss : 0.7406\n",
      "Target Avg Loss : 0.7561\n",
      "Source Avg reg Loss : 0.0682\n",
      "Target Avg reg Loss : 0.0474\n",
      "Source Avg cla Loss : 0.6723\n",
      "Target Avg cla Loss : 0.7087\n",
      "Avg Regression Loss : 0.1156\n",
      "Avg Classification Loss : 1.3810\n",
      "\n",
      "Epoch : 132, Total Avg Loss : 1.5112\n",
      "Source Avg Loss : 0.7338\n",
      "Target Avg Loss : 0.7774\n",
      "Source Avg reg Loss : 0.0834\n",
      "Target Avg reg Loss : 0.0579\n",
      "Source Avg cla Loss : 0.6505\n",
      "Target Avg cla Loss : 0.7194\n",
      "Avg Regression Loss : 0.1413\n",
      "Avg Classification Loss : 1.3699\n",
      "\n",
      "Epoch : 133, Total Avg Loss : 1.4516\n",
      "Source Avg Loss : 0.6629\n",
      "Target Avg Loss : 0.7887\n",
      "Source Avg reg Loss : 0.0544\n",
      "Target Avg reg Loss : 0.0385\n",
      "Source Avg cla Loss : 0.6085\n",
      "Target Avg cla Loss : 0.7502\n",
      "Avg Regression Loss : 0.0929\n",
      "Avg Classification Loss : 1.3587\n",
      "\n",
      "Epoch : 134, Total Avg Loss : 1.4768\n",
      "Source Avg Loss : 0.6389\n",
      "Target Avg Loss : 0.8380\n",
      "Source Avg reg Loss : 0.0745\n",
      "Target Avg reg Loss : 0.0491\n",
      "Source Avg cla Loss : 0.5644\n",
      "Target Avg cla Loss : 0.7889\n",
      "Avg Regression Loss : 0.1236\n",
      "Avg Classification Loss : 1.3532\n",
      "\n",
      "Epoch : 135, Total Avg Loss : 1.5826\n",
      "Source Avg Loss : 0.6517\n",
      "Target Avg Loss : 0.9309\n",
      "Source Avg reg Loss : 0.0719\n",
      "Target Avg reg Loss : 0.0694\n",
      "Source Avg cla Loss : 0.5798\n",
      "Target Avg cla Loss : 0.8615\n",
      "Avg Regression Loss : 0.1413\n",
      "Avg Classification Loss : 1.4413\n",
      "\n",
      "Epoch : 136, Total Avg Loss : 1.4954\n",
      "Source Avg Loss : 0.6983\n",
      "Target Avg Loss : 0.7971\n",
      "Source Avg reg Loss : 0.0627\n",
      "Target Avg reg Loss : 0.0474\n",
      "Source Avg cla Loss : 0.6356\n",
      "Target Avg cla Loss : 0.7497\n",
      "Avg Regression Loss : 0.1101\n",
      "Avg Classification Loss : 1.3853\n",
      "\n",
      "Epoch : 137, Total Avg Loss : 1.4550\n",
      "Source Avg Loss : 0.7010\n",
      "Target Avg Loss : 0.7539\n",
      "Source Avg reg Loss : 0.0505\n",
      "Target Avg reg Loss : 0.0314\n",
      "Source Avg cla Loss : 0.6506\n",
      "Target Avg cla Loss : 0.7225\n",
      "Avg Regression Loss : 0.0819\n",
      "Avg Classification Loss : 1.3731\n",
      "\n",
      "Epoch : 138, Total Avg Loss : 1.4492\n",
      "Source Avg Loss : 0.7009\n",
      "Target Avg Loss : 0.7482\n",
      "Source Avg reg Loss : 0.0482\n",
      "Target Avg reg Loss : 0.0293\n",
      "Source Avg cla Loss : 0.6528\n",
      "Target Avg cla Loss : 0.7190\n",
      "Avg Regression Loss : 0.0775\n",
      "Avg Classification Loss : 1.3717\n",
      "\n",
      "Epoch : 139, Total Avg Loss : 1.4589\n",
      "Source Avg Loss : 0.7117\n",
      "Target Avg Loss : 0.7472\n",
      "Source Avg reg Loss : 0.0520\n",
      "Target Avg reg Loss : 0.0322\n",
      "Source Avg cla Loss : 0.6598\n",
      "Target Avg cla Loss : 0.7150\n",
      "Avg Regression Loss : 0.0842\n",
      "Avg Classification Loss : 1.3747\n",
      "\n",
      "Epoch : 140, Total Avg Loss : 1.5043\n",
      "Source Avg Loss : 0.7417\n",
      "Target Avg Loss : 0.7627\n",
      "Source Avg reg Loss : 0.0765\n",
      "Target Avg reg Loss : 0.0479\n",
      "Source Avg cla Loss : 0.6651\n",
      "Target Avg cla Loss : 0.7147\n",
      "Avg Regression Loss : 0.1245\n",
      "Avg Classification Loss : 1.3799\n",
      "\n",
      "Epoch : 141, Total Avg Loss : 1.4988\n",
      "Source Avg Loss : 0.7181\n",
      "Target Avg Loss : 0.7807\n",
      "Source Avg reg Loss : 0.0737\n",
      "Target Avg reg Loss : 0.0481\n",
      "Source Avg cla Loss : 0.6444\n",
      "Target Avg cla Loss : 0.7326\n",
      "Avg Regression Loss : 0.1218\n",
      "Avg Classification Loss : 1.3770\n",
      "\n",
      "Epoch : 142, Total Avg Loss : 1.4944\n",
      "Source Avg Loss : 0.6930\n",
      "Target Avg Loss : 0.8015\n",
      "Source Avg reg Loss : 0.0660\n",
      "Target Avg reg Loss : 0.0441\n",
      "Source Avg cla Loss : 0.6269\n",
      "Target Avg cla Loss : 0.7574\n",
      "Avg Regression Loss : 0.1101\n",
      "Avg Classification Loss : 1.3843\n",
      "\n",
      "Epoch : 143, Total Avg Loss : 1.4773\n",
      "Source Avg Loss : 0.6936\n",
      "Target Avg Loss : 0.7838\n",
      "Source Avg reg Loss : 0.0592\n",
      "Target Avg reg Loss : 0.0324\n",
      "Source Avg cla Loss : 0.6344\n",
      "Target Avg cla Loss : 0.7514\n",
      "Avg Regression Loss : 0.0916\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 144, Total Avg Loss : 1.5011\n",
      "Source Avg Loss : 0.7095\n",
      "Target Avg Loss : 0.7916\n",
      "Source Avg reg Loss : 0.0780\n",
      "Target Avg reg Loss : 0.0478\n",
      "Source Avg cla Loss : 0.6315\n",
      "Target Avg cla Loss : 0.7438\n",
      "Avg Regression Loss : 0.1258\n",
      "Avg Classification Loss : 1.3753\n",
      "\n",
      "Epoch : 145, Total Avg Loss : 1.5015\n",
      "Source Avg Loss : 0.7198\n",
      "Target Avg Loss : 0.7817\n",
      "Source Avg reg Loss : 0.0816\n",
      "Target Avg reg Loss : 0.0543\n",
      "Source Avg cla Loss : 0.6382\n",
      "Target Avg cla Loss : 0.7274\n",
      "Avg Regression Loss : 0.1359\n",
      "Avg Classification Loss : 1.3655\n",
      "\n",
      "Epoch : 146, Total Avg Loss : 1.4604\n",
      "Source Avg Loss : 0.6279\n",
      "Target Avg Loss : 0.8325\n",
      "Source Avg reg Loss : 0.0702\n",
      "Target Avg reg Loss : 0.0437\n",
      "Source Avg cla Loss : 0.5577\n",
      "Target Avg cla Loss : 0.7887\n",
      "Avg Regression Loss : 0.1140\n",
      "Avg Classification Loss : 1.3464\n",
      "\n",
      "Epoch : 147, Total Avg Loss : 1.5040\n",
      "Source Avg Loss : 0.6055\n",
      "Target Avg Loss : 0.8985\n",
      "Source Avg reg Loss : 0.0873\n",
      "Target Avg reg Loss : 0.0601\n",
      "Source Avg cla Loss : 0.5182\n",
      "Target Avg cla Loss : 0.8383\n",
      "Avg Regression Loss : 0.1474\n",
      "Avg Classification Loss : 1.3566\n",
      "\n",
      "Epoch : 148, Total Avg Loss : 1.4775\n",
      "Source Avg Loss : 0.6036\n",
      "Target Avg Loss : 0.8739\n",
      "Source Avg reg Loss : 0.0779\n",
      "Target Avg reg Loss : 0.0463\n",
      "Source Avg cla Loss : 0.5256\n",
      "Target Avg cla Loss : 0.8276\n",
      "Avg Regression Loss : 0.1242\n",
      "Avg Classification Loss : 1.3533\n",
      "\n",
      "Epoch : 149, Total Avg Loss : 1.4468\n",
      "Source Avg Loss : 0.5962\n",
      "Target Avg Loss : 0.8506\n",
      "Source Avg reg Loss : 0.0575\n",
      "Target Avg reg Loss : 0.0333\n",
      "Source Avg cla Loss : 0.5386\n",
      "Target Avg cla Loss : 0.8173\n",
      "Avg Regression Loss : 0.0908\n",
      "Avg Classification Loss : 1.3560\n",
      "\n",
      "Epoch : 150, Total Avg Loss : 1.4314\n",
      "Source Avg Loss : 0.6230\n",
      "Target Avg Loss : 0.8084\n",
      "Source Avg reg Loss : 0.0438\n",
      "Target Avg reg Loss : 0.0326\n",
      "Source Avg cla Loss : 0.5793\n",
      "Target Avg cla Loss : 0.7758\n",
      "Avg Regression Loss : 0.0764\n",
      "Avg Classification Loss : 1.3551\n",
      "\n",
      "Epoch : 151, Total Avg Loss : 1.4475\n",
      "Source Avg Loss : 0.6140\n",
      "Target Avg Loss : 0.8336\n",
      "Source Avg reg Loss : 0.0562\n",
      "Target Avg reg Loss : 0.0358\n",
      "Source Avg cla Loss : 0.5578\n",
      "Target Avg cla Loss : 0.7977\n",
      "Avg Regression Loss : 0.0920\n",
      "Avg Classification Loss : 1.3555\n",
      "\n",
      "Epoch : 152, Total Avg Loss : 1.4455\n",
      "Source Avg Loss : 0.6159\n",
      "Target Avg Loss : 0.8296\n",
      "Source Avg reg Loss : 0.0550\n",
      "Target Avg reg Loss : 0.0349\n",
      "Source Avg cla Loss : 0.5609\n",
      "Target Avg cla Loss : 0.7947\n",
      "Avg Regression Loss : 0.0899\n",
      "Avg Classification Loss : 1.3556\n",
      "\n",
      "Epoch : 153, Total Avg Loss : 1.4356\n",
      "Source Avg Loss : 0.5992\n",
      "Target Avg Loss : 0.8364\n",
      "Source Avg reg Loss : 0.0496\n",
      "Target Avg reg Loss : 0.0302\n",
      "Source Avg cla Loss : 0.5496\n",
      "Target Avg cla Loss : 0.8062\n",
      "Avg Regression Loss : 0.0798\n",
      "Avg Classification Loss : 1.3558\n",
      "\n",
      "Epoch : 154, Total Avg Loss : 1.4080\n",
      "Source Avg Loss : 0.5731\n",
      "Target Avg Loss : 0.8349\n",
      "Source Avg reg Loss : 0.0354\n",
      "Target Avg reg Loss : 0.0248\n",
      "Source Avg cla Loss : 0.5377\n",
      "Target Avg cla Loss : 0.8102\n",
      "Avg Regression Loss : 0.0602\n",
      "Avg Classification Loss : 1.3478\n",
      "\n",
      "Epoch : 155, Total Avg Loss : 1.4331\n",
      "Source Avg Loss : 0.5704\n",
      "Target Avg Loss : 0.8626\n",
      "Source Avg reg Loss : 0.0486\n",
      "Target Avg reg Loss : 0.0401\n",
      "Source Avg cla Loss : 0.5218\n",
      "Target Avg cla Loss : 0.8225\n",
      "Avg Regression Loss : 0.0887\n",
      "Avg Classification Loss : 1.3443\n",
      "\n",
      "Epoch : 156, Total Avg Loss : 1.4853\n",
      "Source Avg Loss : 0.5850\n",
      "Target Avg Loss : 0.9003\n",
      "Source Avg reg Loss : 0.0780\n",
      "Target Avg reg Loss : 0.0532\n",
      "Source Avg cla Loss : 0.5070\n",
      "Target Avg cla Loss : 0.8471\n",
      "Avg Regression Loss : 0.1313\n",
      "Avg Classification Loss : 1.3540\n",
      "\n",
      "Epoch : 157, Total Avg Loss : 1.4513\n",
      "Source Avg Loss : 0.5726\n",
      "Target Avg Loss : 0.8786\n",
      "Source Avg reg Loss : 0.0630\n",
      "Target Avg reg Loss : 0.0393\n",
      "Source Avg cla Loss : 0.5096\n",
      "Target Avg cla Loss : 0.8393\n",
      "Avg Regression Loss : 0.1023\n",
      "Avg Classification Loss : 1.3490\n",
      "\n",
      "Epoch : 158, Total Avg Loss : 1.4451\n",
      "Source Avg Loss : 0.5814\n",
      "Target Avg Loss : 0.8637\n",
      "Source Avg reg Loss : 0.0538\n",
      "Target Avg reg Loss : 0.0414\n",
      "Source Avg cla Loss : 0.5276\n",
      "Target Avg cla Loss : 0.8223\n",
      "Avg Regression Loss : 0.0952\n",
      "Avg Classification Loss : 1.3499\n",
      "\n",
      "Epoch : 159, Total Avg Loss : 1.4485\n",
      "Source Avg Loss : 0.5865\n",
      "Target Avg Loss : 0.8621\n",
      "Source Avg reg Loss : 0.0592\n",
      "Target Avg reg Loss : 0.0393\n",
      "Source Avg cla Loss : 0.5272\n",
      "Target Avg cla Loss : 0.8227\n",
      "Avg Regression Loss : 0.0985\n",
      "Avg Classification Loss : 1.3500\n",
      "\n",
      "Epoch : 160, Total Avg Loss : 1.4859\n",
      "Source Avg Loss : 0.5925\n",
      "Target Avg Loss : 0.8934\n",
      "Source Avg reg Loss : 0.0786\n",
      "Target Avg reg Loss : 0.0648\n",
      "Source Avg cla Loss : 0.5139\n",
      "Target Avg cla Loss : 0.8285\n",
      "Avg Regression Loss : 0.1434\n",
      "Avg Classification Loss : 1.3424\n",
      "\n",
      "Epoch : 161, Total Avg Loss : 1.4308\n",
      "Source Avg Loss : 0.5228\n",
      "Target Avg Loss : 0.9080\n",
      "Source Avg reg Loss : 0.0532\n",
      "Target Avg reg Loss : 0.0403\n",
      "Source Avg cla Loss : 0.4696\n",
      "Target Avg cla Loss : 0.8677\n",
      "Avg Regression Loss : 0.0935\n",
      "Avg Classification Loss : 1.3373\n",
      "\n",
      "Epoch : 162, Total Avg Loss : 1.4834\n",
      "Source Avg Loss : 0.5973\n",
      "Target Avg Loss : 0.8861\n",
      "Source Avg reg Loss : 0.0787\n",
      "Target Avg reg Loss : 0.0502\n",
      "Source Avg cla Loss : 0.5187\n",
      "Target Avg cla Loss : 0.8359\n",
      "Avg Regression Loss : 0.1288\n",
      "Avg Classification Loss : 1.3546\n",
      "\n",
      "Epoch : 163, Total Avg Loss : 1.4399\n",
      "Source Avg Loss : 0.5846\n",
      "Target Avg Loss : 0.8553\n",
      "Source Avg reg Loss : 0.0540\n",
      "Target Avg reg Loss : 0.0366\n",
      "Source Avg cla Loss : 0.5306\n",
      "Target Avg cla Loss : 0.8187\n",
      "Avg Regression Loss : 0.0906\n",
      "Avg Classification Loss : 1.3492\n",
      "\n",
      "Epoch : 164, Total Avg Loss : 1.4175\n",
      "Source Avg Loss : 0.5505\n",
      "Target Avg Loss : 0.8669\n",
      "Source Avg reg Loss : 0.0379\n",
      "Target Avg reg Loss : 0.0296\n",
      "Source Avg cla Loss : 0.5126\n",
      "Target Avg cla Loss : 0.8374\n",
      "Avg Regression Loss : 0.0675\n",
      "Avg Classification Loss : 1.3500\n",
      "\n",
      "Epoch : 165, Total Avg Loss : 1.4320\n",
      "Source Avg Loss : 0.5952\n",
      "Target Avg Loss : 0.8368\n",
      "Source Avg reg Loss : 0.0478\n",
      "Target Avg reg Loss : 0.0314\n",
      "Source Avg cla Loss : 0.5474\n",
      "Target Avg cla Loss : 0.8054\n",
      "Avg Regression Loss : 0.0792\n",
      "Avg Classification Loss : 1.3528\n",
      "\n",
      "Epoch : 166, Total Avg Loss : 1.4042\n",
      "Source Avg Loss : 0.5483\n",
      "Target Avg Loss : 0.8559\n",
      "Source Avg reg Loss : 0.0361\n",
      "Target Avg reg Loss : 0.0268\n",
      "Source Avg cla Loss : 0.5122\n",
      "Target Avg cla Loss : 0.8291\n",
      "Avg Regression Loss : 0.0629\n",
      "Avg Classification Loss : 1.3413\n",
      "\n",
      "Epoch : 167, Total Avg Loss : 1.4373\n",
      "Source Avg Loss : 0.5527\n",
      "Target Avg Loss : 0.8846\n",
      "Source Avg reg Loss : 0.0550\n",
      "Target Avg reg Loss : 0.0349\n",
      "Source Avg cla Loss : 0.4976\n",
      "Target Avg cla Loss : 0.8498\n",
      "Avg Regression Loss : 0.0899\n",
      "Avg Classification Loss : 1.3474\n",
      "\n",
      "Epoch : 168, Total Avg Loss : 1.4155\n",
      "Source Avg Loss : 0.5523\n",
      "Target Avg Loss : 0.8632\n",
      "Source Avg reg Loss : 0.0406\n",
      "Target Avg reg Loss : 0.0291\n",
      "Source Avg cla Loss : 0.5118\n",
      "Target Avg cla Loss : 0.8341\n",
      "Avg Regression Loss : 0.0696\n",
      "Avg Classification Loss : 1.3459\n",
      "\n",
      "Epoch : 169, Total Avg Loss : 1.4126\n",
      "Source Avg Loss : 0.5801\n",
      "Target Avg Loss : 0.8325\n",
      "Source Avg reg Loss : 0.0357\n",
      "Target Avg reg Loss : 0.0255\n",
      "Source Avg cla Loss : 0.5444\n",
      "Target Avg cla Loss : 0.8070\n",
      "Avg Regression Loss : 0.0612\n",
      "Avg Classification Loss : 1.3514\n",
      "\n",
      "Epoch : 170, Total Avg Loss : 1.4625\n",
      "Source Avg Loss : 0.6052\n",
      "Target Avg Loss : 0.8573\n",
      "Source Avg reg Loss : 0.0642\n",
      "Target Avg reg Loss : 0.0416\n",
      "Source Avg cla Loss : 0.5409\n",
      "Target Avg cla Loss : 0.8157\n",
      "Avg Regression Loss : 0.1059\n",
      "Avg Classification Loss : 1.3567\n",
      "\n",
      "Epoch : 171, Total Avg Loss : 1.4377\n",
      "Source Avg Loss : 0.5823\n",
      "Target Avg Loss : 0.8554\n",
      "Source Avg reg Loss : 0.0521\n",
      "Target Avg reg Loss : 0.0373\n",
      "Source Avg cla Loss : 0.5302\n",
      "Target Avg cla Loss : 0.8181\n",
      "Avg Regression Loss : 0.0895\n",
      "Avg Classification Loss : 1.3483\n",
      "\n",
      "Epoch : 172, Total Avg Loss : 1.4657\n",
      "Source Avg Loss : 0.5871\n",
      "Target Avg Loss : 0.8786\n",
      "Source Avg reg Loss : 0.0714\n",
      "Target Avg reg Loss : 0.0584\n",
      "Source Avg cla Loss : 0.5157\n",
      "Target Avg cla Loss : 0.8203\n",
      "Avg Regression Loss : 0.1298\n",
      "Avg Classification Loss : 1.3359\n",
      "\n",
      "Epoch : 173, Total Avg Loss : 1.4460\n",
      "Source Avg Loss : 0.5353\n",
      "Target Avg Loss : 0.9107\n",
      "Source Avg reg Loss : 0.0656\n",
      "Target Avg reg Loss : 0.0460\n",
      "Source Avg cla Loss : 0.4697\n",
      "Target Avg cla Loss : 0.8647\n",
      "Avg Regression Loss : 0.1116\n",
      "Avg Classification Loss : 1.3344\n",
      "\n",
      "Epoch : 174, Total Avg Loss : 1.4705\n",
      "Source Avg Loss : 0.5540\n",
      "Target Avg Loss : 0.9165\n",
      "Source Avg reg Loss : 0.0738\n",
      "Target Avg reg Loss : 0.0463\n",
      "Source Avg cla Loss : 0.4801\n",
      "Target Avg cla Loss : 0.8703\n",
      "Avg Regression Loss : 0.1201\n",
      "Avg Classification Loss : 1.3504\n",
      "\n",
      "Epoch : 175, Total Avg Loss : 1.4411\n",
      "Source Avg Loss : 0.5669\n",
      "Target Avg Loss : 0.8742\n",
      "Source Avg reg Loss : 0.0561\n",
      "Target Avg reg Loss : 0.0426\n",
      "Source Avg cla Loss : 0.5109\n",
      "Target Avg cla Loss : 0.8316\n",
      "Avg Regression Loss : 0.0986\n",
      "Avg Classification Loss : 1.3425\n",
      "\n",
      "Epoch : 176, Total Avg Loss : 1.4724\n",
      "Source Avg Loss : 0.5539\n",
      "Target Avg Loss : 0.9185\n",
      "Source Avg reg Loss : 0.0709\n",
      "Target Avg reg Loss : 0.0474\n",
      "Source Avg cla Loss : 0.4830\n",
      "Target Avg cla Loss : 0.8711\n",
      "Avg Regression Loss : 0.1183\n",
      "Avg Classification Loss : 1.3541\n",
      "\n",
      "Epoch : 177, Total Avg Loss : 1.4579\n",
      "Source Avg Loss : 0.5669\n",
      "Target Avg Loss : 0.8910\n",
      "Source Avg reg Loss : 0.0574\n",
      "Target Avg reg Loss : 0.0452\n",
      "Source Avg cla Loss : 0.5095\n",
      "Target Avg cla Loss : 0.8458\n",
      "Avg Regression Loss : 0.1026\n",
      "Avg Classification Loss : 1.3553\n",
      "\n",
      "Epoch : 178, Total Avg Loss : 1.4831\n",
      "Source Avg Loss : 0.5698\n",
      "Target Avg Loss : 0.9133\n",
      "Source Avg reg Loss : 0.0769\n",
      "Target Avg reg Loss : 0.0484\n",
      "Source Avg cla Loss : 0.4930\n",
      "Target Avg cla Loss : 0.8648\n",
      "Avg Regression Loss : 0.1253\n",
      "Avg Classification Loss : 1.3578\n",
      "\n",
      "Epoch : 179, Total Avg Loss : 1.5111\n",
      "Source Avg Loss : 0.5803\n",
      "Target Avg Loss : 0.9308\n",
      "Source Avg reg Loss : 0.0741\n",
      "Target Avg reg Loss : 0.0685\n",
      "Source Avg cla Loss : 0.5061\n",
      "Target Avg cla Loss : 0.8623\n",
      "Avg Regression Loss : 0.1426\n",
      "Avg Classification Loss : 1.3685\n",
      "\n",
      "Epoch : 180, Total Avg Loss : 1.4666\n",
      "Source Avg Loss : 0.5634\n",
      "Target Avg Loss : 0.9032\n",
      "Source Avg reg Loss : 0.0709\n",
      "Target Avg reg Loss : 0.0409\n",
      "Source Avg cla Loss : 0.4925\n",
      "Target Avg cla Loss : 0.8622\n",
      "Avg Regression Loss : 0.1119\n",
      "Avg Classification Loss : 1.3547\n",
      "\n",
      "Epoch : 181, Total Avg Loss : 1.4265\n",
      "Source Avg Loss : 0.5021\n",
      "Target Avg Loss : 0.9244\n",
      "Source Avg reg Loss : 0.0430\n",
      "Target Avg reg Loss : 0.0355\n",
      "Source Avg cla Loss : 0.4591\n",
      "Target Avg cla Loss : 0.8889\n",
      "Avg Regression Loss : 0.0785\n",
      "Avg Classification Loss : 1.3480\n",
      "\n",
      "Epoch : 182, Total Avg Loss : 1.4423\n",
      "Source Avg Loss : 0.5884\n",
      "Target Avg Loss : 0.8539\n",
      "Source Avg reg Loss : 0.0449\n",
      "Target Avg reg Loss : 0.0286\n",
      "Source Avg cla Loss : 0.5435\n",
      "Target Avg cla Loss : 0.8253\n",
      "Avg Regression Loss : 0.0734\n",
      "Avg Classification Loss : 1.3688\n",
      "\n",
      "Epoch : 183, Total Avg Loss : 1.4535\n",
      "Source Avg Loss : 0.5672\n",
      "Target Avg Loss : 0.8863\n",
      "Source Avg reg Loss : 0.0487\n",
      "Target Avg reg Loss : 0.0311\n",
      "Source Avg cla Loss : 0.5185\n",
      "Target Avg cla Loss : 0.8552\n",
      "Avg Regression Loss : 0.0799\n",
      "Avg Classification Loss : 1.3736\n",
      "\n",
      "Epoch : 184, Total Avg Loss : 1.4574\n",
      "Source Avg Loss : 0.5704\n",
      "Target Avg Loss : 0.8871\n",
      "Source Avg reg Loss : 0.0514\n",
      "Target Avg reg Loss : 0.0331\n",
      "Source Avg cla Loss : 0.5190\n",
      "Target Avg cla Loss : 0.8540\n",
      "Avg Regression Loss : 0.0845\n",
      "Avg Classification Loss : 1.3730\n",
      "\n",
      "Epoch : 185, Total Avg Loss : 1.4701\n",
      "Source Avg Loss : 0.6163\n",
      "Target Avg Loss : 0.8537\n",
      "Source Avg reg Loss : 0.0492\n",
      "Target Avg reg Loss : 0.0315\n",
      "Source Avg cla Loss : 0.5671\n",
      "Target Avg cla Loss : 0.8223\n",
      "Avg Regression Loss : 0.0807\n",
      "Avg Classification Loss : 1.3894\n",
      "\n",
      "Epoch : 186, Total Avg Loss : 1.5035\n",
      "Source Avg Loss : 0.6665\n",
      "Target Avg Loss : 0.8370\n",
      "Source Avg reg Loss : 0.0621\n",
      "Target Avg reg Loss : 0.0386\n",
      "Source Avg cla Loss : 0.6044\n",
      "Target Avg cla Loss : 0.7984\n",
      "Avg Regression Loss : 0.1006\n",
      "Avg Classification Loss : 1.4028\n",
      "\n",
      "Epoch : 187, Total Avg Loss : 1.4814\n",
      "Source Avg Loss : 0.6662\n",
      "Target Avg Loss : 0.8152\n",
      "Source Avg reg Loss : 0.0509\n",
      "Target Avg reg Loss : 0.0345\n",
      "Source Avg cla Loss : 0.6153\n",
      "Target Avg cla Loss : 0.7807\n",
      "Avg Regression Loss : 0.0854\n",
      "Avg Classification Loss : 1.3960\n",
      "\n",
      "Epoch : 188, Total Avg Loss : 1.4514\n",
      "Source Avg Loss : 0.6469\n",
      "Target Avg Loss : 0.8045\n",
      "Source Avg reg Loss : 0.0414\n",
      "Target Avg reg Loss : 0.0256\n",
      "Source Avg cla Loss : 0.6055\n",
      "Target Avg cla Loss : 0.7789\n",
      "Avg Regression Loss : 0.0670\n",
      "Avg Classification Loss : 1.3844\n",
      "\n",
      "Epoch : 189, Total Avg Loss : 1.4781\n",
      "Source Avg Loss : 0.6665\n",
      "Target Avg Loss : 0.8116\n",
      "Source Avg reg Loss : 0.0453\n",
      "Target Avg reg Loss : 0.0278\n",
      "Source Avg cla Loss : 0.6212\n",
      "Target Avg cla Loss : 0.7838\n",
      "Avg Regression Loss : 0.0731\n",
      "Avg Classification Loss : 1.4051\n",
      "\n",
      "Epoch : 190, Total Avg Loss : 1.4599\n",
      "Source Avg Loss : 0.6629\n",
      "Target Avg Loss : 0.7970\n",
      "Source Avg reg Loss : 0.0432\n",
      "Target Avg reg Loss : 0.0240\n",
      "Source Avg cla Loss : 0.6197\n",
      "Target Avg cla Loss : 0.7730\n",
      "Avg Regression Loss : 0.0672\n",
      "Avg Classification Loss : 1.3927\n",
      "\n",
      "Epoch : 191, Total Avg Loss : 1.4540\n",
      "Source Avg Loss : 0.6264\n",
      "Target Avg Loss : 0.8275\n",
      "Source Avg reg Loss : 0.0478\n",
      "Target Avg reg Loss : 0.0294\n",
      "Source Avg cla Loss : 0.5786\n",
      "Target Avg cla Loss : 0.7982\n",
      "Avg Regression Loss : 0.0771\n",
      "Avg Classification Loss : 1.3768\n",
      "\n",
      "Epoch : 192, Total Avg Loss : 1.5058\n",
      "Source Avg Loss : 0.6301\n",
      "Target Avg Loss : 0.8757\n",
      "Source Avg reg Loss : 0.0678\n",
      "Target Avg reg Loss : 0.0403\n",
      "Source Avg cla Loss : 0.5623\n",
      "Target Avg cla Loss : 0.8354\n",
      "Avg Regression Loss : 0.1081\n",
      "Avg Classification Loss : 1.3977\n",
      "\n",
      "Epoch : 193, Total Avg Loss : 1.5916\n",
      "Source Avg Loss : 0.6549\n",
      "Target Avg Loss : 0.9367\n",
      "Source Avg reg Loss : 0.0938\n",
      "Target Avg reg Loss : 0.1045\n",
      "Source Avg cla Loss : 0.5611\n",
      "Target Avg cla Loss : 0.8322\n",
      "Avg Regression Loss : 0.1983\n",
      "Avg Classification Loss : 1.3934\n",
      "\n",
      "Epoch : 194, Total Avg Loss : 1.5121\n",
      "Source Avg Loss : 0.6607\n",
      "Target Avg Loss : 0.8514\n",
      "Source Avg reg Loss : 0.0692\n",
      "Target Avg reg Loss : 0.0418\n",
      "Source Avg cla Loss : 0.5915\n",
      "Target Avg cla Loss : 0.8095\n",
      "Avg Regression Loss : 0.1110\n",
      "Avg Classification Loss : 1.4010\n",
      "\n",
      "Epoch : 195, Total Avg Loss : 1.5155\n",
      "Source Avg Loss : 0.6381\n",
      "Target Avg Loss : 0.8774\n",
      "Source Avg reg Loss : 0.0633\n",
      "Target Avg reg Loss : 0.0331\n",
      "Source Avg cla Loss : 0.5749\n",
      "Target Avg cla Loss : 0.8444\n",
      "Avg Regression Loss : 0.0963\n",
      "Avg Classification Loss : 1.4192\n",
      "\n",
      "Epoch : 196, Total Avg Loss : 1.5118\n",
      "Source Avg Loss : 0.6550\n",
      "Target Avg Loss : 0.8568\n",
      "Source Avg reg Loss : 0.0508\n",
      "Target Avg reg Loss : 0.0288\n",
      "Source Avg cla Loss : 0.6042\n",
      "Target Avg cla Loss : 0.8280\n",
      "Avg Regression Loss : 0.0796\n",
      "Avg Classification Loss : 1.4322\n",
      "\n",
      "Epoch : 197, Total Avg Loss : 1.4900\n",
      "Source Avg Loss : 0.7158\n",
      "Target Avg Loss : 0.7742\n",
      "Source Avg reg Loss : 0.0535\n",
      "Target Avg reg Loss : 0.0292\n",
      "Source Avg cla Loss : 0.6623\n",
      "Target Avg cla Loss : 0.7450\n",
      "Avg Regression Loss : 0.0826\n",
      "Avg Classification Loss : 1.4073\n",
      "\n",
      "Epoch : 198, Total Avg Loss : 1.4603\n",
      "Source Avg Loss : 0.7160\n",
      "Target Avg Loss : 0.7443\n",
      "Source Avg reg Loss : 0.0404\n",
      "Target Avg reg Loss : 0.0306\n",
      "Source Avg cla Loss : 0.6755\n",
      "Target Avg cla Loss : 0.7137\n",
      "Avg Regression Loss : 0.0711\n",
      "Avg Classification Loss : 1.3893\n",
      "\n",
      "Epoch : 199, Total Avg Loss : 1.4521\n",
      "Source Avg Loss : 0.7178\n",
      "Target Avg Loss : 0.7342\n",
      "Source Avg reg Loss : 0.0395\n",
      "Target Avg reg Loss : 0.0244\n",
      "Source Avg cla Loss : 0.6784\n",
      "Target Avg cla Loss : 0.7098\n",
      "Avg Regression Loss : 0.0639\n",
      "Avg Classification Loss : 1.3882\n",
      "\n",
      "Epoch : 200, Total Avg Loss : 1.4548\n",
      "Source Avg Loss : 0.7166\n",
      "Target Avg Loss : 0.7382\n",
      "Source Avg reg Loss : 0.0405\n",
      "Target Avg reg Loss : 0.0285\n",
      "Source Avg cla Loss : 0.6761\n",
      "Target Avg cla Loss : 0.7097\n",
      "Avg Regression Loss : 0.0689\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 201, Total Avg Loss : 1.4381\n",
      "Source Avg Loss : 0.7038\n",
      "Target Avg Loss : 0.7344\n",
      "Source Avg reg Loss : 0.0344\n",
      "Target Avg reg Loss : 0.0220\n",
      "Source Avg cla Loss : 0.6694\n",
      "Target Avg cla Loss : 0.7124\n",
      "Avg Regression Loss : 0.0564\n",
      "Avg Classification Loss : 1.3818\n",
      "\n",
      "Epoch : 202, Total Avg Loss : 1.4423\n",
      "Source Avg Loss : 0.6968\n",
      "Target Avg Loss : 0.7454\n",
      "Source Avg reg Loss : 0.0410\n",
      "Target Avg reg Loss : 0.0262\n",
      "Source Avg cla Loss : 0.6558\n",
      "Target Avg cla Loss : 0.7192\n",
      "Avg Regression Loss : 0.0673\n",
      "Avg Classification Loss : 1.3750\n",
      "\n",
      "Epoch : 203, Total Avg Loss : 1.4212\n",
      "Source Avg Loss : 0.6426\n",
      "Target Avg Loss : 0.7787\n",
      "Source Avg reg Loss : 0.0486\n",
      "Target Avg reg Loss : 0.0326\n",
      "Source Avg cla Loss : 0.5940\n",
      "Target Avg cla Loss : 0.7460\n",
      "Avg Regression Loss : 0.0812\n",
      "Avg Classification Loss : 1.3400\n",
      "\n",
      "Epoch : 204, Total Avg Loss : 1.4493\n",
      "Source Avg Loss : 0.6124\n",
      "Target Avg Loss : 0.8369\n",
      "Source Avg reg Loss : 0.0542\n",
      "Target Avg reg Loss : 0.0384\n",
      "Source Avg cla Loss : 0.5582\n",
      "Target Avg cla Loss : 0.7985\n",
      "Avg Regression Loss : 0.0926\n",
      "Avg Classification Loss : 1.3567\n",
      "\n",
      "Epoch : 205, Total Avg Loss : 1.4487\n",
      "Source Avg Loss : 0.6193\n",
      "Target Avg Loss : 0.8294\n",
      "Source Avg reg Loss : 0.0481\n",
      "Target Avg reg Loss : 0.0363\n",
      "Source Avg cla Loss : 0.5713\n",
      "Target Avg cla Loss : 0.7931\n",
      "Avg Regression Loss : 0.0843\n",
      "Avg Classification Loss : 1.3644\n",
      "\n",
      "Epoch : 206, Total Avg Loss : 1.4501\n",
      "Source Avg Loss : 0.6209\n",
      "Target Avg Loss : 0.8293\n",
      "Source Avg reg Loss : 0.0434\n",
      "Target Avg reg Loss : 0.0319\n",
      "Source Avg cla Loss : 0.5775\n",
      "Target Avg cla Loss : 0.7974\n",
      "Avg Regression Loss : 0.0753\n",
      "Avg Classification Loss : 1.3749\n",
      "\n",
      "Epoch : 207, Total Avg Loss : 1.4551\n",
      "Source Avg Loss : 0.6383\n",
      "Target Avg Loss : 0.8167\n",
      "Source Avg reg Loss : 0.0420\n",
      "Target Avg reg Loss : 0.0262\n",
      "Source Avg cla Loss : 0.5964\n",
      "Target Avg cla Loss : 0.7905\n",
      "Avg Regression Loss : 0.0682\n",
      "Avg Classification Loss : 1.3869\n",
      "\n",
      "Epoch : 208, Total Avg Loss : 1.4706\n",
      "Source Avg Loss : 0.6662\n",
      "Target Avg Loss : 0.8044\n",
      "Source Avg reg Loss : 0.0497\n",
      "Target Avg reg Loss : 0.0268\n",
      "Source Avg cla Loss : 0.6165\n",
      "Target Avg cla Loss : 0.7776\n",
      "Avg Regression Loss : 0.0765\n",
      "Avg Classification Loss : 1.3941\n",
      "\n",
      "Epoch : 209, Total Avg Loss : 1.4819\n",
      "Source Avg Loss : 0.6626\n",
      "Target Avg Loss : 0.8192\n",
      "Source Avg reg Loss : 0.0523\n",
      "Target Avg reg Loss : 0.0338\n",
      "Source Avg cla Loss : 0.6103\n",
      "Target Avg cla Loss : 0.7854\n",
      "Avg Regression Loss : 0.0861\n",
      "Avg Classification Loss : 1.3958\n",
      "\n",
      "Epoch : 210, Total Avg Loss : 1.4508\n",
      "Source Avg Loss : 0.6539\n",
      "Target Avg Loss : 0.7969\n",
      "Source Avg reg Loss : 0.0355\n",
      "Target Avg reg Loss : 0.0263\n",
      "Source Avg cla Loss : 0.6184\n",
      "Target Avg cla Loss : 0.7707\n",
      "Avg Regression Loss : 0.0618\n",
      "Avg Classification Loss : 1.3890\n",
      "\n",
      "Epoch : 211, Total Avg Loss : 1.4560\n",
      "Source Avg Loss : 0.6685\n",
      "Target Avg Loss : 0.7875\n",
      "Source Avg reg Loss : 0.0426\n",
      "Target Avg reg Loss : 0.0315\n",
      "Source Avg cla Loss : 0.6259\n",
      "Target Avg cla Loss : 0.7559\n",
      "Avg Regression Loss : 0.0741\n",
      "Avg Classification Loss : 1.3819\n",
      "\n",
      "Epoch : 212, Total Avg Loss : 1.4504\n",
      "Source Avg Loss : 0.6734\n",
      "Target Avg Loss : 0.7771\n",
      "Source Avg reg Loss : 0.0431\n",
      "Target Avg reg Loss : 0.0253\n",
      "Source Avg cla Loss : 0.6303\n",
      "Target Avg cla Loss : 0.7518\n",
      "Avg Regression Loss : 0.0683\n",
      "Avg Classification Loss : 1.3821\n",
      "\n",
      "Epoch : 213, Total Avg Loss : 1.4589\n",
      "Source Avg Loss : 0.6822\n",
      "Target Avg Loss : 0.7768\n",
      "Source Avg reg Loss : 0.0445\n",
      "Target Avg reg Loss : 0.0248\n",
      "Source Avg cla Loss : 0.6377\n",
      "Target Avg cla Loss : 0.7520\n",
      "Avg Regression Loss : 0.0693\n",
      "Avg Classification Loss : 1.3896\n",
      "\n",
      "Epoch : 214, Total Avg Loss : 1.4531\n",
      "Source Avg Loss : 0.6888\n",
      "Target Avg Loss : 0.7642\n",
      "Source Avg reg Loss : 0.0398\n",
      "Target Avg reg Loss : 0.0227\n",
      "Source Avg cla Loss : 0.6491\n",
      "Target Avg cla Loss : 0.7415\n",
      "Avg Regression Loss : 0.0625\n",
      "Avg Classification Loss : 1.3906\n",
      "\n",
      "Epoch : 215, Total Avg Loss : 1.4512\n",
      "Source Avg Loss : 0.6885\n",
      "Target Avg Loss : 0.7628\n",
      "Source Avg reg Loss : 0.0345\n",
      "Target Avg reg Loss : 0.0251\n",
      "Source Avg cla Loss : 0.6540\n",
      "Target Avg cla Loss : 0.7377\n",
      "Avg Regression Loss : 0.0596\n",
      "Avg Classification Loss : 1.3917\n",
      "\n",
      "Epoch : 216, Total Avg Loss : 1.4731\n",
      "Source Avg Loss : 0.7085\n",
      "Target Avg Loss : 0.7646\n",
      "Source Avg reg Loss : 0.0433\n",
      "Target Avg reg Loss : 0.0315\n",
      "Source Avg cla Loss : 0.6652\n",
      "Target Avg cla Loss : 0.7330\n",
      "Avg Regression Loss : 0.0748\n",
      "Avg Classification Loss : 1.3983\n",
      "\n",
      "Epoch : 217, Total Avg Loss : 1.4657\n",
      "Source Avg Loss : 0.7071\n",
      "Target Avg Loss : 0.7586\n",
      "Source Avg reg Loss : 0.0374\n",
      "Target Avg reg Loss : 0.0276\n",
      "Source Avg cla Loss : 0.6696\n",
      "Target Avg cla Loss : 0.7310\n",
      "Avg Regression Loss : 0.0650\n",
      "Avg Classification Loss : 1.4007\n",
      "\n",
      "Epoch : 218, Total Avg Loss : 1.4586\n",
      "Source Avg Loss : 0.7073\n",
      "Target Avg Loss : 0.7513\n",
      "Source Avg reg Loss : 0.0351\n",
      "Target Avg reg Loss : 0.0250\n",
      "Source Avg cla Loss : 0.6722\n",
      "Target Avg cla Loss : 0.7264\n",
      "Avg Regression Loss : 0.0601\n",
      "Avg Classification Loss : 1.3986\n",
      "\n",
      "Epoch : 219, Total Avg Loss : 1.4537\n",
      "Source Avg Loss : 0.7118\n",
      "Target Avg Loss : 0.7419\n",
      "Source Avg reg Loss : 0.0351\n",
      "Target Avg reg Loss : 0.0245\n",
      "Source Avg cla Loss : 0.6766\n",
      "Target Avg cla Loss : 0.7175\n",
      "Avg Regression Loss : 0.0596\n",
      "Avg Classification Loss : 1.3941\n",
      "\n",
      "Epoch : 220, Total Avg Loss : 1.4538\n",
      "Source Avg Loss : 0.7162\n",
      "Target Avg Loss : 0.7376\n",
      "Source Avg reg Loss : 0.0382\n",
      "Target Avg reg Loss : 0.0252\n",
      "Source Avg cla Loss : 0.6780\n",
      "Target Avg cla Loss : 0.7124\n",
      "Avg Regression Loss : 0.0634\n",
      "Avg Classification Loss : 1.3903\n",
      "\n",
      "Epoch : 221, Total Avg Loss : 1.4478\n",
      "Source Avg Loss : 0.7112\n",
      "Target Avg Loss : 0.7366\n",
      "Source Avg reg Loss : 0.0341\n",
      "Target Avg reg Loss : 0.0276\n",
      "Source Avg cla Loss : 0.6771\n",
      "Target Avg cla Loss : 0.7090\n",
      "Avg Regression Loss : 0.0617\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 222, Total Avg Loss : 1.4608\n",
      "Source Avg Loss : 0.7141\n",
      "Target Avg Loss : 0.7467\n",
      "Source Avg reg Loss : 0.0507\n",
      "Target Avg reg Loss : 0.0302\n",
      "Source Avg cla Loss : 0.6634\n",
      "Target Avg cla Loss : 0.7165\n",
      "Avg Regression Loss : 0.0809\n",
      "Avg Classification Loss : 1.3799\n",
      "\n",
      "Epoch : 223, Total Avg Loss : 1.4637\n",
      "Source Avg Loss : 0.6970\n",
      "Target Avg Loss : 0.7667\n",
      "Source Avg reg Loss : 0.0449\n",
      "Target Avg reg Loss : 0.0346\n",
      "Source Avg cla Loss : 0.6522\n",
      "Target Avg cla Loss : 0.7321\n",
      "Avg Regression Loss : 0.0795\n",
      "Avg Classification Loss : 1.3842\n",
      "\n",
      "Epoch : 224, Total Avg Loss : 1.4482\n",
      "Source Avg Loss : 0.6755\n",
      "Target Avg Loss : 0.7727\n",
      "Source Avg reg Loss : 0.0370\n",
      "Target Avg reg Loss : 0.0274\n",
      "Source Avg cla Loss : 0.6385\n",
      "Target Avg cla Loss : 0.7453\n",
      "Avg Regression Loss : 0.0644\n",
      "Avg Classification Loss : 1.3838\n",
      "\n",
      "Epoch : 225, Total Avg Loss : 1.4335\n",
      "Source Avg Loss : 0.6497\n",
      "Target Avg Loss : 0.7838\n",
      "Source Avg reg Loss : 0.0279\n",
      "Target Avg reg Loss : 0.0196\n",
      "Source Avg cla Loss : 0.6218\n",
      "Target Avg cla Loss : 0.7641\n",
      "Avg Regression Loss : 0.0476\n",
      "Avg Classification Loss : 1.3859\n",
      "\n",
      "Epoch : 226, Total Avg Loss : 1.4270\n",
      "Source Avg Loss : 0.6538\n",
      "Target Avg Loss : 0.7731\n",
      "Source Avg reg Loss : 0.0273\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.6266\n",
      "Target Avg cla Loss : 0.7533\n",
      "Avg Regression Loss : 0.0471\n",
      "Avg Classification Loss : 1.3799\n",
      "\n",
      "Epoch : 227, Total Avg Loss : 1.4358\n",
      "Source Avg Loss : 0.6697\n",
      "Target Avg Loss : 0.7661\n",
      "Source Avg reg Loss : 0.0295\n",
      "Target Avg reg Loss : 0.0236\n",
      "Source Avg cla Loss : 0.6402\n",
      "Target Avg cla Loss : 0.7425\n",
      "Avg Regression Loss : 0.0531\n",
      "Avg Classification Loss : 1.3827\n",
      "\n",
      "Epoch : 228, Total Avg Loss : 1.4520\n",
      "Source Avg Loss : 0.6945\n",
      "Target Avg Loss : 0.7574\n",
      "Source Avg reg Loss : 0.0366\n",
      "Target Avg reg Loss : 0.0260\n",
      "Source Avg cla Loss : 0.6579\n",
      "Target Avg cla Loss : 0.7314\n",
      "Avg Regression Loss : 0.0626\n",
      "Avg Classification Loss : 1.3894\n",
      "\n",
      "Epoch : 229, Total Avg Loss : 1.4183\n",
      "Source Avg Loss : 0.6516\n",
      "Target Avg Loss : 0.7666\n",
      "Source Avg reg Loss : 0.0378\n",
      "Target Avg reg Loss : 0.0240\n",
      "Source Avg cla Loss : 0.6139\n",
      "Target Avg cla Loss : 0.7426\n",
      "Avg Regression Loss : 0.0617\n",
      "Avg Classification Loss : 1.3565\n",
      "\n",
      "Epoch : 230, Total Avg Loss : 1.4453\n",
      "Source Avg Loss : 0.6070\n",
      "Target Avg Loss : 0.8383\n",
      "Source Avg reg Loss : 0.0427\n",
      "Target Avg reg Loss : 0.0321\n",
      "Source Avg cla Loss : 0.5643\n",
      "Target Avg cla Loss : 0.8062\n",
      "Avg Regression Loss : 0.0748\n",
      "Avg Classification Loss : 1.3705\n",
      "\n",
      "Epoch : 231, Total Avg Loss : 1.4715\n",
      "Source Avg Loss : 0.6109\n",
      "Target Avg Loss : 0.8606\n",
      "Source Avg reg Loss : 0.0559\n",
      "Target Avg reg Loss : 0.0360\n",
      "Source Avg cla Loss : 0.5550\n",
      "Target Avg cla Loss : 0.8246\n",
      "Avg Regression Loss : 0.0919\n",
      "Avg Classification Loss : 1.3796\n",
      "\n",
      "Epoch : 232, Total Avg Loss : 1.4742\n",
      "Source Avg Loss : 0.6463\n",
      "Target Avg Loss : 0.8279\n",
      "Source Avg reg Loss : 0.0455\n",
      "Target Avg reg Loss : 0.0379\n",
      "Source Avg cla Loss : 0.6008\n",
      "Target Avg cla Loss : 0.7901\n",
      "Avg Regression Loss : 0.0834\n",
      "Avg Classification Loss : 1.3909\n",
      "\n",
      "Epoch : 233, Total Avg Loss : 1.4469\n",
      "Source Avg Loss : 0.6531\n",
      "Target Avg Loss : 0.7938\n",
      "Source Avg reg Loss : 0.0374\n",
      "Target Avg reg Loss : 0.0278\n",
      "Source Avg cla Loss : 0.6157\n",
      "Target Avg cla Loss : 0.7660\n",
      "Avg Regression Loss : 0.0652\n",
      "Avg Classification Loss : 1.3817\n",
      "\n",
      "Epoch : 234, Total Avg Loss : 1.4481\n",
      "Source Avg Loss : 0.6551\n",
      "Target Avg Loss : 0.7930\n",
      "Source Avg reg Loss : 0.0394\n",
      "Target Avg reg Loss : 0.0247\n",
      "Source Avg cla Loss : 0.6157\n",
      "Target Avg cla Loss : 0.7682\n",
      "Avg Regression Loss : 0.0641\n",
      "Avg Classification Loss : 1.3840\n",
      "\n",
      "Epoch : 235, Total Avg Loss : 1.4459\n",
      "Source Avg Loss : 0.6364\n",
      "Target Avg Loss : 0.8095\n",
      "Source Avg reg Loss : 0.0371\n",
      "Target Avg reg Loss : 0.0272\n",
      "Source Avg cla Loss : 0.5994\n",
      "Target Avg cla Loss : 0.7822\n",
      "Avg Regression Loss : 0.0643\n",
      "Avg Classification Loss : 1.3816\n",
      "\n",
      "Epoch : 236, Total Avg Loss : 1.4537\n",
      "Source Avg Loss : 0.6430\n",
      "Target Avg Loss : 0.8107\n",
      "Source Avg reg Loss : 0.0344\n",
      "Target Avg reg Loss : 0.0257\n",
      "Source Avg cla Loss : 0.6085\n",
      "Target Avg cla Loss : 0.7850\n",
      "Avg Regression Loss : 0.0601\n",
      "Avg Classification Loss : 1.3936\n",
      "\n",
      "Epoch : 237, Total Avg Loss : 1.4554\n",
      "Source Avg Loss : 0.6652\n",
      "Target Avg Loss : 0.7902\n",
      "Source Avg reg Loss : 0.0355\n",
      "Target Avg reg Loss : 0.0230\n",
      "Source Avg cla Loss : 0.6297\n",
      "Target Avg cla Loss : 0.7672\n",
      "Avg Regression Loss : 0.0585\n",
      "Avg Classification Loss : 1.3969\n",
      "\n",
      "Epoch : 238, Total Avg Loss : 1.4668\n",
      "Source Avg Loss : 0.6780\n",
      "Target Avg Loss : 0.7888\n",
      "Source Avg reg Loss : 0.0389\n",
      "Target Avg reg Loss : 0.0261\n",
      "Source Avg cla Loss : 0.6391\n",
      "Target Avg cla Loss : 0.7627\n",
      "Avg Regression Loss : 0.0650\n",
      "Avg Classification Loss : 1.4018\n",
      "\n",
      "Epoch : 239, Total Avg Loss : 1.4610\n",
      "Source Avg Loss : 0.6741\n",
      "Target Avg Loss : 0.7869\n",
      "Source Avg reg Loss : 0.0476\n",
      "Target Avg reg Loss : 0.0295\n",
      "Source Avg cla Loss : 0.6265\n",
      "Target Avg cla Loss : 0.7574\n",
      "Avg Regression Loss : 0.0772\n",
      "Avg Classification Loss : 1.3839\n",
      "\n",
      "Epoch : 240, Total Avg Loss : 1.5007\n",
      "Source Avg Loss : 0.6423\n",
      "Target Avg Loss : 0.8584\n",
      "Source Avg reg Loss : 0.0694\n",
      "Target Avg reg Loss : 0.0418\n",
      "Source Avg cla Loss : 0.5729\n",
      "Target Avg cla Loss : 0.8167\n",
      "Avg Regression Loss : 0.1112\n",
      "Avg Classification Loss : 1.3895\n",
      "\n",
      "Epoch : 241, Total Avg Loss : 1.5698\n",
      "Source Avg Loss : 0.6292\n",
      "Target Avg Loss : 0.9407\n",
      "Source Avg reg Loss : 0.0641\n",
      "Target Avg reg Loss : 0.0555\n",
      "Source Avg cla Loss : 0.5650\n",
      "Target Avg cla Loss : 0.8852\n",
      "Avg Regression Loss : 0.1196\n",
      "Avg Classification Loss : 1.4502\n",
      "\n",
      "Epoch : 242, Total Avg Loss : 1.6038\n",
      "Source Avg Loss : 0.6417\n",
      "Target Avg Loss : 0.9621\n",
      "Source Avg reg Loss : 0.0621\n",
      "Target Avg reg Loss : 0.0545\n",
      "Source Avg cla Loss : 0.5796\n",
      "Target Avg cla Loss : 0.9076\n",
      "Avg Regression Loss : 0.1166\n",
      "Avg Classification Loss : 1.4872\n",
      "\n",
      "Epoch : 243, Total Avg Loss : 1.6501\n",
      "Source Avg Loss : 0.5983\n",
      "Target Avg Loss : 1.0518\n",
      "Source Avg reg Loss : 0.0614\n",
      "Target Avg reg Loss : 0.0558\n",
      "Source Avg cla Loss : 0.5369\n",
      "Target Avg cla Loss : 0.9960\n",
      "Avg Regression Loss : 0.1172\n",
      "Avg Classification Loss : 1.5329\n",
      "\n",
      "Epoch : 244, Total Avg Loss : 1.6985\n",
      "Source Avg Loss : 0.6214\n",
      "Target Avg Loss : 1.0772\n",
      "Source Avg reg Loss : 0.0800\n",
      "Target Avg reg Loss : 0.0999\n",
      "Source Avg cla Loss : 0.5414\n",
      "Target Avg cla Loss : 0.9773\n",
      "Avg Regression Loss : 0.1799\n",
      "Avg Classification Loss : 1.5187\n",
      "\n",
      "Epoch : 245, Total Avg Loss : 1.5304\n",
      "Source Avg Loss : 0.7462\n",
      "Target Avg Loss : 0.7843\n",
      "Source Avg reg Loss : 0.0788\n",
      "Target Avg reg Loss : 0.0776\n",
      "Source Avg cla Loss : 0.6673\n",
      "Target Avg cla Loss : 0.7067\n",
      "Avg Regression Loss : 0.1564\n",
      "Avg Classification Loss : 1.3740\n",
      "\n",
      "Epoch : 246, Total Avg Loss : 1.4842\n",
      "Source Avg Loss : 0.7449\n",
      "Target Avg Loss : 0.7392\n",
      "Source Avg reg Loss : 0.0564\n",
      "Target Avg reg Loss : 0.0397\n",
      "Source Avg cla Loss : 0.6885\n",
      "Target Avg cla Loss : 0.6995\n",
      "Avg Regression Loss : 0.0961\n",
      "Avg Classification Loss : 1.3880\n",
      "\n",
      "Epoch : 247, Total Avg Loss : 1.4615\n",
      "Source Avg Loss : 0.7340\n",
      "Target Avg Loss : 0.7275\n",
      "Source Avg reg Loss : 0.0427\n",
      "Target Avg reg Loss : 0.0308\n",
      "Source Avg cla Loss : 0.6913\n",
      "Target Avg cla Loss : 0.6967\n",
      "Avg Regression Loss : 0.0735\n",
      "Avg Classification Loss : 1.3880\n",
      "\n",
      "Epoch : 248, Total Avg Loss : 1.4465\n",
      "Source Avg Loss : 0.7256\n",
      "Target Avg Loss : 0.7209\n",
      "Source Avg reg Loss : 0.0342\n",
      "Target Avg reg Loss : 0.0248\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6960\n",
      "Avg Regression Loss : 0.0591\n",
      "Avg Classification Loss : 1.3874\n",
      "\n",
      "Epoch : 249, Total Avg Loss : 1.4412\n",
      "Source Avg Loss : 0.7234\n",
      "Target Avg Loss : 0.7177\n",
      "Source Avg reg Loss : 0.0319\n",
      "Target Avg reg Loss : 0.0224\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6953\n",
      "Avg Regression Loss : 0.0544\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 250, Total Avg Loss : 1.4406\n",
      "Source Avg Loss : 0.7215\n",
      "Target Avg Loss : 0.7191\n",
      "Source Avg reg Loss : 0.0299\n",
      "Target Avg reg Loss : 0.0237\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6954\n",
      "Avg Regression Loss : 0.0536\n",
      "Avg Classification Loss : 1.3870\n",
      "\n",
      "Epoch : 251, Total Avg Loss : 1.4610\n",
      "Source Avg Loss : 0.7395\n",
      "Target Avg Loss : 0.7215\n",
      "Source Avg reg Loss : 0.0480\n",
      "Target Avg reg Loss : 0.0262\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6953\n",
      "Avg Regression Loss : 0.0742\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 252, Total Avg Loss : 1.4576\n",
      "Source Avg Loss : 0.7338\n",
      "Target Avg Loss : 0.7237\n",
      "Source Avg reg Loss : 0.0423\n",
      "Target Avg reg Loss : 0.0285\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6952\n",
      "Avg Regression Loss : 0.0709\n",
      "Avg Classification Loss : 1.3867\n",
      "\n",
      "Epoch : 253, Total Avg Loss : 1.4476\n",
      "Source Avg Loss : 0.7243\n",
      "Target Avg Loss : 0.7233\n",
      "Source Avg reg Loss : 0.0329\n",
      "Target Avg reg Loss : 0.0282\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0611\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 254, Total Avg Loss : 1.4438\n",
      "Source Avg Loss : 0.7231\n",
      "Target Avg Loss : 0.7208\n",
      "Source Avg reg Loss : 0.0315\n",
      "Target Avg reg Loss : 0.0261\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0576\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 255, Total Avg Loss : 1.4400\n",
      "Source Avg Loss : 0.7216\n",
      "Target Avg Loss : 0.7184\n",
      "Source Avg reg Loss : 0.0300\n",
      "Target Avg reg Loss : 0.0237\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0536\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 256, Total Avg Loss : 1.4581\n",
      "Source Avg Loss : 0.7365\n",
      "Target Avg Loss : 0.7217\n",
      "Source Avg reg Loss : 0.0450\n",
      "Target Avg reg Loss : 0.0268\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0718\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 257, Total Avg Loss : 1.4478\n",
      "Source Avg Loss : 0.7275\n",
      "Target Avg Loss : 0.7203\n",
      "Source Avg reg Loss : 0.0359\n",
      "Target Avg reg Loss : 0.0256\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0614\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 258, Total Avg Loss : 1.4327\n",
      "Source Avg Loss : 0.7193\n",
      "Target Avg Loss : 0.7135\n",
      "Source Avg reg Loss : 0.0278\n",
      "Target Avg reg Loss : 0.0187\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0465\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 259, Total Avg Loss : 1.4308\n",
      "Source Avg Loss : 0.7175\n",
      "Target Avg Loss : 0.7132\n",
      "Source Avg reg Loss : 0.0261\n",
      "Target Avg reg Loss : 0.0185\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0446\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 260, Total Avg Loss : 1.4322\n",
      "Source Avg Loss : 0.7171\n",
      "Target Avg Loss : 0.7152\n",
      "Source Avg reg Loss : 0.0255\n",
      "Target Avg reg Loss : 0.0205\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0460\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 261, Total Avg Loss : 1.4558\n",
      "Source Avg Loss : 0.7366\n",
      "Target Avg Loss : 0.7192\n",
      "Source Avg reg Loss : 0.0452\n",
      "Target Avg reg Loss : 0.0245\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0697\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 262, Total Avg Loss : 1.4479\n",
      "Source Avg Loss : 0.7276\n",
      "Target Avg Loss : 0.7202\n",
      "Source Avg reg Loss : 0.0362\n",
      "Target Avg reg Loss : 0.0255\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0618\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 263, Total Avg Loss : 1.4368\n",
      "Source Avg Loss : 0.7212\n",
      "Target Avg Loss : 0.7157\n",
      "Source Avg reg Loss : 0.0298\n",
      "Target Avg reg Loss : 0.0209\n",
      "Source Avg cla Loss : 0.6913\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0507\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 264, Total Avg Loss : 1.4391\n",
      "Source Avg Loss : 0.7232\n",
      "Target Avg Loss : 0.7159\n",
      "Source Avg reg Loss : 0.0318\n",
      "Target Avg reg Loss : 0.0213\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0531\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 265, Total Avg Loss : 1.4648\n",
      "Source Avg Loss : 0.7385\n",
      "Target Avg Loss : 0.7262\n",
      "Source Avg reg Loss : 0.0473\n",
      "Target Avg reg Loss : 0.0315\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0788\n",
      "Avg Classification Loss : 1.3859\n",
      "\n",
      "Epoch : 266, Total Avg Loss : 1.4391\n",
      "Source Avg Loss : 0.7209\n",
      "Target Avg Loss : 0.7182\n",
      "Source Avg reg Loss : 0.0298\n",
      "Target Avg reg Loss : 0.0235\n",
      "Source Avg cla Loss : 0.6911\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0532\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 267, Total Avg Loss : 1.4364\n",
      "Source Avg Loss : 0.7222\n",
      "Target Avg Loss : 0.7141\n",
      "Source Avg reg Loss : 0.0310\n",
      "Target Avg reg Loss : 0.0193\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0504\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 268, Total Avg Loss : 1.4398\n",
      "Source Avg Loss : 0.7251\n",
      "Target Avg Loss : 0.7147\n",
      "Source Avg reg Loss : 0.0340\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6911\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0540\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 269, Total Avg Loss : 1.4387\n",
      "Source Avg Loss : 0.7232\n",
      "Target Avg Loss : 0.7155\n",
      "Source Avg reg Loss : 0.0323\n",
      "Target Avg reg Loss : 0.0207\n",
      "Source Avg cla Loss : 0.6909\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0530\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 270, Total Avg Loss : 1.4309\n",
      "Source Avg Loss : 0.7153\n",
      "Target Avg Loss : 0.7156\n",
      "Source Avg reg Loss : 0.0245\n",
      "Target Avg reg Loss : 0.0206\n",
      "Source Avg cla Loss : 0.6907\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.0451\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 271, Total Avg Loss : 1.4493\n",
      "Source Avg Loss : 0.7301\n",
      "Target Avg Loss : 0.7193\n",
      "Source Avg reg Loss : 0.0393\n",
      "Target Avg reg Loss : 0.0243\n",
      "Source Avg cla Loss : 0.6907\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0637\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 272, Total Avg Loss : 1.4506\n",
      "Source Avg Loss : 0.7306\n",
      "Target Avg Loss : 0.7200\n",
      "Source Avg reg Loss : 0.0401\n",
      "Target Avg reg Loss : 0.0250\n",
      "Source Avg cla Loss : 0.6906\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.0651\n",
      "Avg Classification Loss : 1.3856\n",
      "\n",
      "Epoch : 273, Total Avg Loss : 1.4341\n",
      "Source Avg Loss : 0.7191\n",
      "Target Avg Loss : 0.7149\n",
      "Source Avg reg Loss : 0.0286\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6905\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0486\n",
      "Avg Classification Loss : 1.3854\n",
      "\n",
      "Epoch : 274, Total Avg Loss : 1.4290\n",
      "Source Avg Loss : 0.7167\n",
      "Target Avg Loss : 0.7123\n",
      "Source Avg reg Loss : 0.0263\n",
      "Target Avg reg Loss : 0.0172\n",
      "Source Avg cla Loss : 0.6904\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0436\n",
      "Avg Classification Loss : 1.3855\n",
      "\n",
      "Epoch : 275, Total Avg Loss : 1.4231\n",
      "Source Avg Loss : 0.7122\n",
      "Target Avg Loss : 0.7108\n",
      "Source Avg reg Loss : 0.0221\n",
      "Target Avg reg Loss : 0.0155\n",
      "Source Avg cla Loss : 0.6901\n",
      "Target Avg cla Loss : 0.6953\n",
      "Avg Regression Loss : 0.0377\n",
      "Avg Classification Loss : 1.3854\n",
      "\n",
      "Epoch : 276, Total Avg Loss : 1.4212\n",
      "Source Avg Loss : 0.7113\n",
      "Target Avg Loss : 0.7099\n",
      "Source Avg reg Loss : 0.0217\n",
      "Target Avg reg Loss : 0.0144\n",
      "Source Avg cla Loss : 0.6896\n",
      "Target Avg cla Loss : 0.6956\n",
      "Avg Regression Loss : 0.0360\n",
      "Avg Classification Loss : 1.3852\n",
      "\n",
      "Epoch : 277, Total Avg Loss : 1.4178\n",
      "Source Avg Loss : 0.7091\n",
      "Target Avg Loss : 0.7086\n",
      "Source Avg reg Loss : 0.0201\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.6891\n",
      "Target Avg cla Loss : 0.6959\n",
      "Avg Regression Loss : 0.0328\n",
      "Avg Classification Loss : 1.3850\n",
      "\n",
      "Epoch : 278, Total Avg Loss : 1.4188\n",
      "Source Avg Loss : 0.7083\n",
      "Target Avg Loss : 0.7105\n",
      "Source Avg reg Loss : 0.0201\n",
      "Target Avg reg Loss : 0.0141\n",
      "Source Avg cla Loss : 0.6882\n",
      "Target Avg cla Loss : 0.6963\n",
      "Avg Regression Loss : 0.0342\n",
      "Avg Classification Loss : 1.3846\n",
      "\n",
      "Epoch : 279, Total Avg Loss : 1.4283\n",
      "Source Avg Loss : 0.7136\n",
      "Target Avg Loss : 0.7147\n",
      "Source Avg reg Loss : 0.0272\n",
      "Target Avg reg Loss : 0.0171\n",
      "Source Avg cla Loss : 0.6864\n",
      "Target Avg cla Loss : 0.6976\n",
      "Avg Regression Loss : 0.0443\n",
      "Avg Classification Loss : 1.3840\n",
      "\n",
      "Epoch : 280, Total Avg Loss : 1.4259\n",
      "Source Avg Loss : 0.7086\n",
      "Target Avg Loss : 0.7173\n",
      "Source Avg reg Loss : 0.0258\n",
      "Target Avg reg Loss : 0.0171\n",
      "Source Avg cla Loss : 0.6828\n",
      "Target Avg cla Loss : 0.7002\n",
      "Avg Regression Loss : 0.0430\n",
      "Avg Classification Loss : 1.3830\n",
      "\n",
      "Epoch : 281, Total Avg Loss : 1.4199\n",
      "Source Avg Loss : 0.6947\n",
      "Target Avg Loss : 0.7251\n",
      "Source Avg reg Loss : 0.0229\n",
      "Target Avg reg Loss : 0.0142\n",
      "Source Avg cla Loss : 0.6718\n",
      "Target Avg cla Loss : 0.7109\n",
      "Avg Regression Loss : 0.0372\n",
      "Avg Classification Loss : 1.3827\n",
      "\n",
      "Epoch : 282, Total Avg Loss : 1.4402\n",
      "Source Avg Loss : 0.6926\n",
      "Target Avg Loss : 0.7476\n",
      "Source Avg reg Loss : 0.0284\n",
      "Target Avg reg Loss : 0.0208\n",
      "Source Avg cla Loss : 0.6642\n",
      "Target Avg cla Loss : 0.7268\n",
      "Avg Regression Loss : 0.0493\n",
      "Avg Classification Loss : 1.3910\n",
      "\n",
      "Epoch : 283, Total Avg Loss : 1.4609\n",
      "Source Avg Loss : 0.7243\n",
      "Target Avg Loss : 0.7365\n",
      "Source Avg reg Loss : 0.0462\n",
      "Target Avg reg Loss : 0.0275\n",
      "Source Avg cla Loss : 0.6781\n",
      "Target Avg cla Loss : 0.7091\n",
      "Avg Regression Loss : 0.0737\n",
      "Avg Classification Loss : 1.3872\n",
      "\n",
      "Epoch : 284, Total Avg Loss : 1.4518\n",
      "Source Avg Loss : 0.7202\n",
      "Target Avg Loss : 0.7315\n",
      "Source Avg reg Loss : 0.0366\n",
      "Target Avg reg Loss : 0.0300\n",
      "Source Avg cla Loss : 0.6836\n",
      "Target Avg cla Loss : 0.7016\n",
      "Avg Regression Loss : 0.0666\n",
      "Avg Classification Loss : 1.3852\n",
      "\n",
      "Epoch : 285, Total Avg Loss : 1.4365\n",
      "Source Avg Loss : 0.7136\n",
      "Target Avg Loss : 0.7228\n",
      "Source Avg reg Loss : 0.0326\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6810\n",
      "Target Avg cla Loss : 0.7028\n",
      "Avg Regression Loss : 0.0526\n",
      "Avg Classification Loss : 1.3838\n",
      "\n",
      "Epoch : 286, Total Avg Loss : 1.4325\n",
      "Source Avg Loss : 0.7004\n",
      "Target Avg Loss : 0.7321\n",
      "Source Avg reg Loss : 0.0304\n",
      "Target Avg reg Loss : 0.0209\n",
      "Source Avg cla Loss : 0.6700\n",
      "Target Avg cla Loss : 0.7112\n",
      "Avg Regression Loss : 0.0513\n",
      "Avg Classification Loss : 1.3811\n",
      "\n",
      "Epoch : 287, Total Avg Loss : 1.4268\n",
      "Source Avg Loss : 0.6893\n",
      "Target Avg Loss : 0.7375\n",
      "Source Avg reg Loss : 0.0284\n",
      "Target Avg reg Loss : 0.0166\n",
      "Source Avg cla Loss : 0.6609\n",
      "Target Avg cla Loss : 0.7209\n",
      "Avg Regression Loss : 0.0451\n",
      "Avg Classification Loss : 1.3818\n",
      "\n",
      "Epoch : 288, Total Avg Loss : 1.4419\n",
      "Source Avg Loss : 0.6940\n",
      "Target Avg Loss : 0.7479\n",
      "Source Avg reg Loss : 0.0309\n",
      "Target Avg reg Loss : 0.0248\n",
      "Source Avg cla Loss : 0.6631\n",
      "Target Avg cla Loss : 0.7231\n",
      "Avg Regression Loss : 0.0557\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 289, Total Avg Loss : 1.4685\n",
      "Source Avg Loss : 0.7189\n",
      "Target Avg Loss : 0.7496\n",
      "Source Avg reg Loss : 0.0475\n",
      "Target Avg reg Loss : 0.0290\n",
      "Source Avg cla Loss : 0.6714\n",
      "Target Avg cla Loss : 0.7206\n",
      "Avg Regression Loss : 0.0765\n",
      "Avg Classification Loss : 1.3920\n",
      "\n",
      "Epoch : 290, Total Avg Loss : 1.4574\n",
      "Source Avg Loss : 0.7093\n",
      "Target Avg Loss : 0.7480\n",
      "Source Avg reg Loss : 0.0361\n",
      "Target Avg reg Loss : 0.0267\n",
      "Source Avg cla Loss : 0.6733\n",
      "Target Avg cla Loss : 0.7214\n",
      "Avg Regression Loss : 0.0627\n",
      "Avg Classification Loss : 1.3947\n",
      "\n",
      "Epoch : 291, Total Avg Loss : 1.4476\n",
      "Source Avg Loss : 0.7064\n",
      "Target Avg Loss : 0.7411\n",
      "Source Avg reg Loss : 0.0291\n",
      "Target Avg reg Loss : 0.0213\n",
      "Source Avg cla Loss : 0.6773\n",
      "Target Avg cla Loss : 0.7198\n",
      "Avg Regression Loss : 0.0504\n",
      "Avg Classification Loss : 1.3971\n",
      "\n",
      "Epoch : 292, Total Avg Loss : 1.4411\n",
      "Source Avg Loss : 0.7187\n",
      "Target Avg Loss : 0.7224\n",
      "Source Avg reg Loss : 0.0306\n",
      "Target Avg reg Loss : 0.0203\n",
      "Source Avg cla Loss : 0.6881\n",
      "Target Avg cla Loss : 0.7021\n",
      "Avg Regression Loss : 0.0509\n",
      "Avg Classification Loss : 1.3902\n",
      "\n",
      "Epoch : 293, Total Avg Loss : 1.4332\n",
      "Source Avg Loss : 0.7186\n",
      "Target Avg Loss : 0.7147\n",
      "Source Avg reg Loss : 0.0278\n",
      "Target Avg reg Loss : 0.0185\n",
      "Source Avg cla Loss : 0.6907\n",
      "Target Avg cla Loss : 0.6961\n",
      "Avg Regression Loss : 0.0463\n",
      "Avg Classification Loss : 1.3869\n",
      "\n",
      "Epoch : 294, Total Avg Loss : 1.4310\n",
      "Source Avg Loss : 0.7182\n",
      "Target Avg Loss : 0.7128\n",
      "Source Avg reg Loss : 0.0272\n",
      "Target Avg reg Loss : 0.0175\n",
      "Source Avg cla Loss : 0.6910\n",
      "Target Avg cla Loss : 0.6953\n",
      "Avg Regression Loss : 0.0446\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 295, Total Avg Loss : 1.4256\n",
      "Source Avg Loss : 0.7137\n",
      "Target Avg Loss : 0.7119\n",
      "Source Avg reg Loss : 0.0230\n",
      "Target Avg reg Loss : 0.0164\n",
      "Source Avg cla Loss : 0.6907\n",
      "Target Avg cla Loss : 0.6955\n",
      "Avg Regression Loss : 0.0394\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 296, Total Avg Loss : 1.4291\n",
      "Source Avg Loss : 0.7148\n",
      "Target Avg Loss : 0.7143\n",
      "Source Avg reg Loss : 0.0243\n",
      "Target Avg reg Loss : 0.0186\n",
      "Source Avg cla Loss : 0.6905\n",
      "Target Avg cla Loss : 0.6957\n",
      "Avg Regression Loss : 0.0429\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 297, Total Avg Loss : 1.4204\n",
      "Source Avg Loss : 0.7104\n",
      "Target Avg Loss : 0.7100\n",
      "Source Avg reg Loss : 0.0201\n",
      "Target Avg reg Loss : 0.0141\n",
      "Source Avg cla Loss : 0.6903\n",
      "Target Avg cla Loss : 0.6959\n",
      "Avg Regression Loss : 0.0342\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 298, Total Avg Loss : 1.4307\n",
      "Source Avg Loss : 0.7182\n",
      "Target Avg Loss : 0.7125\n",
      "Source Avg reg Loss : 0.0285\n",
      "Target Avg reg Loss : 0.0162\n",
      "Source Avg cla Loss : 0.6897\n",
      "Target Avg cla Loss : 0.6963\n",
      "Avg Regression Loss : 0.0447\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 299, Total Avg Loss : 1.4380\n",
      "Source Avg Loss : 0.7211\n",
      "Target Avg Loss : 0.7169\n",
      "Source Avg reg Loss : 0.0318\n",
      "Target Avg reg Loss : 0.0201\n",
      "Source Avg cla Loss : 0.6893\n",
      "Target Avg cla Loss : 0.6969\n",
      "Avg Regression Loss : 0.0519\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 300, Total Avg Loss : 1.4324\n",
      "Source Avg Loss : 0.7155\n",
      "Target Avg Loss : 0.7170\n",
      "Source Avg reg Loss : 0.0290\n",
      "Target Avg reg Loss : 0.0191\n",
      "Source Avg cla Loss : 0.6865\n",
      "Target Avg cla Loss : 0.6979\n",
      "Avg Regression Loss : 0.0481\n",
      "Avg Classification Loss : 1.3843\n",
      "\n",
      "Epoch : 301, Total Avg Loss : 1.4318\n",
      "Source Avg Loss : 0.7089\n",
      "Target Avg Loss : 0.7230\n",
      "Source Avg reg Loss : 0.0333\n",
      "Target Avg reg Loss : 0.0205\n",
      "Source Avg cla Loss : 0.6756\n",
      "Target Avg cla Loss : 0.7024\n",
      "Avg Regression Loss : 0.0538\n",
      "Avg Classification Loss : 1.3780\n",
      "\n",
      "Epoch : 302, Total Avg Loss : 1.4422\n",
      "Source Avg Loss : 0.6555\n",
      "Target Avg Loss : 0.7867\n",
      "Source Avg reg Loss : 0.0519\n",
      "Target Avg reg Loss : 0.0340\n",
      "Source Avg cla Loss : 0.6036\n",
      "Target Avg cla Loss : 0.7526\n",
      "Avg Regression Loss : 0.0859\n",
      "Avg Classification Loss : 1.3563\n",
      "\n",
      "Epoch : 303, Total Avg Loss : 1.5191\n",
      "Source Avg Loss : 0.6189\n",
      "Target Avg Loss : 0.9002\n",
      "Source Avg reg Loss : 0.0497\n",
      "Target Avg reg Loss : 0.0370\n",
      "Source Avg cla Loss : 0.5691\n",
      "Target Avg cla Loss : 0.8632\n",
      "Avg Regression Loss : 0.0868\n",
      "Avg Classification Loss : 1.4323\n",
      "\n",
      "Epoch : 304, Total Avg Loss : 1.5781\n",
      "Source Avg Loss : 0.6162\n",
      "Target Avg Loss : 0.9619\n",
      "Source Avg reg Loss : 0.0599\n",
      "Target Avg reg Loss : 0.0407\n",
      "Source Avg cla Loss : 0.5563\n",
      "Target Avg cla Loss : 0.9212\n",
      "Avg Regression Loss : 0.1006\n",
      "Avg Classification Loss : 1.4775\n",
      "\n",
      "Epoch : 305, Total Avg Loss : 1.5550\n",
      "Source Avg Loss : 0.5192\n",
      "Target Avg Loss : 1.0357\n",
      "Source Avg reg Loss : 0.0475\n",
      "Target Avg reg Loss : 0.0488\n",
      "Source Avg cla Loss : 0.4717\n",
      "Target Avg cla Loss : 0.9869\n",
      "Avg Regression Loss : 0.0963\n",
      "Avg Classification Loss : 1.4586\n",
      "\n",
      "Epoch : 306, Total Avg Loss : 1.4642\n",
      "Source Avg Loss : 0.6249\n",
      "Target Avg Loss : 0.8393\n",
      "Source Avg reg Loss : 0.0452\n",
      "Target Avg reg Loss : 0.0428\n",
      "Source Avg cla Loss : 0.5797\n",
      "Target Avg cla Loss : 0.7965\n",
      "Avg Regression Loss : 0.0880\n",
      "Avg Classification Loss : 1.3762\n",
      "\n",
      "Epoch : 307, Total Avg Loss : 1.4910\n",
      "Source Avg Loss : 0.7088\n",
      "Target Avg Loss : 0.7822\n",
      "Source Avg reg Loss : 0.0503\n",
      "Target Avg reg Loss : 0.0391\n",
      "Source Avg cla Loss : 0.6586\n",
      "Target Avg cla Loss : 0.7431\n",
      "Avg Regression Loss : 0.0893\n",
      "Avg Classification Loss : 1.4017\n",
      "\n",
      "Epoch : 308, Total Avg Loss : 1.4783\n",
      "Source Avg Loss : 0.7221\n",
      "Target Avg Loss : 0.7562\n",
      "Source Avg reg Loss : 0.0454\n",
      "Target Avg reg Loss : 0.0336\n",
      "Source Avg cla Loss : 0.6767\n",
      "Target Avg cla Loss : 0.7226\n",
      "Avg Regression Loss : 0.0790\n",
      "Avg Classification Loss : 1.3993\n",
      "\n",
      "Epoch : 309, Total Avg Loss : 1.4544\n",
      "Source Avg Loss : 0.7193\n",
      "Target Avg Loss : 0.7351\n",
      "Source Avg reg Loss : 0.0340\n",
      "Target Avg reg Loss : 0.0252\n",
      "Source Avg cla Loss : 0.6853\n",
      "Target Avg cla Loss : 0.7099\n",
      "Avg Regression Loss : 0.0592\n",
      "Avg Classification Loss : 1.3952\n",
      "\n",
      "Epoch : 310, Total Avg Loss : 1.4393\n",
      "Source Avg Loss : 0.7165\n",
      "Target Avg Loss : 0.7229\n",
      "Source Avg reg Loss : 0.0267\n",
      "Target Avg reg Loss : 0.0208\n",
      "Source Avg cla Loss : 0.6897\n",
      "Target Avg cla Loss : 0.7020\n",
      "Avg Regression Loss : 0.0475\n",
      "Avg Classification Loss : 1.3918\n",
      "\n",
      "Epoch : 311, Total Avg Loss : 1.4354\n",
      "Source Avg Loss : 0.7155\n",
      "Target Avg Loss : 0.7198\n",
      "Source Avg reg Loss : 0.0247\n",
      "Target Avg reg Loss : 0.0192\n",
      "Source Avg cla Loss : 0.6908\n",
      "Target Avg cla Loss : 0.7006\n",
      "Avg Regression Loss : 0.0440\n",
      "Avg Classification Loss : 1.3914\n",
      "\n",
      "Epoch : 312, Total Avg Loss : 1.4379\n",
      "Source Avg Loss : 0.7198\n",
      "Target Avg Loss : 0.7182\n",
      "Source Avg reg Loss : 0.0286\n",
      "Target Avg reg Loss : 0.0193\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6988\n",
      "Avg Regression Loss : 0.0480\n",
      "Avg Classification Loss : 1.3900\n",
      "\n",
      "Epoch : 313, Total Avg Loss : 1.4389\n",
      "Source Avg Loss : 0.7206\n",
      "Target Avg Loss : 0.7183\n",
      "Source Avg reg Loss : 0.0294\n",
      "Target Avg reg Loss : 0.0208\n",
      "Source Avg cla Loss : 0.6913\n",
      "Target Avg cla Loss : 0.6974\n",
      "Avg Regression Loss : 0.0502\n",
      "Avg Classification Loss : 1.3887\n",
      "\n",
      "Epoch : 314, Total Avg Loss : 1.4362\n",
      "Source Avg Loss : 0.7192\n",
      "Target Avg Loss : 0.7170\n",
      "Source Avg reg Loss : 0.0280\n",
      "Target Avg reg Loss : 0.0208\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6962\n",
      "Avg Regression Loss : 0.0488\n",
      "Avg Classification Loss : 1.3874\n",
      "\n",
      "Epoch : 315, Total Avg Loss : 1.4431\n",
      "Source Avg Loss : 0.7252\n",
      "Target Avg Loss : 0.7179\n",
      "Source Avg reg Loss : 0.0339\n",
      "Target Avg reg Loss : 0.0229\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0567\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 316, Total Avg Loss : 1.4418\n",
      "Source Avg Loss : 0.7248\n",
      "Target Avg Loss : 0.7170\n",
      "Source Avg reg Loss : 0.0334\n",
      "Target Avg reg Loss : 0.0218\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6952\n",
      "Avg Regression Loss : 0.0552\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 317, Total Avg Loss : 1.4546\n",
      "Source Avg Loss : 0.7312\n",
      "Target Avg Loss : 0.7234\n",
      "Source Avg reg Loss : 0.0400\n",
      "Target Avg reg Loss : 0.0284\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.0684\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 318, Total Avg Loss : 1.4315\n",
      "Source Avg Loss : 0.7166\n",
      "Target Avg Loss : 0.7149\n",
      "Source Avg reg Loss : 0.0258\n",
      "Target Avg reg Loss : 0.0199\n",
      "Source Avg cla Loss : 0.6909\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0457\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 319, Total Avg Loss : 1.4235\n",
      "Source Avg Loss : 0.7131\n",
      "Target Avg Loss : 0.7103\n",
      "Source Avg reg Loss : 0.0228\n",
      "Target Avg reg Loss : 0.0154\n",
      "Source Avg cla Loss : 0.6904\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.0381\n",
      "Avg Classification Loss : 1.3854\n",
      "\n",
      "Epoch : 320, Total Avg Loss : 1.4184\n",
      "Source Avg Loss : 0.7088\n",
      "Target Avg Loss : 0.7095\n",
      "Source Avg reg Loss : 0.0200\n",
      "Target Avg reg Loss : 0.0138\n",
      "Source Avg cla Loss : 0.6888\n",
      "Target Avg cla Loss : 0.6958\n",
      "Avg Regression Loss : 0.0338\n",
      "Avg Classification Loss : 1.3846\n",
      "\n",
      "Epoch : 321, Total Avg Loss : 1.4219\n",
      "Source Avg Loss : 0.7047\n",
      "Target Avg Loss : 0.7172\n",
      "Source Avg reg Loss : 0.0237\n",
      "Target Avg reg Loss : 0.0164\n",
      "Source Avg cla Loss : 0.6810\n",
      "Target Avg cla Loss : 0.7008\n",
      "Avg Regression Loss : 0.0402\n",
      "Avg Classification Loss : 1.3818\n",
      "\n",
      "Epoch : 322, Total Avg Loss : 1.4182\n",
      "Source Avg Loss : 0.6608\n",
      "Target Avg Loss : 0.7574\n",
      "Source Avg reg Loss : 0.0270\n",
      "Target Avg reg Loss : 0.0161\n",
      "Source Avg cla Loss : 0.6338\n",
      "Target Avg cla Loss : 0.7413\n",
      "Avg Regression Loss : 0.0431\n",
      "Avg Classification Loss : 1.3751\n",
      "\n",
      "Epoch : 323, Total Avg Loss : 1.4534\n",
      "Source Avg Loss : 0.6123\n",
      "Target Avg Loss : 0.8411\n",
      "Source Avg reg Loss : 0.0304\n",
      "Target Avg reg Loss : 0.0174\n",
      "Source Avg cla Loss : 0.5819\n",
      "Target Avg cla Loss : 0.8237\n",
      "Avg Regression Loss : 0.0478\n",
      "Avg Classification Loss : 1.4056\n",
      "\n",
      "Epoch : 324, Total Avg Loss : 1.5026\n",
      "Source Avg Loss : 0.5869\n",
      "Target Avg Loss : 0.9157\n",
      "Source Avg reg Loss : 0.0398\n",
      "Target Avg reg Loss : 0.0258\n",
      "Source Avg cla Loss : 0.5472\n",
      "Target Avg cla Loss : 0.8899\n",
      "Avg Regression Loss : 0.0656\n",
      "Avg Classification Loss : 1.4370\n",
      "\n",
      "Epoch : 325, Total Avg Loss : 1.7073\n",
      "Source Avg Loss : 0.6525\n",
      "Target Avg Loss : 1.0548\n",
      "Source Avg reg Loss : 0.0630\n",
      "Target Avg reg Loss : 0.0522\n",
      "Source Avg cla Loss : 0.5895\n",
      "Target Avg cla Loss : 1.0026\n",
      "Avg Regression Loss : 0.1151\n",
      "Avg Classification Loss : 1.5921\n",
      "\n",
      "Epoch : 326, Total Avg Loss : 1.9340\n",
      "Source Avg Loss : 0.5852\n",
      "Target Avg Loss : 1.3487\n",
      "Source Avg reg Loss : 0.0739\n",
      "Target Avg reg Loss : 0.1188\n",
      "Source Avg cla Loss : 0.5114\n",
      "Target Avg cla Loss : 1.2300\n",
      "Avg Regression Loss : 0.1926\n",
      "Avg Classification Loss : 1.7414\n",
      "\n",
      "Epoch : 327, Total Avg Loss : 1.9526\n",
      "Source Avg Loss : 0.5938\n",
      "Target Avg Loss : 1.3589\n",
      "Source Avg reg Loss : 0.0618\n",
      "Target Avg reg Loss : 0.0642\n",
      "Source Avg cla Loss : 0.5320\n",
      "Target Avg cla Loss : 1.2946\n",
      "Avg Regression Loss : 0.1260\n",
      "Avg Classification Loss : 1.8266\n",
      "\n",
      "Epoch : 328, Total Avg Loss : 1.8402\n",
      "Source Avg Loss : 0.5043\n",
      "Target Avg Loss : 1.3359\n",
      "Source Avg reg Loss : 0.0679\n",
      "Target Avg reg Loss : 0.0480\n",
      "Source Avg cla Loss : 0.4364\n",
      "Target Avg cla Loss : 1.2879\n",
      "Avg Regression Loss : 0.1159\n",
      "Avg Classification Loss : 1.7242\n",
      "\n",
      "Epoch : 329, Total Avg Loss : 1.7334\n",
      "Source Avg Loss : 0.3911\n",
      "Target Avg Loss : 1.3423\n",
      "Source Avg reg Loss : 0.0473\n",
      "Target Avg reg Loss : 0.0425\n",
      "Source Avg cla Loss : 0.3439\n",
      "Target Avg cla Loss : 1.2998\n",
      "Avg Regression Loss : 0.0898\n",
      "Avg Classification Loss : 1.6436\n",
      "\n",
      "Epoch : 330, Total Avg Loss : 1.7071\n",
      "Source Avg Loss : 0.3717\n",
      "Target Avg Loss : 1.3354\n",
      "Source Avg reg Loss : 0.0454\n",
      "Target Avg reg Loss : 0.0318\n",
      "Source Avg cla Loss : 0.3263\n",
      "Target Avg cla Loss : 1.3036\n",
      "Avg Regression Loss : 0.0772\n",
      "Avg Classification Loss : 1.6299\n",
      "\n",
      "Epoch : 331, Total Avg Loss : 1.7307\n",
      "Source Avg Loss : 0.3964\n",
      "Target Avg Loss : 1.3343\n",
      "Source Avg reg Loss : 0.0400\n",
      "Target Avg reg Loss : 0.0299\n",
      "Source Avg cla Loss : 0.3564\n",
      "Target Avg cla Loss : 1.3044\n",
      "Avg Regression Loss : 0.0699\n",
      "Avg Classification Loss : 1.6609\n",
      "\n",
      "Epoch : 332, Total Avg Loss : 1.8114\n",
      "Source Avg Loss : 0.4777\n",
      "Target Avg Loss : 1.3337\n",
      "Source Avg reg Loss : 0.0579\n",
      "Target Avg reg Loss : 0.0277\n",
      "Source Avg cla Loss : 0.4198\n",
      "Target Avg cla Loss : 1.3060\n",
      "Avg Regression Loss : 0.0855\n",
      "Avg Classification Loss : 1.7259\n",
      "\n",
      "Epoch : 333, Total Avg Loss : 1.7954\n",
      "Source Avg Loss : 0.4611\n",
      "Target Avg Loss : 1.3343\n",
      "Source Avg reg Loss : 0.0648\n",
      "Target Avg reg Loss : 0.0250\n",
      "Source Avg cla Loss : 0.3963\n",
      "Target Avg cla Loss : 1.3093\n",
      "Avg Regression Loss : 0.0898\n",
      "Avg Classification Loss : 1.7056\n",
      "\n",
      "Epoch : 334, Total Avg Loss : 1.7007\n",
      "Source Avg Loss : 0.3623\n",
      "Target Avg Loss : 1.3384\n",
      "Source Avg reg Loss : 0.0473\n",
      "Target Avg reg Loss : 0.0255\n",
      "Source Avg cla Loss : 0.3150\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0728\n",
      "Avg Classification Loss : 1.6279\n",
      "\n",
      "Epoch : 335, Total Avg Loss : 1.6838\n",
      "Source Avg Loss : 0.3477\n",
      "Target Avg Loss : 1.3360\n",
      "Source Avg reg Loss : 0.0339\n",
      "Target Avg reg Loss : 0.0230\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0569\n",
      "Avg Classification Loss : 1.6269\n",
      "\n",
      "Epoch : 336, Total Avg Loss : 1.6817\n",
      "Source Avg Loss : 0.3465\n",
      "Target Avg Loss : 1.3352\n",
      "Source Avg reg Loss : 0.0327\n",
      "Target Avg reg Loss : 0.0222\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0549\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 337, Total Avg Loss : 1.6758\n",
      "Source Avg Loss : 0.3426\n",
      "Target Avg Loss : 1.3332\n",
      "Source Avg reg Loss : 0.0288\n",
      "Target Avg reg Loss : 0.0202\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0491\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 338, Total Avg Loss : 1.6706\n",
      "Source Avg Loss : 0.3399\n",
      "Target Avg Loss : 1.3307\n",
      "Source Avg reg Loss : 0.0261\n",
      "Target Avg reg Loss : 0.0177\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0438\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 339, Total Avg Loss : 1.6762\n",
      "Source Avg Loss : 0.3433\n",
      "Target Avg Loss : 1.3329\n",
      "Source Avg reg Loss : 0.0295\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0495\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 340, Total Avg Loss : 1.6730\n",
      "Source Avg Loss : 0.3406\n",
      "Target Avg Loss : 1.3324\n",
      "Source Avg reg Loss : 0.0268\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0462\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 341, Total Avg Loss : 1.6785\n",
      "Source Avg Loss : 0.3432\n",
      "Target Avg Loss : 1.3352\n",
      "Source Avg reg Loss : 0.0294\n",
      "Target Avg reg Loss : 0.0223\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0517\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 342, Total Avg Loss : 1.6881\n",
      "Source Avg Loss : 0.3481\n",
      "Target Avg Loss : 1.3400\n",
      "Source Avg reg Loss : 0.0342\n",
      "Target Avg reg Loss : 0.0270\n",
      "Source Avg cla Loss : 0.3139\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0611\n",
      "Avg Classification Loss : 1.6269\n",
      "\n",
      "Epoch : 343, Total Avg Loss : 1.6926\n",
      "Source Avg Loss : 0.3519\n",
      "Target Avg Loss : 1.3407\n",
      "Source Avg reg Loss : 0.0379\n",
      "Target Avg reg Loss : 0.0279\n",
      "Source Avg cla Loss : 0.3140\n",
      "Target Avg cla Loss : 1.3128\n",
      "Avg Regression Loss : 0.0658\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 344, Total Avg Loss : 1.6747\n",
      "Source Avg Loss : 0.3413\n",
      "Target Avg Loss : 1.3334\n",
      "Source Avg reg Loss : 0.0273\n",
      "Target Avg reg Loss : 0.0206\n",
      "Source Avg cla Loss : 0.3140\n",
      "Target Avg cla Loss : 1.3127\n",
      "Avg Regression Loss : 0.0479\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 345, Total Avg Loss : 1.6686\n",
      "Source Avg Loss : 0.3390\n",
      "Target Avg Loss : 1.3297\n",
      "Source Avg reg Loss : 0.0250\n",
      "Target Avg reg Loss : 0.0168\n",
      "Source Avg cla Loss : 0.3140\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0418\n",
      "Avg Classification Loss : 1.6269\n",
      "\n",
      "Epoch : 346, Total Avg Loss : 1.6732\n",
      "Source Avg Loss : 0.3410\n",
      "Target Avg Loss : 1.3323\n",
      "Source Avg reg Loss : 0.0271\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.3139\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0464\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 347, Total Avg Loss : 1.6742\n",
      "Source Avg Loss : 0.3426\n",
      "Target Avg Loss : 1.3316\n",
      "Source Avg reg Loss : 0.0287\n",
      "Target Avg reg Loss : 0.0187\n",
      "Source Avg cla Loss : 0.3140\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0473\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 348, Total Avg Loss : 1.6768\n",
      "Source Avg Loss : 0.3411\n",
      "Target Avg Loss : 1.3357\n",
      "Source Avg reg Loss : 0.0272\n",
      "Target Avg reg Loss : 0.0227\n",
      "Source Avg cla Loss : 0.3139\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0499\n",
      "Avg Classification Loss : 1.6269\n",
      "\n",
      "Epoch : 349, Total Avg Loss : 1.6923\n",
      "Source Avg Loss : 0.3508\n",
      "Target Avg Loss : 1.3415\n",
      "Source Avg reg Loss : 0.0368\n",
      "Target Avg reg Loss : 0.0286\n",
      "Source Avg cla Loss : 0.3140\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0654\n",
      "Avg Classification Loss : 1.6269\n",
      "\n",
      "Epoch : 350, Total Avg Loss : 1.6837\n",
      "Source Avg Loss : 0.3477\n",
      "Target Avg Loss : 1.3360\n",
      "Source Avg reg Loss : 0.0338\n",
      "Target Avg reg Loss : 0.0230\n",
      "Source Avg cla Loss : 0.3139\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0569\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 351, Total Avg Loss : 1.6757\n",
      "Source Avg Loss : 0.3435\n",
      "Target Avg Loss : 1.3323\n",
      "Source Avg reg Loss : 0.0295\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.3139\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0490\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 352, Total Avg Loss : 1.6746\n",
      "Source Avg Loss : 0.3420\n",
      "Target Avg Loss : 1.3326\n",
      "Source Avg reg Loss : 0.0282\n",
      "Target Avg reg Loss : 0.0196\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0478\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 353, Total Avg Loss : 1.6685\n",
      "Source Avg Loss : 0.3380\n",
      "Target Avg Loss : 1.3305\n",
      "Source Avg reg Loss : 0.0241\n",
      "Target Avg reg Loss : 0.0176\n",
      "Source Avg cla Loss : 0.3139\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0417\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 354, Total Avg Loss : 1.6653\n",
      "Source Avg Loss : 0.3378\n",
      "Target Avg Loss : 1.3276\n",
      "Source Avg reg Loss : 0.0240\n",
      "Target Avg reg Loss : 0.0146\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0386\n",
      "Avg Classification Loss : 1.6268\n",
      "\n",
      "Epoch : 355, Total Avg Loss : 1.6596\n",
      "Source Avg Loss : 0.3336\n",
      "Target Avg Loss : 1.3260\n",
      "Source Avg reg Loss : 0.0198\n",
      "Target Avg reg Loss : 0.0130\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0329\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 356, Total Avg Loss : 1.6562\n",
      "Source Avg Loss : 0.3308\n",
      "Target Avg Loss : 1.3253\n",
      "Source Avg reg Loss : 0.0171\n",
      "Target Avg reg Loss : 0.0124\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0295\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 357, Total Avg Loss : 1.6562\n",
      "Source Avg Loss : 0.3312\n",
      "Target Avg Loss : 1.3250\n",
      "Source Avg reg Loss : 0.0174\n",
      "Target Avg reg Loss : 0.0120\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0295\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 358, Total Avg Loss : 1.6761\n",
      "Source Avg Loss : 0.3428\n",
      "Target Avg Loss : 1.3333\n",
      "Source Avg reg Loss : 0.0291\n",
      "Target Avg reg Loss : 0.0204\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0494\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 359, Total Avg Loss : 1.6816\n",
      "Source Avg Loss : 0.3458\n",
      "Target Avg Loss : 1.3358\n",
      "Source Avg reg Loss : 0.0321\n",
      "Target Avg reg Loss : 0.0229\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0550\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 360, Total Avg Loss : 1.6689\n",
      "Source Avg Loss : 0.3369\n",
      "Target Avg Loss : 1.3320\n",
      "Source Avg reg Loss : 0.0231\n",
      "Target Avg reg Loss : 0.0191\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0422\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 361, Total Avg Loss : 1.6750\n",
      "Source Avg Loss : 0.3436\n",
      "Target Avg Loss : 1.3314\n",
      "Source Avg reg Loss : 0.0299\n",
      "Target Avg reg Loss : 0.0184\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0483\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 362, Total Avg Loss : 1.6709\n",
      "Source Avg Loss : 0.3402\n",
      "Target Avg Loss : 1.3307\n",
      "Source Avg reg Loss : 0.0265\n",
      "Target Avg reg Loss : 0.0177\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0442\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 363, Total Avg Loss : 1.6703\n",
      "Source Avg Loss : 0.3395\n",
      "Target Avg Loss : 1.3309\n",
      "Source Avg reg Loss : 0.0258\n",
      "Target Avg reg Loss : 0.0179\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0437\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 364, Total Avg Loss : 1.6686\n",
      "Source Avg Loss : 0.3392\n",
      "Target Avg Loss : 1.3294\n",
      "Source Avg reg Loss : 0.0255\n",
      "Target Avg reg Loss : 0.0164\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0419\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 365, Total Avg Loss : 1.6631\n",
      "Source Avg Loss : 0.3359\n",
      "Target Avg Loss : 1.3272\n",
      "Source Avg reg Loss : 0.0223\n",
      "Target Avg reg Loss : 0.0142\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0365\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 366, Total Avg Loss : 1.6592\n",
      "Source Avg Loss : 0.3332\n",
      "Target Avg Loss : 1.3261\n",
      "Source Avg reg Loss : 0.0196\n",
      "Target Avg reg Loss : 0.0130\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0326\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 367, Total Avg Loss : 1.6640\n",
      "Source Avg Loss : 0.3347\n",
      "Target Avg Loss : 1.3293\n",
      "Source Avg reg Loss : 0.0211\n",
      "Target Avg reg Loss : 0.0163\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0373\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 368, Total Avg Loss : 1.6729\n",
      "Source Avg Loss : 0.3434\n",
      "Target Avg Loss : 1.3296\n",
      "Source Avg reg Loss : 0.0297\n",
      "Target Avg reg Loss : 0.0166\n",
      "Source Avg cla Loss : 0.3137\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0463\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 369, Total Avg Loss : 1.6681\n",
      "Source Avg Loss : 0.3384\n",
      "Target Avg Loss : 1.3297\n",
      "Source Avg reg Loss : 0.0248\n",
      "Target Avg reg Loss : 0.0166\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0414\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 370, Total Avg Loss : 1.6630\n",
      "Source Avg Loss : 0.3353\n",
      "Target Avg Loss : 1.3277\n",
      "Source Avg reg Loss : 0.0217\n",
      "Target Avg reg Loss : 0.0146\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0363\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 371, Total Avg Loss : 1.6564\n",
      "Source Avg Loss : 0.3309\n",
      "Target Avg Loss : 1.3254\n",
      "Source Avg reg Loss : 0.0173\n",
      "Target Avg reg Loss : 0.0124\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0297\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 372, Total Avg Loss : 1.6563\n",
      "Source Avg Loss : 0.3311\n",
      "Target Avg Loss : 1.3252\n",
      "Source Avg reg Loss : 0.0175\n",
      "Target Avg reg Loss : 0.0122\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0297\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 373, Total Avg Loss : 1.6587\n",
      "Source Avg Loss : 0.3330\n",
      "Target Avg Loss : 1.3256\n",
      "Source Avg reg Loss : 0.0194\n",
      "Target Avg reg Loss : 0.0126\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0320\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 374, Total Avg Loss : 1.6731\n",
      "Source Avg Loss : 0.3402\n",
      "Target Avg Loss : 1.3328\n",
      "Source Avg reg Loss : 0.0266\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0464\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 375, Total Avg Loss : 1.6890\n",
      "Source Avg Loss : 0.3474\n",
      "Target Avg Loss : 1.3416\n",
      "Source Avg reg Loss : 0.0339\n",
      "Target Avg reg Loss : 0.0286\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0625\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 376, Total Avg Loss : 1.6834\n",
      "Source Avg Loss : 0.3450\n",
      "Target Avg Loss : 1.3384\n",
      "Source Avg reg Loss : 0.0314\n",
      "Target Avg reg Loss : 0.0254\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0568\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 377, Total Avg Loss : 1.6862\n",
      "Source Avg Loss : 0.3481\n",
      "Target Avg Loss : 1.3381\n",
      "Source Avg reg Loss : 0.0345\n",
      "Target Avg reg Loss : 0.0251\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0596\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 378, Total Avg Loss : 1.6895\n",
      "Source Avg Loss : 0.3505\n",
      "Target Avg Loss : 1.3390\n",
      "Source Avg reg Loss : 0.0368\n",
      "Target Avg reg Loss : 0.0259\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0628\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 379, Total Avg Loss : 1.6796\n",
      "Source Avg Loss : 0.3447\n",
      "Target Avg Loss : 1.3350\n",
      "Source Avg reg Loss : 0.0311\n",
      "Target Avg reg Loss : 0.0219\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0530\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 380, Total Avg Loss : 1.6925\n",
      "Source Avg Loss : 0.3507\n",
      "Target Avg Loss : 1.3418\n",
      "Source Avg reg Loss : 0.0371\n",
      "Target Avg reg Loss : 0.0288\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0658\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 381, Total Avg Loss : 1.6758\n",
      "Source Avg Loss : 0.3415\n",
      "Target Avg Loss : 1.3343\n",
      "Source Avg reg Loss : 0.0279\n",
      "Target Avg reg Loss : 0.0212\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0492\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 382, Total Avg Loss : 1.6929\n",
      "Source Avg Loss : 0.3514\n",
      "Target Avg Loss : 1.3415\n",
      "Source Avg reg Loss : 0.0378\n",
      "Target Avg reg Loss : 0.0284\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0662\n",
      "Avg Classification Loss : 1.6267\n",
      "\n",
      "Epoch : 383, Total Avg Loss : 1.6740\n",
      "Source Avg Loss : 0.3415\n",
      "Target Avg Loss : 1.3325\n",
      "Source Avg reg Loss : 0.0280\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0473\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 384, Total Avg Loss : 1.6906\n",
      "Source Avg Loss : 0.3496\n",
      "Target Avg Loss : 1.3410\n",
      "Source Avg reg Loss : 0.0361\n",
      "Target Avg reg Loss : 0.0279\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0640\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 385, Total Avg Loss : 1.6697\n",
      "Source Avg Loss : 0.3387\n",
      "Target Avg Loss : 1.3311\n",
      "Source Avg reg Loss : 0.0252\n",
      "Target Avg reg Loss : 0.0180\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0431\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 386, Total Avg Loss : 1.6872\n",
      "Source Avg Loss : 0.3486\n",
      "Target Avg Loss : 1.3386\n",
      "Source Avg reg Loss : 0.0351\n",
      "Target Avg reg Loss : 0.0255\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0606\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 387, Total Avg Loss : 1.6688\n",
      "Source Avg Loss : 0.3381\n",
      "Target Avg Loss : 1.3307\n",
      "Source Avg reg Loss : 0.0246\n",
      "Target Avg reg Loss : 0.0176\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0422\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 388, Total Avg Loss : 1.6836\n",
      "Source Avg Loss : 0.3464\n",
      "Target Avg Loss : 1.3372\n",
      "Source Avg reg Loss : 0.0329\n",
      "Target Avg reg Loss : 0.0241\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0570\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 389, Total Avg Loss : 1.6671\n",
      "Source Avg Loss : 0.3360\n",
      "Target Avg Loss : 1.3312\n",
      "Source Avg reg Loss : 0.0225\n",
      "Target Avg reg Loss : 0.0180\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0405\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 390, Total Avg Loss : 1.6801\n",
      "Source Avg Loss : 0.3447\n",
      "Target Avg Loss : 1.3354\n",
      "Source Avg reg Loss : 0.0312\n",
      "Target Avg reg Loss : 0.0223\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0535\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 391, Total Avg Loss : 1.6742\n",
      "Source Avg Loss : 0.3422\n",
      "Target Avg Loss : 1.3320\n",
      "Source Avg reg Loss : 0.0288\n",
      "Target Avg reg Loss : 0.0188\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0476\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 392, Total Avg Loss : 1.6817\n",
      "Source Avg Loss : 0.3454\n",
      "Target Avg Loss : 1.3362\n",
      "Source Avg reg Loss : 0.0320\n",
      "Target Avg reg Loss : 0.0231\n",
      "Source Avg cla Loss : 0.3135\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0551\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 393, Total Avg Loss : 1.6640\n",
      "Source Avg Loss : 0.3348\n",
      "Target Avg Loss : 1.3292\n",
      "Source Avg reg Loss : 0.0214\n",
      "Target Avg reg Loss : 0.0161\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0374\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 394, Total Avg Loss : 1.6609\n",
      "Source Avg Loss : 0.3349\n",
      "Target Avg Loss : 1.3260\n",
      "Source Avg reg Loss : 0.0215\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0343\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 395, Total Avg Loss : 1.6598\n",
      "Source Avg Loss : 0.3339\n",
      "Target Avg Loss : 1.3259\n",
      "Source Avg reg Loss : 0.0204\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0332\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 396, Total Avg Loss : 1.6596\n",
      "Source Avg Loss : 0.3339\n",
      "Target Avg Loss : 1.3256\n",
      "Source Avg reg Loss : 0.0205\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0330\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 397, Total Avg Loss : 1.6665\n",
      "Source Avg Loss : 0.3387\n",
      "Target Avg Loss : 1.3278\n",
      "Source Avg reg Loss : 0.0253\n",
      "Target Avg reg Loss : 0.0147\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0399\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 398, Total Avg Loss : 1.6685\n",
      "Source Avg Loss : 0.3366\n",
      "Target Avg Loss : 1.3318\n",
      "Source Avg reg Loss : 0.0232\n",
      "Target Avg reg Loss : 0.0187\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0419\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 399, Total Avg Loss : 1.6619\n",
      "Source Avg Loss : 0.3343\n",
      "Target Avg Loss : 1.3276\n",
      "Source Avg reg Loss : 0.0209\n",
      "Target Avg reg Loss : 0.0144\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0354\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 400, Total Avg Loss : 1.6561\n",
      "Source Avg Loss : 0.3302\n",
      "Target Avg Loss : 1.3260\n",
      "Source Avg reg Loss : 0.0167\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0295\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 401, Total Avg Loss : 1.6573\n",
      "Source Avg Loss : 0.3320\n",
      "Target Avg Loss : 1.3253\n",
      "Source Avg reg Loss : 0.0186\n",
      "Target Avg reg Loss : 0.0121\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0307\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 402, Total Avg Loss : 1.6683\n",
      "Source Avg Loss : 0.3371\n",
      "Target Avg Loss : 1.3313\n",
      "Source Avg reg Loss : 0.0237\n",
      "Target Avg reg Loss : 0.0181\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0418\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 403, Total Avg Loss : 1.6644\n",
      "Source Avg Loss : 0.3362\n",
      "Target Avg Loss : 1.3282\n",
      "Source Avg reg Loss : 0.0228\n",
      "Target Avg reg Loss : 0.0150\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0378\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 404, Total Avg Loss : 1.6637\n",
      "Source Avg Loss : 0.3364\n",
      "Target Avg Loss : 1.3273\n",
      "Source Avg reg Loss : 0.0229\n",
      "Target Avg reg Loss : 0.0142\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0371\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 405, Total Avg Loss : 1.6665\n",
      "Source Avg Loss : 0.3383\n",
      "Target Avg Loss : 1.3282\n",
      "Source Avg reg Loss : 0.0249\n",
      "Target Avg reg Loss : 0.0150\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0399\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 406, Total Avg Loss : 1.6602\n",
      "Source Avg Loss : 0.3332\n",
      "Target Avg Loss : 1.3270\n",
      "Source Avg reg Loss : 0.0198\n",
      "Target Avg reg Loss : 0.0139\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0337\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 407, Total Avg Loss : 1.6532\n",
      "Source Avg Loss : 0.3296\n",
      "Target Avg Loss : 1.3236\n",
      "Source Avg reg Loss : 0.0162\n",
      "Target Avg reg Loss : 0.0105\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0266\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 408, Total Avg Loss : 1.6526\n",
      "Source Avg Loss : 0.3286\n",
      "Target Avg Loss : 1.3240\n",
      "Source Avg reg Loss : 0.0152\n",
      "Target Avg reg Loss : 0.0109\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0261\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 409, Total Avg Loss : 1.6578\n",
      "Source Avg Loss : 0.3323\n",
      "Target Avg Loss : 1.3254\n",
      "Source Avg reg Loss : 0.0189\n",
      "Target Avg reg Loss : 0.0123\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0312\n",
      "Avg Classification Loss : 1.6266\n",
      "\n",
      "Epoch : 410, Total Avg Loss : 1.6755\n",
      "Source Avg Loss : 0.3433\n",
      "Target Avg Loss : 1.3322\n",
      "Source Avg reg Loss : 0.0300\n",
      "Target Avg reg Loss : 0.0190\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0490\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 411, Total Avg Loss : 1.6895\n",
      "Source Avg Loss : 0.3505\n",
      "Target Avg Loss : 1.3390\n",
      "Source Avg reg Loss : 0.0371\n",
      "Target Avg reg Loss : 0.0258\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0630\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 412, Total Avg Loss : 1.6612\n",
      "Source Avg Loss : 0.3331\n",
      "Target Avg Loss : 1.3281\n",
      "Source Avg reg Loss : 0.0198\n",
      "Target Avg reg Loss : 0.0149\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0347\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 413, Total Avg Loss : 1.6752\n",
      "Source Avg Loss : 0.3437\n",
      "Target Avg Loss : 1.3315\n",
      "Source Avg reg Loss : 0.0303\n",
      "Target Avg reg Loss : 0.0184\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0487\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 414, Total Avg Loss : 1.6730\n",
      "Source Avg Loss : 0.3389\n",
      "Target Avg Loss : 1.3341\n",
      "Source Avg reg Loss : 0.0256\n",
      "Target Avg reg Loss : 0.0209\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0465\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 415, Total Avg Loss : 1.6833\n",
      "Source Avg Loss : 0.3461\n",
      "Target Avg Loss : 1.3371\n",
      "Source Avg reg Loss : 0.0328\n",
      "Target Avg reg Loss : 0.0240\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0567\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 416, Total Avg Loss : 1.6646\n",
      "Source Avg Loss : 0.3343\n",
      "Target Avg Loss : 1.3303\n",
      "Source Avg reg Loss : 0.0210\n",
      "Target Avg reg Loss : 0.0171\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0380\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 417, Total Avg Loss : 1.6716\n",
      "Source Avg Loss : 0.3401\n",
      "Target Avg Loss : 1.3316\n",
      "Source Avg reg Loss : 0.0267\n",
      "Target Avg reg Loss : 0.0184\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0451\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 418, Total Avg Loss : 1.6790\n",
      "Source Avg Loss : 0.3452\n",
      "Target Avg Loss : 1.3338\n",
      "Source Avg reg Loss : 0.0319\n",
      "Target Avg reg Loss : 0.0206\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0524\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 419, Total Avg Loss : 1.6837\n",
      "Source Avg Loss : 0.3465\n",
      "Target Avg Loss : 1.3372\n",
      "Source Avg reg Loss : 0.0331\n",
      "Target Avg reg Loss : 0.0241\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0572\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 420, Total Avg Loss : 1.6634\n",
      "Source Avg Loss : 0.3364\n",
      "Target Avg Loss : 1.3270\n",
      "Source Avg reg Loss : 0.0230\n",
      "Target Avg reg Loss : 0.0139\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0369\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 421, Total Avg Loss : 1.6818\n",
      "Source Avg Loss : 0.3451\n",
      "Target Avg Loss : 1.3367\n",
      "Source Avg reg Loss : 0.0317\n",
      "Target Avg reg Loss : 0.0236\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0553\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 422, Total Avg Loss : 1.6659\n",
      "Source Avg Loss : 0.3382\n",
      "Target Avg Loss : 1.3277\n",
      "Source Avg reg Loss : 0.0248\n",
      "Target Avg reg Loss : 0.0145\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0393\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 423, Total Avg Loss : 1.6807\n",
      "Source Avg Loss : 0.3453\n",
      "Target Avg Loss : 1.3354\n",
      "Source Avg reg Loss : 0.0320\n",
      "Target Avg reg Loss : 0.0222\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0541\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 424, Total Avg Loss : 1.6700\n",
      "Source Avg Loss : 0.3418\n",
      "Target Avg Loss : 1.3283\n",
      "Source Avg reg Loss : 0.0284\n",
      "Target Avg reg Loss : 0.0151\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0435\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 425, Total Avg Loss : 1.6753\n",
      "Source Avg Loss : 0.3428\n",
      "Target Avg Loss : 1.3325\n",
      "Source Avg reg Loss : 0.0295\n",
      "Target Avg reg Loss : 0.0193\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0488\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 426, Total Avg Loss : 1.6596\n",
      "Source Avg Loss : 0.3340\n",
      "Target Avg Loss : 1.3256\n",
      "Source Avg reg Loss : 0.0206\n",
      "Target Avg reg Loss : 0.0124\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0331\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 427, Total Avg Loss : 1.6583\n",
      "Source Avg Loss : 0.3319\n",
      "Target Avg Loss : 1.3264\n",
      "Source Avg reg Loss : 0.0185\n",
      "Target Avg reg Loss : 0.0132\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0318\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 428, Total Avg Loss : 1.6652\n",
      "Source Avg Loss : 0.3372\n",
      "Target Avg Loss : 1.3279\n",
      "Source Avg reg Loss : 0.0239\n",
      "Target Avg reg Loss : 0.0148\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0386\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 429, Total Avg Loss : 1.6783\n",
      "Source Avg Loss : 0.3447\n",
      "Target Avg Loss : 1.3335\n",
      "Source Avg reg Loss : 0.0314\n",
      "Target Avg reg Loss : 0.0204\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0517\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 430, Total Avg Loss : 1.6718\n",
      "Source Avg Loss : 0.3388\n",
      "Target Avg Loss : 1.3330\n",
      "Source Avg reg Loss : 0.0254\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0452\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 431, Total Avg Loss : 1.6836\n",
      "Source Avg Loss : 0.3470\n",
      "Target Avg Loss : 1.3367\n",
      "Source Avg reg Loss : 0.0336\n",
      "Target Avg reg Loss : 0.0235\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0571\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 432, Total Avg Loss : 1.6640\n",
      "Source Avg Loss : 0.3341\n",
      "Target Avg Loss : 1.3299\n",
      "Source Avg reg Loss : 0.0207\n",
      "Target Avg reg Loss : 0.0167\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0375\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 433, Total Avg Loss : 1.6600\n",
      "Source Avg Loss : 0.3334\n",
      "Target Avg Loss : 1.3266\n",
      "Source Avg reg Loss : 0.0201\n",
      "Target Avg reg Loss : 0.0134\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0335\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 434, Total Avg Loss : 1.6563\n",
      "Source Avg Loss : 0.3304\n",
      "Target Avg Loss : 1.3259\n",
      "Source Avg reg Loss : 0.0171\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0298\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 435, Total Avg Loss : 1.6601\n",
      "Source Avg Loss : 0.3326\n",
      "Target Avg Loss : 1.3274\n",
      "Source Avg reg Loss : 0.0193\n",
      "Target Avg reg Loss : 0.0142\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0335\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 436, Total Avg Loss : 1.6651\n",
      "Source Avg Loss : 0.3377\n",
      "Target Avg Loss : 1.3275\n",
      "Source Avg reg Loss : 0.0243\n",
      "Target Avg reg Loss : 0.0143\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0386\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 437, Total Avg Loss : 1.6715\n",
      "Source Avg Loss : 0.3406\n",
      "Target Avg Loss : 1.3309\n",
      "Source Avg reg Loss : 0.0273\n",
      "Target Avg reg Loss : 0.0177\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0449\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 438, Total Avg Loss : 1.6693\n",
      "Source Avg Loss : 0.3407\n",
      "Target Avg Loss : 1.3286\n",
      "Source Avg reg Loss : 0.0273\n",
      "Target Avg reg Loss : 0.0154\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0428\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 439, Total Avg Loss : 1.6648\n",
      "Source Avg Loss : 0.3355\n",
      "Target Avg Loss : 1.3293\n",
      "Source Avg reg Loss : 0.0222\n",
      "Target Avg reg Loss : 0.0161\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0383\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 440, Total Avg Loss : 1.6601\n",
      "Source Avg Loss : 0.3331\n",
      "Target Avg Loss : 1.3270\n",
      "Source Avg reg Loss : 0.0198\n",
      "Target Avg reg Loss : 0.0138\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0336\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 441, Total Avg Loss : 1.6799\n",
      "Source Avg Loss : 0.3451\n",
      "Target Avg Loss : 1.3349\n",
      "Source Avg reg Loss : 0.0317\n",
      "Target Avg reg Loss : 0.0217\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0534\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 442, Total Avg Loss : 1.6685\n",
      "Source Avg Loss : 0.3372\n",
      "Target Avg Loss : 1.3312\n",
      "Source Avg reg Loss : 0.0239\n",
      "Target Avg reg Loss : 0.0181\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0419\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 443, Total Avg Loss : 1.6727\n",
      "Source Avg Loss : 0.3409\n",
      "Target Avg Loss : 1.3318\n",
      "Source Avg reg Loss : 0.0275\n",
      "Target Avg reg Loss : 0.0186\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0462\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 444, Total Avg Loss : 1.6645\n",
      "Source Avg Loss : 0.3375\n",
      "Target Avg Loss : 1.3270\n",
      "Source Avg reg Loss : 0.0242\n",
      "Target Avg reg Loss : 0.0138\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0380\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 445, Total Avg Loss : 1.6683\n",
      "Source Avg Loss : 0.3371\n",
      "Target Avg Loss : 1.3311\n",
      "Source Avg reg Loss : 0.0238\n",
      "Target Avg reg Loss : 0.0179\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0417\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 446, Total Avg Loss : 1.6555\n",
      "Source Avg Loss : 0.3308\n",
      "Target Avg Loss : 1.3246\n",
      "Source Avg reg Loss : 0.0175\n",
      "Target Avg reg Loss : 0.0114\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0289\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 447, Total Avg Loss : 1.6533\n",
      "Source Avg Loss : 0.3292\n",
      "Target Avg Loss : 1.3241\n",
      "Source Avg reg Loss : 0.0159\n",
      "Target Avg reg Loss : 0.0109\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0268\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 448, Total Avg Loss : 1.6601\n",
      "Source Avg Loss : 0.3332\n",
      "Target Avg Loss : 1.3269\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0137\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0336\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 449, Total Avg Loss : 1.6785\n",
      "Source Avg Loss : 0.3442\n",
      "Target Avg Loss : 1.3342\n",
      "Source Avg reg Loss : 0.0309\n",
      "Target Avg reg Loss : 0.0210\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0519\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 450, Total Avg Loss : 1.6695\n",
      "Source Avg Loss : 0.3393\n",
      "Target Avg Loss : 1.3301\n",
      "Source Avg reg Loss : 0.0260\n",
      "Target Avg reg Loss : 0.0169\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0429\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 451, Total Avg Loss : 1.6705\n",
      "Source Avg Loss : 0.3398\n",
      "Target Avg Loss : 1.3307\n",
      "Source Avg reg Loss : 0.0264\n",
      "Target Avg reg Loss : 0.0175\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0440\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 452, Total Avg Loss : 1.6639\n",
      "Source Avg Loss : 0.3368\n",
      "Target Avg Loss : 1.3271\n",
      "Source Avg reg Loss : 0.0234\n",
      "Target Avg reg Loss : 0.0140\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0374\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 453, Total Avg Loss : 1.6791\n",
      "Source Avg Loss : 0.3446\n",
      "Target Avg Loss : 1.3346\n",
      "Source Avg reg Loss : 0.0312\n",
      "Target Avg reg Loss : 0.0214\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0526\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 454, Total Avg Loss : 1.6590\n",
      "Source Avg Loss : 0.3339\n",
      "Target Avg Loss : 1.3252\n",
      "Source Avg reg Loss : 0.0205\n",
      "Target Avg reg Loss : 0.0120\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0325\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 455, Total Avg Loss : 1.6545\n",
      "Source Avg Loss : 0.3294\n",
      "Target Avg Loss : 1.3251\n",
      "Source Avg reg Loss : 0.0161\n",
      "Target Avg reg Loss : 0.0119\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0280\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 456, Total Avg Loss : 1.6578\n",
      "Source Avg Loss : 0.3315\n",
      "Target Avg Loss : 1.3262\n",
      "Source Avg reg Loss : 0.0182\n",
      "Target Avg reg Loss : 0.0131\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0312\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 457, Total Avg Loss : 1.6824\n",
      "Source Avg Loss : 0.3464\n",
      "Target Avg Loss : 1.3360\n",
      "Source Avg reg Loss : 0.0330\n",
      "Target Avg reg Loss : 0.0228\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0558\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 458, Total Avg Loss : 1.6599\n",
      "Source Avg Loss : 0.3340\n",
      "Target Avg Loss : 1.3259\n",
      "Source Avg reg Loss : 0.0207\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0334\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 459, Total Avg Loss : 1.6541\n",
      "Source Avg Loss : 0.3290\n",
      "Target Avg Loss : 1.3250\n",
      "Source Avg reg Loss : 0.0157\n",
      "Target Avg reg Loss : 0.0119\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0276\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 460, Total Avg Loss : 1.6592\n",
      "Source Avg Loss : 0.3333\n",
      "Target Avg Loss : 1.3259\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0327\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 461, Total Avg Loss : 1.6708\n",
      "Source Avg Loss : 0.3407\n",
      "Target Avg Loss : 1.3301\n",
      "Source Avg reg Loss : 0.0274\n",
      "Target Avg reg Loss : 0.0169\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0443\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 462, Total Avg Loss : 1.6648\n",
      "Source Avg Loss : 0.3367\n",
      "Target Avg Loss : 1.3282\n",
      "Source Avg reg Loss : 0.0233\n",
      "Target Avg reg Loss : 0.0150\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0383\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 463, Total Avg Loss : 1.6586\n",
      "Source Avg Loss : 0.3335\n",
      "Target Avg Loss : 1.3251\n",
      "Source Avg reg Loss : 0.0201\n",
      "Target Avg reg Loss : 0.0120\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0321\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 464, Total Avg Loss : 1.6616\n",
      "Source Avg Loss : 0.3331\n",
      "Target Avg Loss : 1.3285\n",
      "Source Avg reg Loss : 0.0198\n",
      "Target Avg reg Loss : 0.0153\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0350\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 465, Total Avg Loss : 1.6593\n",
      "Source Avg Loss : 0.3334\n",
      "Target Avg Loss : 1.3259\n",
      "Source Avg reg Loss : 0.0201\n",
      "Target Avg reg Loss : 0.0127\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0327\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 466, Total Avg Loss : 1.6556\n",
      "Source Avg Loss : 0.3306\n",
      "Target Avg Loss : 1.3250\n",
      "Source Avg reg Loss : 0.0173\n",
      "Target Avg reg Loss : 0.0118\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0291\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 467, Total Avg Loss : 1.6659\n",
      "Source Avg Loss : 0.3379\n",
      "Target Avg Loss : 1.3280\n",
      "Source Avg reg Loss : 0.0245\n",
      "Target Avg reg Loss : 0.0149\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0394\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 468, Total Avg Loss : 1.6648\n",
      "Source Avg Loss : 0.3368\n",
      "Target Avg Loss : 1.3279\n",
      "Source Avg reg Loss : 0.0235\n",
      "Target Avg reg Loss : 0.0148\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0383\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 469, Total Avg Loss : 1.6803\n",
      "Source Avg Loss : 0.3463\n",
      "Target Avg Loss : 1.3340\n",
      "Source Avg reg Loss : 0.0329\n",
      "Target Avg reg Loss : 0.0208\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0538\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 470, Total Avg Loss : 1.6600\n",
      "Source Avg Loss : 0.3346\n",
      "Target Avg Loss : 1.3254\n",
      "Source Avg reg Loss : 0.0212\n",
      "Target Avg reg Loss : 0.0123\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0335\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 471, Total Avg Loss : 1.6584\n",
      "Source Avg Loss : 0.3319\n",
      "Target Avg Loss : 1.3266\n",
      "Source Avg reg Loss : 0.0185\n",
      "Target Avg reg Loss : 0.0134\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0319\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 472, Total Avg Loss : 1.6604\n",
      "Source Avg Loss : 0.3347\n",
      "Target Avg Loss : 1.3257\n",
      "Source Avg reg Loss : 0.0214\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0339\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 473, Total Avg Loss : 1.6562\n",
      "Source Avg Loss : 0.3313\n",
      "Target Avg Loss : 1.3249\n",
      "Source Avg reg Loss : 0.0179\n",
      "Target Avg reg Loss : 0.0118\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0297\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 474, Total Avg Loss : 1.6516\n",
      "Source Avg Loss : 0.3287\n",
      "Target Avg Loss : 1.3228\n",
      "Source Avg reg Loss : 0.0154\n",
      "Target Avg reg Loss : 0.0097\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0250\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 475, Total Avg Loss : 1.6521\n",
      "Source Avg Loss : 0.3287\n",
      "Target Avg Loss : 1.3234\n",
      "Source Avg reg Loss : 0.0153\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0255\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 476, Total Avg Loss : 1.6523\n",
      "Source Avg Loss : 0.3289\n",
      "Target Avg Loss : 1.3234\n",
      "Source Avg reg Loss : 0.0156\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0258\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 477, Total Avg Loss : 1.6694\n",
      "Source Avg Loss : 0.3391\n",
      "Target Avg Loss : 1.3304\n",
      "Source Avg reg Loss : 0.0257\n",
      "Target Avg reg Loss : 0.0172\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0429\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 478, Total Avg Loss : 1.6566\n",
      "Source Avg Loss : 0.3323\n",
      "Target Avg Loss : 1.3243\n",
      "Source Avg reg Loss : 0.0189\n",
      "Target Avg reg Loss : 0.0111\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0300\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 479, Total Avg Loss : 1.6516\n",
      "Source Avg Loss : 0.3283\n",
      "Target Avg Loss : 1.3233\n",
      "Source Avg reg Loss : 0.0149\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0251\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 480, Total Avg Loss : 1.6556\n",
      "Source Avg Loss : 0.3301\n",
      "Target Avg Loss : 1.3255\n",
      "Source Avg reg Loss : 0.0168\n",
      "Target Avg reg Loss : 0.0123\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0291\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 481, Total Avg Loss : 1.6757\n",
      "Source Avg Loss : 0.3429\n",
      "Target Avg Loss : 1.3328\n",
      "Source Avg reg Loss : 0.0296\n",
      "Target Avg reg Loss : 0.0196\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0492\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 482, Total Avg Loss : 1.6589\n",
      "Source Avg Loss : 0.3338\n",
      "Target Avg Loss : 1.3251\n",
      "Source Avg reg Loss : 0.0205\n",
      "Target Avg reg Loss : 0.0119\n",
      "Source Avg cla Loss : 0.3133\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0324\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 483, Total Avg Loss : 1.6508\n",
      "Source Avg Loss : 0.3279\n",
      "Target Avg Loss : 1.3230\n",
      "Source Avg reg Loss : 0.0145\n",
      "Target Avg reg Loss : 0.0098\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0243\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 484, Total Avg Loss : 1.6520\n",
      "Source Avg Loss : 0.3283\n",
      "Target Avg Loss : 1.3236\n",
      "Source Avg reg Loss : 0.0150\n",
      "Target Avg reg Loss : 0.0105\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0255\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 485, Total Avg Loss : 1.6719\n",
      "Source Avg Loss : 0.3420\n",
      "Target Avg Loss : 1.3299\n",
      "Source Avg reg Loss : 0.0286\n",
      "Target Avg reg Loss : 0.0168\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0454\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 486, Total Avg Loss : 1.6628\n",
      "Source Avg Loss : 0.3351\n",
      "Target Avg Loss : 1.3277\n",
      "Source Avg reg Loss : 0.0217\n",
      "Target Avg reg Loss : 0.0146\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0363\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 487, Total Avg Loss : 1.6547\n",
      "Source Avg Loss : 0.3310\n",
      "Target Avg Loss : 1.3238\n",
      "Source Avg reg Loss : 0.0176\n",
      "Target Avg reg Loss : 0.0106\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0282\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 488, Total Avg Loss : 1.6524\n",
      "Source Avg Loss : 0.3284\n",
      "Target Avg Loss : 1.3241\n",
      "Source Avg reg Loss : 0.0150\n",
      "Target Avg reg Loss : 0.0109\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0259\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 489, Total Avg Loss : 1.6586\n",
      "Source Avg Loss : 0.3329\n",
      "Target Avg Loss : 1.3256\n",
      "Source Avg reg Loss : 0.0196\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3132\n",
      "Avg Regression Loss : 0.0321\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 490, Total Avg Loss : 1.6530\n",
      "Source Avg Loss : 0.3292\n",
      "Target Avg Loss : 1.3238\n",
      "Source Avg reg Loss : 0.0158\n",
      "Target Avg reg Loss : 0.0107\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0265\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 491, Total Avg Loss : 1.6562\n",
      "Source Avg Loss : 0.3307\n",
      "Target Avg Loss : 1.3254\n",
      "Source Avg reg Loss : 0.0174\n",
      "Target Avg reg Loss : 0.0123\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0297\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 492, Total Avg Loss : 1.6589\n",
      "Source Avg Loss : 0.3340\n",
      "Target Avg Loss : 1.3249\n",
      "Source Avg reg Loss : 0.0207\n",
      "Target Avg reg Loss : 0.0118\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0324\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 493, Total Avg Loss : 1.6556\n",
      "Source Avg Loss : 0.3311\n",
      "Target Avg Loss : 1.3245\n",
      "Source Avg reg Loss : 0.0177\n",
      "Target Avg reg Loss : 0.0114\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0291\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 494, Total Avg Loss : 1.6518\n",
      "Source Avg Loss : 0.3286\n",
      "Target Avg Loss : 1.3232\n",
      "Source Avg reg Loss : 0.0152\n",
      "Target Avg reg Loss : 0.0100\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0253\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 495, Total Avg Loss : 1.6479\n",
      "Source Avg Loss : 0.3266\n",
      "Target Avg Loss : 1.3212\n",
      "Source Avg reg Loss : 0.0132\n",
      "Target Avg reg Loss : 0.0081\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0213\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 496, Total Avg Loss : 1.6466\n",
      "Source Avg Loss : 0.3253\n",
      "Target Avg Loss : 1.3213\n",
      "Source Avg reg Loss : 0.0119\n",
      "Target Avg reg Loss : 0.0082\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0201\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 497, Total Avg Loss : 1.6534\n",
      "Source Avg Loss : 0.3290\n",
      "Target Avg Loss : 1.3244\n",
      "Source Avg reg Loss : 0.0156\n",
      "Target Avg reg Loss : 0.0113\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0269\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 498, Total Avg Loss : 1.6536\n",
      "Source Avg Loss : 0.3296\n",
      "Target Avg Loss : 1.3239\n",
      "Source Avg reg Loss : 0.0163\n",
      "Target Avg reg Loss : 0.0108\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0271\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 499, Total Avg Loss : 1.6651\n",
      "Source Avg Loss : 0.3372\n",
      "Target Avg Loss : 1.3279\n",
      "Source Avg reg Loss : 0.0238\n",
      "Target Avg reg Loss : 0.0148\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0386\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 500, Total Avg Loss : 1.6632\n",
      "Source Avg Loss : 0.3365\n",
      "Target Avg Loss : 1.3268\n",
      "Source Avg reg Loss : 0.0231\n",
      "Target Avg reg Loss : 0.0137\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3131\n",
      "Avg Regression Loss : 0.0368\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 501, Total Avg Loss : 1.6527\n",
      "Source Avg Loss : 0.3296\n",
      "Target Avg Loss : 1.3231\n",
      "Source Avg reg Loss : 0.0162\n",
      "Target Avg reg Loss : 0.0100\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3130\n",
      "Avg Regression Loss : 0.0262\n",
      "Avg Classification Loss : 1.6265\n",
      "\n",
      "Epoch : 502, Total Avg Loss : 1.6503\n",
      "Source Avg Loss : 0.3285\n",
      "Target Avg Loss : 1.3218\n",
      "Source Avg reg Loss : 0.0151\n",
      "Target Avg reg Loss : 0.0089\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3129\n",
      "Avg Regression Loss : 0.0240\n",
      "Avg Classification Loss : 1.6263\n",
      "\n",
      "Epoch : 503, Total Avg Loss : 1.6525\n",
      "Source Avg Loss : 0.3299\n",
      "Target Avg Loss : 1.3226\n",
      "Source Avg reg Loss : 0.0164\n",
      "Target Avg reg Loss : 0.0104\n",
      "Source Avg cla Loss : 0.3134\n",
      "Target Avg cla Loss : 1.3122\n",
      "Avg Regression Loss : 0.0269\n",
      "Avg Classification Loss : 1.6256\n",
      "\n",
      "Epoch : 504, Total Avg Loss : 1.6441\n",
      "Source Avg Loss : 0.3276\n",
      "Target Avg Loss : 1.3165\n",
      "Source Avg reg Loss : 0.0140\n",
      "Target Avg reg Loss : 0.0103\n",
      "Source Avg cla Loss : 0.3136\n",
      "Target Avg cla Loss : 1.3062\n",
      "Avg Regression Loss : 0.0243\n",
      "Avg Classification Loss : 1.6198\n",
      "\n",
      "Epoch : 505, Total Avg Loss : 1.6403\n",
      "Source Avg Loss : 0.3334\n",
      "Target Avg Loss : 1.3070\n",
      "Source Avg reg Loss : 0.0196\n",
      "Target Avg reg Loss : 0.0156\n",
      "Source Avg cla Loss : 0.3138\n",
      "Target Avg cla Loss : 1.2914\n",
      "Avg Regression Loss : 0.0351\n",
      "Avg Classification Loss : 1.6052\n",
      "\n",
      "Epoch : 506, Total Avg Loss : 1.6394\n",
      "Source Avg Loss : 0.3358\n",
      "Target Avg Loss : 1.3036\n",
      "Source Avg reg Loss : 0.0209\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.3149\n",
      "Target Avg cla Loss : 1.2842\n",
      "Avg Regression Loss : 0.0403\n",
      "Avg Classification Loss : 1.5990\n",
      "\n",
      "Epoch : 507, Total Avg Loss : 1.6590\n",
      "Source Avg Loss : 0.3495\n",
      "Target Avg Loss : 1.3095\n",
      "Source Avg reg Loss : 0.0313\n",
      "Target Avg reg Loss : 0.0247\n",
      "Source Avg cla Loss : 0.3182\n",
      "Target Avg cla Loss : 1.2848\n",
      "Avg Regression Loss : 0.0560\n",
      "Avg Classification Loss : 1.6030\n",
      "\n",
      "Epoch : 508, Total Avg Loss : 1.7047\n",
      "Source Avg Loss : 0.3931\n",
      "Target Avg Loss : 1.3116\n",
      "Source Avg reg Loss : 0.0365\n",
      "Target Avg reg Loss : 0.0231\n",
      "Source Avg cla Loss : 0.3566\n",
      "Target Avg cla Loss : 1.2885\n",
      "Avg Regression Loss : 0.0595\n",
      "Avg Classification Loss : 1.6451\n",
      "\n",
      "Epoch : 509, Total Avg Loss : 1.7569\n",
      "Source Avg Loss : 0.4372\n",
      "Target Avg Loss : 1.3196\n",
      "Source Avg reg Loss : 0.0436\n",
      "Target Avg reg Loss : 0.0217\n",
      "Source Avg cla Loss : 0.3936\n",
      "Target Avg cla Loss : 1.2980\n",
      "Avg Regression Loss : 0.0653\n",
      "Avg Classification Loss : 1.6916\n",
      "\n",
      "Epoch : 510, Total Avg Loss : 1.7027\n",
      "Source Avg Loss : 0.3896\n",
      "Target Avg Loss : 1.3131\n",
      "Source Avg reg Loss : 0.0440\n",
      "Target Avg reg Loss : 0.0305\n",
      "Source Avg cla Loss : 0.3457\n",
      "Target Avg cla Loss : 1.2825\n",
      "Avg Regression Loss : 0.0745\n",
      "Avg Classification Loss : 1.6282\n",
      "\n",
      "Epoch : 511, Total Avg Loss : 1.6781\n",
      "Source Avg Loss : 0.3571\n",
      "Target Avg Loss : 1.3210\n",
      "Source Avg reg Loss : 0.0384\n",
      "Target Avg reg Loss : 0.0292\n",
      "Source Avg cla Loss : 0.3187\n",
      "Target Avg cla Loss : 1.2918\n",
      "Avg Regression Loss : 0.0676\n",
      "Avg Classification Loss : 1.6105\n",
      "\n",
      "Epoch : 512, Total Avg Loss : 1.6662\n",
      "Source Avg Loss : 0.3524\n",
      "Target Avg Loss : 1.3137\n",
      "Source Avg reg Loss : 0.0330\n",
      "Target Avg reg Loss : 0.0274\n",
      "Source Avg cla Loss : 0.3194\n",
      "Target Avg cla Loss : 1.2864\n",
      "Avg Regression Loss : 0.0604\n",
      "Avg Classification Loss : 1.6058\n",
      "\n",
      "Epoch : 513, Total Avg Loss : 1.6512\n",
      "Source Avg Loss : 0.3585\n",
      "Target Avg Loss : 1.2928\n",
      "Source Avg reg Loss : 0.0309\n",
      "Target Avg reg Loss : 0.0348\n",
      "Source Avg cla Loss : 0.3275\n",
      "Target Avg cla Loss : 1.2580\n",
      "Avg Regression Loss : 0.0657\n",
      "Avg Classification Loss : 1.5855\n",
      "\n",
      "Epoch : 514, Total Avg Loss : 1.5940\n",
      "Source Avg Loss : 0.3827\n",
      "Target Avg Loss : 1.2113\n",
      "Source Avg reg Loss : 0.0281\n",
      "Target Avg reg Loss : 0.0325\n",
      "Source Avg cla Loss : 0.3545\n",
      "Target Avg cla Loss : 1.1789\n",
      "Avg Regression Loss : 0.0606\n",
      "Avg Classification Loss : 1.5334\n",
      "\n",
      "Epoch : 515, Total Avg Loss : 1.5541\n",
      "Source Avg Loss : 0.5074\n",
      "Target Avg Loss : 1.0466\n",
      "Source Avg reg Loss : 0.0362\n",
      "Target Avg reg Loss : 0.0294\n",
      "Source Avg cla Loss : 0.4712\n",
      "Target Avg cla Loss : 1.0173\n",
      "Avg Regression Loss : 0.0656\n",
      "Avg Classification Loss : 1.4885\n",
      "\n",
      "Epoch : 516, Total Avg Loss : 1.8417\n",
      "Source Avg Loss : 0.6048\n",
      "Target Avg Loss : 1.2369\n",
      "Source Avg reg Loss : 0.0544\n",
      "Target Avg reg Loss : 0.0855\n",
      "Source Avg cla Loss : 0.5504\n",
      "Target Avg cla Loss : 1.1513\n",
      "Avg Regression Loss : 0.1399\n",
      "Avg Classification Loss : 1.7018\n",
      "\n",
      "Epoch : 517, Total Avg Loss : 1.9000\n",
      "Source Avg Loss : 0.6003\n",
      "Target Avg Loss : 1.2997\n",
      "Source Avg reg Loss : 0.0582\n",
      "Target Avg reg Loss : 0.0900\n",
      "Source Avg cla Loss : 0.5421\n",
      "Target Avg cla Loss : 1.2097\n",
      "Avg Regression Loss : 0.1481\n",
      "Avg Classification Loss : 1.7519\n",
      "\n",
      "Epoch : 518, Total Avg Loss : 1.9264\n",
      "Source Avg Loss : 0.6652\n",
      "Target Avg Loss : 1.2612\n",
      "Source Avg reg Loss : 0.0689\n",
      "Target Avg reg Loss : 0.0688\n",
      "Source Avg cla Loss : 0.5963\n",
      "Target Avg cla Loss : 1.1923\n",
      "Avg Regression Loss : 0.1377\n",
      "Avg Classification Loss : 1.7886\n",
      "\n",
      "Epoch : 519, Total Avg Loss : 1.9594\n",
      "Source Avg Loss : 0.7109\n",
      "Target Avg Loss : 1.2485\n",
      "Source Avg reg Loss : 0.0666\n",
      "Target Avg reg Loss : 0.0965\n",
      "Source Avg cla Loss : 0.6442\n",
      "Target Avg cla Loss : 1.1521\n",
      "Avg Regression Loss : 0.1631\n",
      "Avg Classification Loss : 1.7963\n",
      "\n",
      "Epoch : 520, Total Avg Loss : 1.7068\n",
      "Source Avg Loss : 0.7420\n",
      "Target Avg Loss : 0.9648\n",
      "Source Avg reg Loss : 0.0964\n",
      "Target Avg reg Loss : 0.1021\n",
      "Source Avg cla Loss : 0.6456\n",
      "Target Avg cla Loss : 0.8627\n",
      "Avg Regression Loss : 0.1985\n",
      "Avg Classification Loss : 1.5083\n",
      "\n",
      "Epoch : 521, Total Avg Loss : 1.5333\n",
      "Source Avg Loss : 0.7823\n",
      "Target Avg Loss : 0.7511\n",
      "Source Avg reg Loss : 0.0935\n",
      "Target Avg reg Loss : 0.0547\n",
      "Source Avg cla Loss : 0.6888\n",
      "Target Avg cla Loss : 0.6963\n",
      "Avg Regression Loss : 0.1482\n",
      "Avg Classification Loss : 1.3851\n",
      "\n",
      "Epoch : 522, Total Avg Loss : 1.4876\n",
      "Source Avg Loss : 0.7494\n",
      "Target Avg Loss : 0.7382\n",
      "Source Avg reg Loss : 0.0614\n",
      "Target Avg reg Loss : 0.0425\n",
      "Source Avg cla Loss : 0.6880\n",
      "Target Avg cla Loss : 0.6957\n",
      "Avg Regression Loss : 0.1038\n",
      "Avg Classification Loss : 1.3838\n",
      "\n",
      "Epoch : 523, Total Avg Loss : 1.4642\n",
      "Source Avg Loss : 0.7345\n",
      "Target Avg Loss : 0.7297\n",
      "Source Avg reg Loss : 0.0488\n",
      "Target Avg reg Loss : 0.0328\n",
      "Source Avg cla Loss : 0.6857\n",
      "Target Avg cla Loss : 0.6969\n",
      "Avg Regression Loss : 0.0816\n",
      "Avg Classification Loss : 1.3826\n",
      "\n",
      "Epoch : 524, Total Avg Loss : 1.4502\n",
      "Source Avg Loss : 0.7218\n",
      "Target Avg Loss : 0.7284\n",
      "Source Avg reg Loss : 0.0397\n",
      "Target Avg reg Loss : 0.0289\n",
      "Source Avg cla Loss : 0.6822\n",
      "Target Avg cla Loss : 0.6994\n",
      "Avg Regression Loss : 0.0686\n",
      "Avg Classification Loss : 1.3816\n",
      "\n",
      "Epoch : 525, Total Avg Loss : 1.4410\n",
      "Source Avg Loss : 0.7119\n",
      "Target Avg Loss : 0.7291\n",
      "Source Avg reg Loss : 0.0374\n",
      "Target Avg reg Loss : 0.0255\n",
      "Source Avg cla Loss : 0.6745\n",
      "Target Avg cla Loss : 0.7036\n",
      "Avg Regression Loss : 0.0629\n",
      "Avg Classification Loss : 1.3780\n",
      "\n",
      "Epoch : 526, Total Avg Loss : 1.4285\n",
      "Source Avg Loss : 0.6911\n",
      "Target Avg Loss : 0.7373\n",
      "Source Avg reg Loss : 0.0357\n",
      "Target Avg reg Loss : 0.0237\n",
      "Source Avg cla Loss : 0.6555\n",
      "Target Avg cla Loss : 0.7136\n",
      "Avg Regression Loss : 0.0594\n",
      "Avg Classification Loss : 1.3690\n",
      "\n",
      "Epoch : 527, Total Avg Loss : 1.4141\n",
      "Source Avg Loss : 0.6520\n",
      "Target Avg Loss : 0.7621\n",
      "Source Avg reg Loss : 0.0383\n",
      "Target Avg reg Loss : 0.0232\n",
      "Source Avg cla Loss : 0.6137\n",
      "Target Avg cla Loss : 0.7389\n",
      "Avg Regression Loss : 0.0615\n",
      "Avg Classification Loss : 1.3526\n",
      "\n",
      "Epoch : 528, Total Avg Loss : 1.4093\n",
      "Source Avg Loss : 0.5964\n",
      "Target Avg Loss : 0.8129\n",
      "Source Avg reg Loss : 0.0392\n",
      "Target Avg reg Loss : 0.0251\n",
      "Source Avg cla Loss : 0.5572\n",
      "Target Avg cla Loss : 0.7878\n",
      "Avg Regression Loss : 0.0643\n",
      "Avg Classification Loss : 1.3450\n",
      "\n",
      "Epoch : 529, Total Avg Loss : 1.4647\n",
      "Source Avg Loss : 0.5348\n",
      "Target Avg Loss : 0.9298\n",
      "Source Avg reg Loss : 0.0404\n",
      "Target Avg reg Loss : 0.0341\n",
      "Source Avg cla Loss : 0.4944\n",
      "Target Avg cla Loss : 0.8958\n",
      "Avg Regression Loss : 0.0745\n",
      "Avg Classification Loss : 1.3902\n",
      "\n",
      "Epoch : 530, Total Avg Loss : 1.5753\n",
      "Source Avg Loss : 0.5453\n",
      "Target Avg Loss : 1.0300\n",
      "Source Avg reg Loss : 0.0477\n",
      "Target Avg reg Loss : 0.0474\n",
      "Source Avg cla Loss : 0.4976\n",
      "Target Avg cla Loss : 0.9825\n",
      "Avg Regression Loss : 0.0952\n",
      "Avg Classification Loss : 1.4801\n",
      "\n",
      "Epoch : 531, Total Avg Loss : 1.5021\n",
      "Source Avg Loss : 0.6317\n",
      "Target Avg Loss : 0.8704\n",
      "Source Avg reg Loss : 0.0518\n",
      "Target Avg reg Loss : 0.0351\n",
      "Source Avg cla Loss : 0.5799\n",
      "Target Avg cla Loss : 0.8353\n",
      "Avg Regression Loss : 0.0869\n",
      "Avg Classification Loss : 1.4152\n",
      "\n",
      "Epoch : 532, Total Avg Loss : 1.4835\n",
      "Source Avg Loss : 0.6669\n",
      "Target Avg Loss : 0.8165\n",
      "Source Avg reg Loss : 0.0497\n",
      "Target Avg reg Loss : 0.0304\n",
      "Source Avg cla Loss : 0.6172\n",
      "Target Avg cla Loss : 0.7861\n",
      "Avg Regression Loss : 0.0801\n",
      "Avg Classification Loss : 1.4033\n",
      "\n",
      "Epoch : 533, Total Avg Loss : 1.4660\n",
      "Source Avg Loss : 0.6809\n",
      "Target Avg Loss : 0.7851\n",
      "Source Avg reg Loss : 0.0419\n",
      "Target Avg reg Loss : 0.0249\n",
      "Source Avg cla Loss : 0.6390\n",
      "Target Avg cla Loss : 0.7602\n",
      "Avg Regression Loss : 0.0668\n",
      "Avg Classification Loss : 1.3992\n",
      "\n",
      "Epoch : 534, Total Avg Loss : 1.4601\n",
      "Source Avg Loss : 0.6940\n",
      "Target Avg Loss : 0.7661\n",
      "Source Avg reg Loss : 0.0378\n",
      "Target Avg reg Loss : 0.0231\n",
      "Source Avg cla Loss : 0.6562\n",
      "Target Avg cla Loss : 0.7429\n",
      "Avg Regression Loss : 0.0609\n",
      "Avg Classification Loss : 1.3992\n",
      "\n",
      "Epoch : 535, Total Avg Loss : 1.4508\n",
      "Source Avg Loss : 0.7039\n",
      "Target Avg Loss : 0.7468\n",
      "Source Avg reg Loss : 0.0350\n",
      "Target Avg reg Loss : 0.0214\n",
      "Source Avg cla Loss : 0.6689\n",
      "Target Avg cla Loss : 0.7254\n",
      "Avg Regression Loss : 0.0564\n",
      "Avg Classification Loss : 1.3943\n",
      "\n",
      "Epoch : 536, Total Avg Loss : 1.4528\n",
      "Source Avg Loss : 0.7155\n",
      "Target Avg Loss : 0.7373\n",
      "Source Avg reg Loss : 0.0371\n",
      "Target Avg reg Loss : 0.0232\n",
      "Source Avg cla Loss : 0.6785\n",
      "Target Avg cla Loss : 0.7141\n",
      "Avg Regression Loss : 0.0602\n",
      "Avg Classification Loss : 1.3926\n",
      "\n",
      "Epoch : 537, Total Avg Loss : 1.4468\n",
      "Source Avg Loss : 0.7157\n",
      "Target Avg Loss : 0.7311\n",
      "Source Avg reg Loss : 0.0335\n",
      "Target Avg reg Loss : 0.0223\n",
      "Source Avg cla Loss : 0.6822\n",
      "Target Avg cla Loss : 0.7088\n",
      "Avg Regression Loss : 0.0558\n",
      "Avg Classification Loss : 1.3910\n",
      "\n",
      "Epoch : 538, Total Avg Loss : 1.4477\n",
      "Source Avg Loss : 0.7176\n",
      "Target Avg Loss : 0.7301\n",
      "Source Avg reg Loss : 0.0351\n",
      "Target Avg reg Loss : 0.0232\n",
      "Source Avg cla Loss : 0.6825\n",
      "Target Avg cla Loss : 0.7068\n",
      "Avg Regression Loss : 0.0584\n",
      "Avg Classification Loss : 1.3893\n",
      "\n",
      "Epoch : 539, Total Avg Loss : 1.4402\n",
      "Source Avg Loss : 0.7140\n",
      "Target Avg Loss : 0.7261\n",
      "Source Avg reg Loss : 0.0298\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6842\n",
      "Target Avg cla Loss : 0.7061\n",
      "Avg Regression Loss : 0.0498\n",
      "Avg Classification Loss : 1.3904\n",
      "\n",
      "Epoch : 540, Total Avg Loss : 1.4499\n",
      "Source Avg Loss : 0.7217\n",
      "Target Avg Loss : 0.7282\n",
      "Source Avg reg Loss : 0.0370\n",
      "Target Avg reg Loss : 0.0238\n",
      "Source Avg cla Loss : 0.6847\n",
      "Target Avg cla Loss : 0.7044\n",
      "Avg Regression Loss : 0.0608\n",
      "Avg Classification Loss : 1.3891\n",
      "\n",
      "Epoch : 541, Total Avg Loss : 1.4364\n",
      "Source Avg Loss : 0.7134\n",
      "Target Avg Loss : 0.7230\n",
      "Source Avg reg Loss : 0.0288\n",
      "Target Avg reg Loss : 0.0187\n",
      "Source Avg cla Loss : 0.6846\n",
      "Target Avg cla Loss : 0.7043\n",
      "Avg Regression Loss : 0.0475\n",
      "Avg Classification Loss : 1.3889\n",
      "\n",
      "Epoch : 542, Total Avg Loss : 1.4429\n",
      "Source Avg Loss : 0.7169\n",
      "Target Avg Loss : 0.7260\n",
      "Source Avg reg Loss : 0.0335\n",
      "Target Avg reg Loss : 0.0214\n",
      "Source Avg cla Loss : 0.6834\n",
      "Target Avg cla Loss : 0.7046\n",
      "Avg Regression Loss : 0.0549\n",
      "Avg Classification Loss : 1.3880\n",
      "\n",
      "Epoch : 543, Total Avg Loss : 1.4380\n",
      "Source Avg Loss : 0.7123\n",
      "Target Avg Loss : 0.7258\n",
      "Source Avg reg Loss : 0.0292\n",
      "Target Avg reg Loss : 0.0203\n",
      "Source Avg cla Loss : 0.6830\n",
      "Target Avg cla Loss : 0.7055\n",
      "Avg Regression Loss : 0.0495\n",
      "Avg Classification Loss : 1.3886\n",
      "\n",
      "Epoch : 544, Total Avg Loss : 1.4559\n",
      "Source Avg Loss : 0.7248\n",
      "Target Avg Loss : 0.7310\n",
      "Source Avg reg Loss : 0.0413\n",
      "Target Avg reg Loss : 0.0280\n",
      "Source Avg cla Loss : 0.6836\n",
      "Target Avg cla Loss : 0.7030\n",
      "Avg Regression Loss : 0.0693\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 545, Total Avg Loss : 1.4417\n",
      "Source Avg Loss : 0.7153\n",
      "Target Avg Loss : 0.7264\n",
      "Source Avg reg Loss : 0.0337\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6817\n",
      "Target Avg cla Loss : 0.7063\n",
      "Avg Regression Loss : 0.0537\n",
      "Avg Classification Loss : 1.3880\n",
      "\n",
      "Epoch : 546, Total Avg Loss : 1.4296\n",
      "Source Avg Loss : 0.7069\n",
      "Target Avg Loss : 0.7226\n",
      "Source Avg reg Loss : 0.0262\n",
      "Target Avg reg Loss : 0.0157\n",
      "Source Avg cla Loss : 0.6807\n",
      "Target Avg cla Loss : 0.7070\n",
      "Avg Regression Loss : 0.0419\n",
      "Avg Classification Loss : 1.3877\n",
      "\n",
      "Epoch : 547, Total Avg Loss : 1.4323\n",
      "Source Avg Loss : 0.7100\n",
      "Target Avg Loss : 0.7223\n",
      "Source Avg reg Loss : 0.0274\n",
      "Target Avg reg Loss : 0.0160\n",
      "Source Avg cla Loss : 0.6826\n",
      "Target Avg cla Loss : 0.7064\n",
      "Avg Regression Loss : 0.0434\n",
      "Avg Classification Loss : 1.3889\n",
      "\n",
      "Epoch : 548, Total Avg Loss : 1.4289\n",
      "Source Avg Loss : 0.7099\n",
      "Target Avg Loss : 0.7190\n",
      "Source Avg reg Loss : 0.0253\n",
      "Target Avg reg Loss : 0.0160\n",
      "Source Avg cla Loss : 0.6845\n",
      "Target Avg cla Loss : 0.7030\n",
      "Avg Regression Loss : 0.0414\n",
      "Avg Classification Loss : 1.3875\n",
      "\n",
      "Epoch : 549, Total Avg Loss : 1.4272\n",
      "Source Avg Loss : 0.7090\n",
      "Target Avg Loss : 0.7182\n",
      "Source Avg reg Loss : 0.0243\n",
      "Target Avg reg Loss : 0.0142\n",
      "Source Avg cla Loss : 0.6846\n",
      "Target Avg cla Loss : 0.7040\n",
      "Avg Regression Loss : 0.0386\n",
      "Avg Classification Loss : 1.3886\n",
      "\n",
      "Epoch : 550, Total Avg Loss : 1.4254\n",
      "Source Avg Loss : 0.7065\n",
      "Target Avg Loss : 0.7188\n",
      "Source Avg reg Loss : 0.0222\n",
      "Target Avg reg Loss : 0.0139\n",
      "Source Avg cla Loss : 0.6843\n",
      "Target Avg cla Loss : 0.7049\n",
      "Avg Regression Loss : 0.0362\n",
      "Avg Classification Loss : 1.3892\n",
      "\n",
      "Epoch : 551, Total Avg Loss : 1.4270\n",
      "Source Avg Loss : 0.7088\n",
      "Target Avg Loss : 0.7182\n",
      "Source Avg reg Loss : 0.0232\n",
      "Target Avg reg Loss : 0.0136\n",
      "Source Avg cla Loss : 0.6856\n",
      "Target Avg cla Loss : 0.7046\n",
      "Avg Regression Loss : 0.0368\n",
      "Avg Classification Loss : 1.3902\n",
      "\n",
      "Epoch : 552, Total Avg Loss : 1.4265\n",
      "Source Avg Loss : 0.7096\n",
      "Target Avg Loss : 0.7168\n",
      "Source Avg reg Loss : 0.0218\n",
      "Target Avg reg Loss : 0.0142\n",
      "Source Avg cla Loss : 0.6878\n",
      "Target Avg cla Loss : 0.7026\n",
      "Avg Regression Loss : 0.0360\n",
      "Avg Classification Loss : 1.3904\n",
      "\n",
      "Epoch : 553, Total Avg Loss : 1.4247\n",
      "Source Avg Loss : 0.7115\n",
      "Target Avg Loss : 0.7132\n",
      "Source Avg reg Loss : 0.0218\n",
      "Target Avg reg Loss : 0.0136\n",
      "Source Avg cla Loss : 0.6897\n",
      "Target Avg cla Loss : 0.6996\n",
      "Avg Regression Loss : 0.0353\n",
      "Avg Classification Loss : 1.3893\n",
      "\n",
      "Epoch : 554, Total Avg Loss : 1.4238\n",
      "Source Avg Loss : 0.7127\n",
      "Target Avg Loss : 0.7111\n",
      "Source Avg reg Loss : 0.0209\n",
      "Target Avg reg Loss : 0.0143\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6968\n",
      "Avg Regression Loss : 0.0352\n",
      "Avg Classification Loss : 1.3886\n",
      "\n",
      "Epoch : 555, Total Avg Loss : 1.4199\n",
      "Source Avg Loss : 0.7123\n",
      "Target Avg Loss : 0.7077\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0126\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0324\n",
      "Avg Classification Loss : 1.3875\n",
      "\n",
      "Epoch : 556, Total Avg Loss : 1.4172\n",
      "Source Avg Loss : 0.7109\n",
      "Target Avg Loss : 0.7063\n",
      "Source Avg reg Loss : 0.0182\n",
      "Target Avg reg Loss : 0.0117\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0299\n",
      "Avg Classification Loss : 1.3873\n",
      "\n",
      "Epoch : 557, Total Avg Loss : 1.4180\n",
      "Source Avg Loss : 0.7110\n",
      "Target Avg Loss : 0.7070\n",
      "Source Avg reg Loss : 0.0183\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0308\n",
      "Avg Classification Loss : 1.3872\n",
      "\n",
      "Epoch : 558, Total Avg Loss : 1.4265\n",
      "Source Avg Loss : 0.7159\n",
      "Target Avg Loss : 0.7106\n",
      "Source Avg reg Loss : 0.0231\n",
      "Target Avg reg Loss : 0.0163\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.0394\n",
      "Avg Classification Loss : 1.3871\n",
      "\n",
      "Epoch : 559, Total Avg Loss : 1.4182\n",
      "Source Avg Loss : 0.7127\n",
      "Target Avg Loss : 0.7055\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0113\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6942\n",
      "Avg Regression Loss : 0.0313\n",
      "Avg Classification Loss : 1.3870\n",
      "\n",
      "Epoch : 560, Total Avg Loss : 1.4155\n",
      "Source Avg Loss : 0.7097\n",
      "Target Avg Loss : 0.7058\n",
      "Source Avg reg Loss : 0.0169\n",
      "Target Avg reg Loss : 0.0116\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6942\n",
      "Avg Regression Loss : 0.0285\n",
      "Avg Classification Loss : 1.3870\n",
      "\n",
      "Epoch : 561, Total Avg Loss : 1.4150\n",
      "Source Avg Loss : 0.7098\n",
      "Target Avg Loss : 0.7052\n",
      "Source Avg reg Loss : 0.0170\n",
      "Target Avg reg Loss : 0.0110\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0280\n",
      "Avg Classification Loss : 1.3870\n",
      "\n",
      "Epoch : 562, Total Avg Loss : 1.4221\n",
      "Source Avg Loss : 0.7151\n",
      "Target Avg Loss : 0.7069\n",
      "Source Avg reg Loss : 0.0223\n",
      "Target Avg reg Loss : 0.0130\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.0352\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 563, Total Avg Loss : 1.4322\n",
      "Source Avg Loss : 0.7215\n",
      "Target Avg Loss : 0.7107\n",
      "Source Avg reg Loss : 0.0287\n",
      "Target Avg reg Loss : 0.0167\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.0454\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 564, Total Avg Loss : 1.4283\n",
      "Source Avg Loss : 0.7168\n",
      "Target Avg Loss : 0.7114\n",
      "Source Avg reg Loss : 0.0239\n",
      "Target Avg reg Loss : 0.0175\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0415\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 565, Total Avg Loss : 1.4386\n",
      "Source Avg Loss : 0.7245\n",
      "Target Avg Loss : 0.7141\n",
      "Source Avg reg Loss : 0.0316\n",
      "Target Avg reg Loss : 0.0204\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0520\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 566, Total Avg Loss : 1.4280\n",
      "Source Avg Loss : 0.7170\n",
      "Target Avg Loss : 0.7110\n",
      "Source Avg reg Loss : 0.0241\n",
      "Target Avg reg Loss : 0.0171\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0412\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 567, Total Avg Loss : 1.4266\n",
      "Source Avg Loss : 0.7168\n",
      "Target Avg Loss : 0.7098\n",
      "Source Avg reg Loss : 0.0239\n",
      "Target Avg reg Loss : 0.0161\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0400\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 568, Total Avg Loss : 1.4189\n",
      "Source Avg Loss : 0.7125\n",
      "Target Avg Loss : 0.7064\n",
      "Source Avg reg Loss : 0.0196\n",
      "Target Avg reg Loss : 0.0127\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0323\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 569, Total Avg Loss : 1.4217\n",
      "Source Avg Loss : 0.7141\n",
      "Target Avg Loss : 0.7076\n",
      "Source Avg reg Loss : 0.0212\n",
      "Target Avg reg Loss : 0.0137\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0349\n",
      "Avg Classification Loss : 1.3867\n",
      "\n",
      "Epoch : 570, Total Avg Loss : 1.4207\n",
      "Source Avg Loss : 0.7133\n",
      "Target Avg Loss : 0.7074\n",
      "Source Avg reg Loss : 0.0204\n",
      "Target Avg reg Loss : 0.0137\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0341\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 571, Total Avg Loss : 1.4302\n",
      "Source Avg Loss : 0.7197\n",
      "Target Avg Loss : 0.7105\n",
      "Source Avg reg Loss : 0.0268\n",
      "Target Avg reg Loss : 0.0168\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0436\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 572, Total Avg Loss : 1.4255\n",
      "Source Avg Loss : 0.7153\n",
      "Target Avg Loss : 0.7101\n",
      "Source Avg reg Loss : 0.0224\n",
      "Target Avg reg Loss : 0.0164\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0388\n",
      "Avg Classification Loss : 1.3867\n",
      "\n",
      "Epoch : 573, Total Avg Loss : 1.4297\n",
      "Source Avg Loss : 0.7192\n",
      "Target Avg Loss : 0.7104\n",
      "Source Avg reg Loss : 0.0263\n",
      "Target Avg reg Loss : 0.0168\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0431\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 574, Total Avg Loss : 1.4199\n",
      "Source Avg Loss : 0.7128\n",
      "Target Avg Loss : 0.7071\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0135\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0334\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 575, Total Avg Loss : 1.4159\n",
      "Source Avg Loss : 0.7107\n",
      "Target Avg Loss : 0.7052\n",
      "Source Avg reg Loss : 0.0178\n",
      "Target Avg reg Loss : 0.0115\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0293\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 576, Total Avg Loss : 1.4158\n",
      "Source Avg Loss : 0.7107\n",
      "Target Avg Loss : 0.7051\n",
      "Source Avg reg Loss : 0.0178\n",
      "Target Avg reg Loss : 0.0115\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0293\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 577, Total Avg Loss : 1.4134\n",
      "Source Avg Loss : 0.7093\n",
      "Target Avg Loss : 0.7040\n",
      "Source Avg reg Loss : 0.0164\n",
      "Target Avg reg Loss : 0.0104\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0268\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 578, Total Avg Loss : 1.4104\n",
      "Source Avg Loss : 0.7073\n",
      "Target Avg Loss : 0.7031\n",
      "Source Avg reg Loss : 0.0144\n",
      "Target Avg reg Loss : 0.0094\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0239\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 579, Total Avg Loss : 1.4086\n",
      "Source Avg Loss : 0.7064\n",
      "Target Avg Loss : 0.7022\n",
      "Source Avg reg Loss : 0.0135\n",
      "Target Avg reg Loss : 0.0086\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0221\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 580, Total Avg Loss : 1.4153\n",
      "Source Avg Loss : 0.7104\n",
      "Target Avg Loss : 0.7050\n",
      "Source Avg reg Loss : 0.0174\n",
      "Target Avg reg Loss : 0.0114\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0288\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 581, Total Avg Loss : 1.4209\n",
      "Source Avg Loss : 0.7142\n",
      "Target Avg Loss : 0.7067\n",
      "Source Avg reg Loss : 0.0213\n",
      "Target Avg reg Loss : 0.0131\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0344\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 582, Total Avg Loss : 1.4259\n",
      "Source Avg Loss : 0.7170\n",
      "Target Avg Loss : 0.7089\n",
      "Source Avg reg Loss : 0.0241\n",
      "Target Avg reg Loss : 0.0153\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0393\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 583, Total Avg Loss : 1.4239\n",
      "Source Avg Loss : 0.7155\n",
      "Target Avg Loss : 0.7084\n",
      "Source Avg reg Loss : 0.0226\n",
      "Target Avg reg Loss : 0.0148\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0374\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 584, Total Avg Loss : 1.4138\n",
      "Source Avg Loss : 0.7097\n",
      "Target Avg Loss : 0.7041\n",
      "Source Avg reg Loss : 0.0168\n",
      "Target Avg reg Loss : 0.0105\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0273\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 585, Total Avg Loss : 1.4250\n",
      "Source Avg Loss : 0.7175\n",
      "Target Avg Loss : 0.7075\n",
      "Source Avg reg Loss : 0.0245\n",
      "Target Avg reg Loss : 0.0140\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0385\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 586, Total Avg Loss : 1.4218\n",
      "Source Avg Loss : 0.7135\n",
      "Target Avg Loss : 0.7082\n",
      "Source Avg reg Loss : 0.0206\n",
      "Target Avg reg Loss : 0.0146\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0352\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 587, Total Avg Loss : 1.4301\n",
      "Source Avg Loss : 0.7196\n",
      "Target Avg Loss : 0.7105\n",
      "Source Avg reg Loss : 0.0267\n",
      "Target Avg reg Loss : 0.0169\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0436\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 588, Total Avg Loss : 1.4231\n",
      "Source Avg Loss : 0.7139\n",
      "Target Avg Loss : 0.7091\n",
      "Source Avg reg Loss : 0.0210\n",
      "Target Avg reg Loss : 0.0156\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0366\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 589, Total Avg Loss : 1.4455\n",
      "Source Avg Loss : 0.7276\n",
      "Target Avg Loss : 0.7178\n",
      "Source Avg reg Loss : 0.0347\n",
      "Target Avg reg Loss : 0.0243\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0590\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 590, Total Avg Loss : 1.4218\n",
      "Source Avg Loss : 0.7146\n",
      "Target Avg Loss : 0.7072\n",
      "Source Avg reg Loss : 0.0217\n",
      "Target Avg reg Loss : 0.0136\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0353\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 591, Total Avg Loss : 1.4200\n",
      "Source Avg Loss : 0.7127\n",
      "Target Avg Loss : 0.7073\n",
      "Source Avg reg Loss : 0.0198\n",
      "Target Avg reg Loss : 0.0138\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0336\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 592, Total Avg Loss : 1.4164\n",
      "Source Avg Loss : 0.7113\n",
      "Target Avg Loss : 0.7051\n",
      "Source Avg reg Loss : 0.0184\n",
      "Target Avg reg Loss : 0.0115\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0300\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 593, Total Avg Loss : 1.4142\n",
      "Source Avg Loss : 0.7099\n",
      "Target Avg Loss : 0.7044\n",
      "Source Avg reg Loss : 0.0169\n",
      "Target Avg reg Loss : 0.0108\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0278\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 594, Total Avg Loss : 1.4145\n",
      "Source Avg Loss : 0.7089\n",
      "Target Avg Loss : 0.7055\n",
      "Source Avg reg Loss : 0.0160\n",
      "Target Avg reg Loss : 0.0120\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0280\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 595, Total Avg Loss : 1.4338\n",
      "Source Avg Loss : 0.7222\n",
      "Target Avg Loss : 0.7116\n",
      "Source Avg reg Loss : 0.0292\n",
      "Target Avg reg Loss : 0.0181\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0473\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 596, Total Avg Loss : 1.4220\n",
      "Source Avg Loss : 0.7138\n",
      "Target Avg Loss : 0.7082\n",
      "Source Avg reg Loss : 0.0209\n",
      "Target Avg reg Loss : 0.0147\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0356\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 597, Total Avg Loss : 1.4338\n",
      "Source Avg Loss : 0.7216\n",
      "Target Avg Loss : 0.7122\n",
      "Source Avg reg Loss : 0.0287\n",
      "Target Avg reg Loss : 0.0187\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0474\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 598, Total Avg Loss : 1.4244\n",
      "Source Avg Loss : 0.7149\n",
      "Target Avg Loss : 0.7095\n",
      "Source Avg reg Loss : 0.0220\n",
      "Target Avg reg Loss : 0.0160\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0380\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 599, Total Avg Loss : 1.4225\n",
      "Source Avg Loss : 0.7142\n",
      "Target Avg Loss : 0.7083\n",
      "Source Avg reg Loss : 0.0213\n",
      "Target Avg reg Loss : 0.0148\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0361\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 600, Total Avg Loss : 1.4148\n",
      "Source Avg Loss : 0.7096\n",
      "Target Avg Loss : 0.7052\n",
      "Source Avg reg Loss : 0.0167\n",
      "Target Avg reg Loss : 0.0117\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0285\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 601, Total Avg Loss : 1.4191\n",
      "Source Avg Loss : 0.7128\n",
      "Target Avg Loss : 0.7063\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0327\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 602, Total Avg Loss : 1.4144\n",
      "Source Avg Loss : 0.7096\n",
      "Target Avg Loss : 0.7048\n",
      "Source Avg reg Loss : 0.0167\n",
      "Target Avg reg Loss : 0.0113\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0280\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 603, Total Avg Loss : 1.4327\n",
      "Source Avg Loss : 0.7215\n",
      "Target Avg Loss : 0.7112\n",
      "Source Avg reg Loss : 0.0286\n",
      "Target Avg reg Loss : 0.0178\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0464\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 604, Total Avg Loss : 1.4244\n",
      "Source Avg Loss : 0.7147\n",
      "Target Avg Loss : 0.7098\n",
      "Source Avg reg Loss : 0.0218\n",
      "Target Avg reg Loss : 0.0162\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0380\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 605, Total Avg Loss : 1.4203\n",
      "Source Avg Loss : 0.7129\n",
      "Target Avg Loss : 0.7074\n",
      "Source Avg reg Loss : 0.0200\n",
      "Target Avg reg Loss : 0.0139\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0339\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 606, Total Avg Loss : 1.4139\n",
      "Source Avg Loss : 0.7097\n",
      "Target Avg Loss : 0.7042\n",
      "Source Avg reg Loss : 0.0169\n",
      "Target Avg reg Loss : 0.0107\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0276\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 607, Total Avg Loss : 1.4132\n",
      "Source Avg Loss : 0.7087\n",
      "Target Avg Loss : 0.7045\n",
      "Source Avg reg Loss : 0.0158\n",
      "Target Avg reg Loss : 0.0110\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0269\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 608, Total Avg Loss : 1.4235\n",
      "Source Avg Loss : 0.7156\n",
      "Target Avg Loss : 0.7079\n",
      "Source Avg reg Loss : 0.0228\n",
      "Target Avg reg Loss : 0.0144\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0372\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 609, Total Avg Loss : 1.4187\n",
      "Source Avg Loss : 0.7114\n",
      "Target Avg Loss : 0.7073\n",
      "Source Avg reg Loss : 0.0185\n",
      "Target Avg reg Loss : 0.0138\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0323\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 610, Total Avg Loss : 1.4360\n",
      "Source Avg Loss : 0.7232\n",
      "Target Avg Loss : 0.7129\n",
      "Source Avg reg Loss : 0.0303\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0497\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 611, Total Avg Loss : 1.4161\n",
      "Source Avg Loss : 0.7113\n",
      "Target Avg Loss : 0.7048\n",
      "Source Avg reg Loss : 0.0185\n",
      "Target Avg reg Loss : 0.0113\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0298\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 612, Total Avg Loss : 1.4089\n",
      "Source Avg Loss : 0.7064\n",
      "Target Avg Loss : 0.7025\n",
      "Source Avg reg Loss : 0.0135\n",
      "Target Avg reg Loss : 0.0090\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0226\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 613, Total Avg Loss : 1.4121\n",
      "Source Avg Loss : 0.7091\n",
      "Target Avg Loss : 0.7030\n",
      "Source Avg reg Loss : 0.0163\n",
      "Target Avg reg Loss : 0.0094\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0257\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 614, Total Avg Loss : 1.4158\n",
      "Source Avg Loss : 0.7098\n",
      "Target Avg Loss : 0.7060\n",
      "Source Avg reg Loss : 0.0169\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0294\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 615, Total Avg Loss : 1.4340\n",
      "Source Avg Loss : 0.7223\n",
      "Target Avg Loss : 0.7117\n",
      "Source Avg reg Loss : 0.0296\n",
      "Target Avg reg Loss : 0.0181\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0477\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 616, Total Avg Loss : 1.4201\n",
      "Source Avg Loss : 0.7134\n",
      "Target Avg Loss : 0.7068\n",
      "Source Avg reg Loss : 0.0206\n",
      "Target Avg reg Loss : 0.0132\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0338\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 617, Total Avg Loss : 1.4094\n",
      "Source Avg Loss : 0.7062\n",
      "Target Avg Loss : 0.7032\n",
      "Source Avg reg Loss : 0.0135\n",
      "Target Avg reg Loss : 0.0096\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0231\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 618, Total Avg Loss : 1.4084\n",
      "Source Avg Loss : 0.7055\n",
      "Target Avg Loss : 0.7029\n",
      "Source Avg reg Loss : 0.0128\n",
      "Target Avg reg Loss : 0.0093\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0221\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 619, Total Avg Loss : 1.4217\n",
      "Source Avg Loss : 0.7150\n",
      "Target Avg Loss : 0.7067\n",
      "Source Avg reg Loss : 0.0223\n",
      "Target Avg reg Loss : 0.0131\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0354\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 620, Total Avg Loss : 1.4177\n",
      "Source Avg Loss : 0.7110\n",
      "Target Avg Loss : 0.7067\n",
      "Source Avg reg Loss : 0.0184\n",
      "Target Avg reg Loss : 0.0131\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0315\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 621, Total Avg Loss : 1.4359\n",
      "Source Avg Loss : 0.7238\n",
      "Target Avg Loss : 0.7121\n",
      "Source Avg reg Loss : 0.0312\n",
      "Target Avg reg Loss : 0.0184\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0497\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 622, Total Avg Loss : 1.4205\n",
      "Source Avg Loss : 0.7132\n",
      "Target Avg Loss : 0.7073\n",
      "Source Avg reg Loss : 0.0206\n",
      "Target Avg reg Loss : 0.0137\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0343\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 623, Total Avg Loss : 1.4057\n",
      "Source Avg Loss : 0.7040\n",
      "Target Avg Loss : 0.7017\n",
      "Source Avg reg Loss : 0.0115\n",
      "Target Avg reg Loss : 0.0080\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0196\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 624, Total Avg Loss : 1.4084\n",
      "Source Avg Loss : 0.7051\n",
      "Target Avg Loss : 0.7033\n",
      "Source Avg reg Loss : 0.0128\n",
      "Target Avg reg Loss : 0.0095\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0223\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 625, Total Avg Loss : 1.4149\n",
      "Source Avg Loss : 0.7105\n",
      "Target Avg Loss : 0.7044\n",
      "Source Avg reg Loss : 0.0183\n",
      "Target Avg reg Loss : 0.0106\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0288\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 626, Total Avg Loss : 1.4218\n",
      "Source Avg Loss : 0.7142\n",
      "Target Avg Loss : 0.7076\n",
      "Source Avg reg Loss : 0.0222\n",
      "Target Avg reg Loss : 0.0136\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.0357\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 627, Total Avg Loss : 1.4170\n",
      "Source Avg Loss : 0.7093\n",
      "Target Avg Loss : 0.7077\n",
      "Source Avg reg Loss : 0.0175\n",
      "Target Avg reg Loss : 0.0135\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6942\n",
      "Avg Regression Loss : 0.0310\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 628, Total Avg Loss : 1.4278\n",
      "Source Avg Loss : 0.7180\n",
      "Target Avg Loss : 0.7098\n",
      "Source Avg reg Loss : 0.0268\n",
      "Target Avg reg Loss : 0.0154\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0422\n",
      "Avg Classification Loss : 1.3856\n",
      "\n",
      "Epoch : 629, Total Avg Loss : 1.4254\n",
      "Source Avg Loss : 0.7167\n",
      "Target Avg Loss : 0.7087\n",
      "Source Avg reg Loss : 0.0264\n",
      "Target Avg reg Loss : 0.0137\n",
      "Source Avg cla Loss : 0.6902\n",
      "Target Avg cla Loss : 0.6950\n",
      "Avg Regression Loss : 0.0402\n",
      "Avg Classification Loss : 1.3852\n",
      "\n",
      "Epoch : 630, Total Avg Loss : 1.4138\n",
      "Source Avg Loss : 0.7061\n",
      "Target Avg Loss : 0.7077\n",
      "Source Avg reg Loss : 0.0174\n",
      "Target Avg reg Loss : 0.0116\n",
      "Source Avg cla Loss : 0.6887\n",
      "Target Avg cla Loss : 0.6961\n",
      "Avg Regression Loss : 0.0290\n",
      "Avg Classification Loss : 1.3848\n",
      "\n",
      "Epoch : 631, Total Avg Loss : 1.4275\n",
      "Source Avg Loss : 0.7123\n",
      "Target Avg Loss : 0.7152\n",
      "Source Avg reg Loss : 0.0271\n",
      "Target Avg reg Loss : 0.0169\n",
      "Source Avg cla Loss : 0.6852\n",
      "Target Avg cla Loss : 0.6982\n",
      "Avg Regression Loss : 0.0440\n",
      "Avg Classification Loss : 1.3835\n",
      "\n",
      "Epoch : 632, Total Avg Loss : 1.4124\n",
      "Source Avg Loss : 0.6978\n",
      "Target Avg Loss : 0.7145\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0122\n",
      "Source Avg cla Loss : 0.6780\n",
      "Target Avg cla Loss : 0.7023\n",
      "Avg Regression Loss : 0.0321\n",
      "Avg Classification Loss : 1.3803\n",
      "\n",
      "Epoch : 633, Total Avg Loss : 1.3952\n",
      "Source Avg Loss : 0.6726\n",
      "Target Avg Loss : 0.7227\n",
      "Source Avg reg Loss : 0.0138\n",
      "Target Avg reg Loss : 0.0091\n",
      "Source Avg cla Loss : 0.6588\n",
      "Target Avg cla Loss : 0.7135\n",
      "Avg Regression Loss : 0.0229\n",
      "Avg Classification Loss : 1.3723\n",
      "\n",
      "Epoch : 634, Total Avg Loss : 1.3938\n",
      "Source Avg Loss : 0.6523\n",
      "Target Avg Loss : 0.7415\n",
      "Source Avg reg Loss : 0.0154\n",
      "Target Avg reg Loss : 0.0111\n",
      "Source Avg cla Loss : 0.6369\n",
      "Target Avg cla Loss : 0.7304\n",
      "Avg Regression Loss : 0.0266\n",
      "Avg Classification Loss : 1.3672\n",
      "\n",
      "Epoch : 635, Total Avg Loss : 1.4050\n",
      "Source Avg Loss : 0.6230\n",
      "Target Avg Loss : 0.7819\n",
      "Source Avg reg Loss : 0.0181\n",
      "Target Avg reg Loss : 0.0127\n",
      "Source Avg cla Loss : 0.6050\n",
      "Target Avg cla Loss : 0.7692\n",
      "Avg Regression Loss : 0.0308\n",
      "Avg Classification Loss : 1.3742\n",
      "\n",
      "Epoch : 636, Total Avg Loss : 1.3873\n",
      "Source Avg Loss : 0.5271\n",
      "Target Avg Loss : 0.8602\n",
      "Source Avg reg Loss : 0.0214\n",
      "Target Avg reg Loss : 0.0149\n",
      "Source Avg cla Loss : 0.5057\n",
      "Target Avg cla Loss : 0.8453\n",
      "Avg Regression Loss : 0.0363\n",
      "Avg Classification Loss : 1.3510\n",
      "\n",
      "Epoch : 637, Total Avg Loss : 1.3891\n",
      "Source Avg Loss : 0.4764\n",
      "Target Avg Loss : 0.9127\n",
      "Source Avg reg Loss : 0.0255\n",
      "Target Avg reg Loss : 0.0185\n",
      "Source Avg cla Loss : 0.4509\n",
      "Target Avg cla Loss : 0.8941\n",
      "Avg Regression Loss : 0.0440\n",
      "Avg Classification Loss : 1.3451\n",
      "\n",
      "Epoch : 638, Total Avg Loss : 1.4429\n",
      "Source Avg Loss : 0.4839\n",
      "Target Avg Loss : 0.9590\n",
      "Source Avg reg Loss : 0.0356\n",
      "Target Avg reg Loss : 0.0269\n",
      "Source Avg cla Loss : 0.4483\n",
      "Target Avg cla Loss : 0.9321\n",
      "Avg Regression Loss : 0.0625\n",
      "Avg Classification Loss : 1.3804\n",
      "\n",
      "Epoch : 639, Total Avg Loss : 1.5501\n",
      "Source Avg Loss : 0.5344\n",
      "Target Avg Loss : 1.0157\n",
      "Source Avg reg Loss : 0.0465\n",
      "Target Avg reg Loss : 0.0331\n",
      "Source Avg cla Loss : 0.4879\n",
      "Target Avg cla Loss : 0.9826\n",
      "Avg Regression Loss : 0.0796\n",
      "Avg Classification Loss : 1.4704\n",
      "\n",
      "Epoch : 640, Total Avg Loss : 1.7027\n",
      "Source Avg Loss : 0.6411\n",
      "Target Avg Loss : 1.0617\n",
      "Source Avg reg Loss : 0.0576\n",
      "Target Avg reg Loss : 0.0530\n",
      "Source Avg cla Loss : 0.5835\n",
      "Target Avg cla Loss : 1.0087\n",
      "Avg Regression Loss : 0.1106\n",
      "Avg Classification Loss : 1.5922\n",
      "\n",
      "Epoch : 641, Total Avg Loss : 1.7416\n",
      "Source Avg Loss : 0.6594\n",
      "Target Avg Loss : 1.0823\n",
      "Source Avg reg Loss : 0.0685\n",
      "Target Avg reg Loss : 0.0576\n",
      "Source Avg cla Loss : 0.5909\n",
      "Target Avg cla Loss : 1.0247\n",
      "Avg Regression Loss : 0.1260\n",
      "Avg Classification Loss : 1.6156\n",
      "\n",
      "Epoch : 642, Total Avg Loss : 1.6044\n",
      "Source Avg Loss : 0.6121\n",
      "Target Avg Loss : 0.9924\n",
      "Source Avg reg Loss : 0.0780\n",
      "Target Avg reg Loss : 0.0486\n",
      "Source Avg cla Loss : 0.5341\n",
      "Target Avg cla Loss : 0.9437\n",
      "Avg Regression Loss : 0.1266\n",
      "Avg Classification Loss : 1.4778\n",
      "\n",
      "Epoch : 643, Total Avg Loss : 1.4145\n",
      "Source Avg Loss : 0.5880\n",
      "Target Avg Loss : 0.8266\n",
      "Source Avg reg Loss : 0.0684\n",
      "Target Avg reg Loss : 0.0360\n",
      "Source Avg cla Loss : 0.5196\n",
      "Target Avg cla Loss : 0.7905\n",
      "Avg Regression Loss : 0.1044\n",
      "Avg Classification Loss : 1.3101\n",
      "\n",
      "Epoch : 644, Total Avg Loss : 1.4488\n",
      "Source Avg Loss : 0.6988\n",
      "Target Avg Loss : 0.7500\n",
      "Source Avg reg Loss : 0.0585\n",
      "Target Avg reg Loss : 0.0279\n",
      "Source Avg cla Loss : 0.6403\n",
      "Target Avg cla Loss : 0.7221\n",
      "Avg Regression Loss : 0.0864\n",
      "Avg Classification Loss : 1.3624\n",
      "\n",
      "Epoch : 645, Total Avg Loss : 1.4695\n",
      "Source Avg Loss : 0.7309\n",
      "Target Avg Loss : 0.7385\n",
      "Source Avg reg Loss : 0.0502\n",
      "Target Avg reg Loss : 0.0249\n",
      "Source Avg cla Loss : 0.6808\n",
      "Target Avg cla Loss : 0.7136\n",
      "Avg Regression Loss : 0.0751\n",
      "Avg Classification Loss : 1.3944\n",
      "\n",
      "Epoch : 646, Total Avg Loss : 1.4707\n",
      "Source Avg Loss : 0.7314\n",
      "Target Avg Loss : 0.7393\n",
      "Source Avg reg Loss : 0.0468\n",
      "Target Avg reg Loss : 0.0268\n",
      "Source Avg cla Loss : 0.6846\n",
      "Target Avg cla Loss : 0.7124\n",
      "Avg Regression Loss : 0.0736\n",
      "Avg Classification Loss : 1.3970\n",
      "\n",
      "Epoch : 647, Total Avg Loss : 1.4674\n",
      "Source Avg Loss : 0.7307\n",
      "Target Avg Loss : 0.7367\n",
      "Source Avg reg Loss : 0.0443\n",
      "Target Avg reg Loss : 0.0270\n",
      "Source Avg cla Loss : 0.6863\n",
      "Target Avg cla Loss : 0.7097\n",
      "Avg Regression Loss : 0.0713\n",
      "Avg Classification Loss : 1.3960\n",
      "\n",
      "Epoch : 648, Total Avg Loss : 1.4592\n",
      "Source Avg Loss : 0.7257\n",
      "Target Avg Loss : 0.7334\n",
      "Source Avg reg Loss : 0.0384\n",
      "Target Avg reg Loss : 0.0248\n",
      "Source Avg cla Loss : 0.6873\n",
      "Target Avg cla Loss : 0.7087\n",
      "Avg Regression Loss : 0.0632\n",
      "Avg Classification Loss : 1.3960\n",
      "\n",
      "Epoch : 649, Total Avg Loss : 1.4486\n",
      "Source Avg Loss : 0.7221\n",
      "Target Avg Loss : 0.7265\n",
      "Source Avg reg Loss : 0.0340\n",
      "Target Avg reg Loss : 0.0206\n",
      "Source Avg cla Loss : 0.6880\n",
      "Target Avg cla Loss : 0.7059\n",
      "Avg Regression Loss : 0.0547\n",
      "Avg Classification Loss : 1.3939\n",
      "\n",
      "Epoch : 650, Total Avg Loss : 1.4433\n",
      "Source Avg Loss : 0.7204\n",
      "Target Avg Loss : 0.7229\n",
      "Source Avg reg Loss : 0.0316\n",
      "Target Avg reg Loss : 0.0186\n",
      "Source Avg cla Loss : 0.6888\n",
      "Target Avg cla Loss : 0.7043\n",
      "Avg Regression Loss : 0.0502\n",
      "Avg Classification Loss : 1.3931\n",
      "\n",
      "Epoch : 651, Total Avg Loss : 1.4354\n",
      "Source Avg Loss : 0.7155\n",
      "Target Avg Loss : 0.7199\n",
      "Source Avg reg Loss : 0.0266\n",
      "Target Avg reg Loss : 0.0158\n",
      "Source Avg cla Loss : 0.6889\n",
      "Target Avg cla Loss : 0.7042\n",
      "Avg Regression Loss : 0.0423\n",
      "Avg Classification Loss : 1.3931\n",
      "\n",
      "Epoch : 652, Total Avg Loss : 1.4299\n",
      "Source Avg Loss : 0.7115\n",
      "Target Avg Loss : 0.7183\n",
      "Source Avg reg Loss : 0.0221\n",
      "Target Avg reg Loss : 0.0145\n",
      "Source Avg cla Loss : 0.6894\n",
      "Target Avg cla Loss : 0.7038\n",
      "Avg Regression Loss : 0.0367\n",
      "Avg Classification Loss : 1.3932\n",
      "\n",
      "Epoch : 653, Total Avg Loss : 1.4363\n",
      "Source Avg Loss : 0.7166\n",
      "Target Avg Loss : 0.7197\n",
      "Source Avg reg Loss : 0.0270\n",
      "Target Avg reg Loss : 0.0167\n",
      "Source Avg cla Loss : 0.6896\n",
      "Target Avg cla Loss : 0.7030\n",
      "Avg Regression Loss : 0.0437\n",
      "Avg Classification Loss : 1.3926\n",
      "\n",
      "Epoch : 654, Total Avg Loss : 1.4283\n",
      "Source Avg Loss : 0.7129\n",
      "Target Avg Loss : 0.7154\n",
      "Source Avg reg Loss : 0.0231\n",
      "Target Avg reg Loss : 0.0144\n",
      "Source Avg cla Loss : 0.6898\n",
      "Target Avg cla Loss : 0.7010\n",
      "Avg Regression Loss : 0.0375\n",
      "Avg Classification Loss : 1.3908\n",
      "\n",
      "Epoch : 655, Total Avg Loss : 1.4439\n",
      "Source Avg Loss : 0.7206\n",
      "Target Avg Loss : 0.7233\n",
      "Source Avg reg Loss : 0.0309\n",
      "Target Avg reg Loss : 0.0221\n",
      "Source Avg cla Loss : 0.6897\n",
      "Target Avg cla Loss : 0.7012\n",
      "Avg Regression Loss : 0.0530\n",
      "Avg Classification Loss : 1.3910\n",
      "\n",
      "Epoch : 656, Total Avg Loss : 1.4276\n",
      "Source Avg Loss : 0.7135\n",
      "Target Avg Loss : 0.7141\n",
      "Source Avg reg Loss : 0.0237\n",
      "Target Avg reg Loss : 0.0139\n",
      "Source Avg cla Loss : 0.6898\n",
      "Target Avg cla Loss : 0.7002\n",
      "Avg Regression Loss : 0.0375\n",
      "Avg Classification Loss : 1.3901\n",
      "\n",
      "Epoch : 657, Total Avg Loss : 1.4171\n",
      "Source Avg Loss : 0.7076\n",
      "Target Avg Loss : 0.7095\n",
      "Source Avg reg Loss : 0.0179\n",
      "Target Avg reg Loss : 0.0114\n",
      "Source Avg cla Loss : 0.6897\n",
      "Target Avg cla Loss : 0.6982\n",
      "Avg Regression Loss : 0.0293\n",
      "Avg Classification Loss : 1.3879\n",
      "\n",
      "Epoch : 658, Total Avg Loss : 1.4209\n",
      "Source Avg Loss : 0.7110\n",
      "Target Avg Loss : 0.7099\n",
      "Source Avg reg Loss : 0.0215\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.6895\n",
      "Target Avg cla Loss : 0.6971\n",
      "Avg Regression Loss : 0.0342\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 659, Total Avg Loss : 1.4212\n",
      "Source Avg Loss : 0.7108\n",
      "Target Avg Loss : 0.7105\n",
      "Source Avg reg Loss : 0.0220\n",
      "Target Avg reg Loss : 0.0137\n",
      "Source Avg cla Loss : 0.6888\n",
      "Target Avg cla Loss : 0.6968\n",
      "Avg Regression Loss : 0.0356\n",
      "Avg Classification Loss : 1.3856\n",
      "\n",
      "Epoch : 660, Total Avg Loss : 1.4365\n",
      "Source Avg Loss : 0.7182\n",
      "Target Avg Loss : 0.7183\n",
      "Source Avg reg Loss : 0.0304\n",
      "Target Avg reg Loss : 0.0210\n",
      "Source Avg cla Loss : 0.6878\n",
      "Target Avg cla Loss : 0.6972\n",
      "Avg Regression Loss : 0.0515\n",
      "Avg Classification Loss : 1.3850\n",
      "\n",
      "Epoch : 661, Total Avg Loss : 1.4134\n",
      "Source Avg Loss : 0.7038\n",
      "Target Avg Loss : 0.7096\n",
      "Source Avg reg Loss : 0.0186\n",
      "Target Avg reg Loss : 0.0117\n",
      "Source Avg cla Loss : 0.6852\n",
      "Target Avg cla Loss : 0.6979\n",
      "Avg Regression Loss : 0.0303\n",
      "Avg Classification Loss : 1.3831\n",
      "\n",
      "Epoch : 662, Total Avg Loss : 1.4223\n",
      "Source Avg Loss : 0.7033\n",
      "Target Avg Loss : 0.7190\n",
      "Source Avg reg Loss : 0.0262\n",
      "Target Avg reg Loss : 0.0177\n",
      "Source Avg cla Loss : 0.6771\n",
      "Target Avg cla Loss : 0.7013\n",
      "Avg Regression Loss : 0.0439\n",
      "Avg Classification Loss : 1.3784\n",
      "\n",
      "Epoch : 663, Total Avg Loss : 1.4048\n",
      "Source Avg Loss : 0.6786\n",
      "Target Avg Loss : 0.7262\n",
      "Source Avg reg Loss : 0.0222\n",
      "Target Avg reg Loss : 0.0139\n",
      "Source Avg cla Loss : 0.6564\n",
      "Target Avg cla Loss : 0.7123\n",
      "Avg Regression Loss : 0.0361\n",
      "Avg Classification Loss : 1.3688\n",
      "\n",
      "Epoch : 664, Total Avg Loss : 1.4094\n",
      "Source Avg Loss : 0.6434\n",
      "Target Avg Loss : 0.7660\n",
      "Source Avg reg Loss : 0.0291\n",
      "Target Avg reg Loss : 0.0190\n",
      "Source Avg cla Loss : 0.6144\n",
      "Target Avg cla Loss : 0.7470\n",
      "Avg Regression Loss : 0.0481\n",
      "Avg Classification Loss : 1.3613\n",
      "\n",
      "Epoch : 665, Total Avg Loss : 1.4099\n",
      "Source Avg Loss : 0.5804\n",
      "Target Avg Loss : 0.8295\n",
      "Source Avg reg Loss : 0.0248\n",
      "Target Avg reg Loss : 0.0142\n",
      "Source Avg cla Loss : 0.5556\n",
      "Target Avg cla Loss : 0.8153\n",
      "Avg Regression Loss : 0.0390\n",
      "Avg Classification Loss : 1.3709\n",
      "\n",
      "Epoch : 666, Total Avg Loss : 1.4430\n",
      "Source Avg Loss : 0.5320\n",
      "Target Avg Loss : 0.9110\n",
      "Source Avg reg Loss : 0.0228\n",
      "Target Avg reg Loss : 0.0197\n",
      "Source Avg cla Loss : 0.5092\n",
      "Target Avg cla Loss : 0.8913\n",
      "Avg Regression Loss : 0.0425\n",
      "Avg Classification Loss : 1.4005\n",
      "\n",
      "Epoch : 667, Total Avg Loss : 1.4825\n",
      "Source Avg Loss : 0.5357\n",
      "Target Avg Loss : 0.9468\n",
      "Source Avg reg Loss : 0.0371\n",
      "Target Avg reg Loss : 0.0277\n",
      "Source Avg cla Loss : 0.4986\n",
      "Target Avg cla Loss : 0.9191\n",
      "Avg Regression Loss : 0.0648\n",
      "Avg Classification Loss : 1.4177\n",
      "\n",
      "Epoch : 668, Total Avg Loss : 1.5167\n",
      "Source Avg Loss : 0.6045\n",
      "Target Avg Loss : 0.9121\n",
      "Source Avg reg Loss : 0.0437\n",
      "Target Avg reg Loss : 0.0332\n",
      "Source Avg cla Loss : 0.5608\n",
      "Target Avg cla Loss : 0.8789\n",
      "Avg Regression Loss : 0.0769\n",
      "Avg Classification Loss : 1.4397\n",
      "\n",
      "Epoch : 669, Total Avg Loss : 1.5313\n",
      "Source Avg Loss : 0.7020\n",
      "Target Avg Loss : 0.8294\n",
      "Source Avg reg Loss : 0.0461\n",
      "Target Avg reg Loss : 0.0422\n",
      "Source Avg cla Loss : 0.6558\n",
      "Target Avg cla Loss : 0.7872\n",
      "Avg Regression Loss : 0.0883\n",
      "Avg Classification Loss : 1.4430\n",
      "\n",
      "Epoch : 670, Total Avg Loss : 1.4908\n",
      "Source Avg Loss : 0.7278\n",
      "Target Avg Loss : 0.7630\n",
      "Source Avg reg Loss : 0.0405\n",
      "Target Avg reg Loss : 0.0338\n",
      "Source Avg cla Loss : 0.6873\n",
      "Target Avg cla Loss : 0.7292\n",
      "Avg Regression Loss : 0.0743\n",
      "Avg Classification Loss : 1.4165\n",
      "\n",
      "Epoch : 671, Total Avg Loss : 1.4677\n",
      "Source Avg Loss : 0.7253\n",
      "Target Avg Loss : 0.7424\n",
      "Source Avg reg Loss : 0.0351\n",
      "Target Avg reg Loss : 0.0292\n",
      "Source Avg cla Loss : 0.6902\n",
      "Target Avg cla Loss : 0.7131\n",
      "Avg Regression Loss : 0.0644\n",
      "Avg Classification Loss : 1.4034\n",
      "\n",
      "Epoch : 672, Total Avg Loss : 1.4429\n",
      "Source Avg Loss : 0.7222\n",
      "Target Avg Loss : 0.7207\n",
      "Source Avg reg Loss : 0.0302\n",
      "Target Avg reg Loss : 0.0221\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6986\n",
      "Avg Regression Loss : 0.0524\n",
      "Avg Classification Loss : 1.3906\n",
      "\n",
      "Epoch : 673, Total Avg Loss : 1.4343\n",
      "Source Avg Loss : 0.7200\n",
      "Target Avg Loss : 0.7142\n",
      "Source Avg reg Loss : 0.0275\n",
      "Target Avg reg Loss : 0.0191\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0466\n",
      "Avg Classification Loss : 1.3876\n",
      "\n",
      "Epoch : 674, Total Avg Loss : 1.4293\n",
      "Source Avg Loss : 0.7176\n",
      "Target Avg Loss : 0.7117\n",
      "Source Avg reg Loss : 0.0250\n",
      "Target Avg reg Loss : 0.0171\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0422\n",
      "Avg Classification Loss : 1.3872\n",
      "\n",
      "Epoch : 675, Total Avg Loss : 1.4262\n",
      "Source Avg Loss : 0.7161\n",
      "Target Avg Loss : 0.7101\n",
      "Source Avg reg Loss : 0.0235\n",
      "Target Avg reg Loss : 0.0157\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0392\n",
      "Avg Classification Loss : 1.3870\n",
      "\n",
      "Epoch : 676, Total Avg Loss : 1.4218\n",
      "Source Avg Loss : 0.7136\n",
      "Target Avg Loss : 0.7081\n",
      "Source Avg reg Loss : 0.0211\n",
      "Target Avg reg Loss : 0.0138\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.0349\n",
      "Avg Classification Loss : 1.3869\n",
      "\n",
      "Epoch : 677, Total Avg Loss : 1.4191\n",
      "Source Avg Loss : 0.7122\n",
      "Target Avg Loss : 0.7069\n",
      "Source Avg reg Loss : 0.0196\n",
      "Target Avg reg Loss : 0.0127\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6942\n",
      "Avg Regression Loss : 0.0323\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 678, Total Avg Loss : 1.4163\n",
      "Source Avg Loss : 0.7106\n",
      "Target Avg Loss : 0.7057\n",
      "Source Avg reg Loss : 0.0180\n",
      "Target Avg reg Loss : 0.0116\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0296\n",
      "Avg Classification Loss : 1.3867\n",
      "\n",
      "Epoch : 679, Total Avg Loss : 1.4170\n",
      "Source Avg Loss : 0.7112\n",
      "Target Avg Loss : 0.7058\n",
      "Source Avg reg Loss : 0.0186\n",
      "Target Avg reg Loss : 0.0119\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0304\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 680, Total Avg Loss : 1.4125\n",
      "Source Avg Loss : 0.7086\n",
      "Target Avg Loss : 0.7039\n",
      "Source Avg reg Loss : 0.0159\n",
      "Target Avg reg Loss : 0.0101\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0260\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 681, Total Avg Loss : 1.4130\n",
      "Source Avg Loss : 0.7087\n",
      "Target Avg Loss : 0.7043\n",
      "Source Avg reg Loss : 0.0161\n",
      "Target Avg reg Loss : 0.0105\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0265\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 682, Total Avg Loss : 1.4128\n",
      "Source Avg Loss : 0.7091\n",
      "Target Avg Loss : 0.7037\n",
      "Source Avg reg Loss : 0.0166\n",
      "Target Avg reg Loss : 0.0098\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0264\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 683, Total Avg Loss : 1.4158\n",
      "Source Avg Loss : 0.7108\n",
      "Target Avg Loss : 0.7049\n",
      "Source Avg reg Loss : 0.0182\n",
      "Target Avg reg Loss : 0.0112\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0294\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 684, Total Avg Loss : 1.4113\n",
      "Source Avg Loss : 0.7076\n",
      "Target Avg Loss : 0.7037\n",
      "Source Avg reg Loss : 0.0150\n",
      "Target Avg reg Loss : 0.0100\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0250\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 685, Total Avg Loss : 1.4214\n",
      "Source Avg Loss : 0.7158\n",
      "Target Avg Loss : 0.7056\n",
      "Source Avg reg Loss : 0.0232\n",
      "Target Avg reg Loss : 0.0120\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0352\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 686, Total Avg Loss : 1.4317\n",
      "Source Avg Loss : 0.7207\n",
      "Target Avg Loss : 0.7110\n",
      "Source Avg reg Loss : 0.0282\n",
      "Target Avg reg Loss : 0.0173\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0455\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 687, Total Avg Loss : 1.4198\n",
      "Source Avg Loss : 0.7136\n",
      "Target Avg Loss : 0.7062\n",
      "Source Avg reg Loss : 0.0211\n",
      "Target Avg reg Loss : 0.0126\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0337\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 688, Total Avg Loss : 1.4320\n",
      "Source Avg Loss : 0.7180\n",
      "Target Avg Loss : 0.7140\n",
      "Source Avg reg Loss : 0.0256\n",
      "Target Avg reg Loss : 0.0203\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0459\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 689, Total Avg Loss : 1.4227\n",
      "Source Avg Loss : 0.7149\n",
      "Target Avg Loss : 0.7078\n",
      "Source Avg reg Loss : 0.0225\n",
      "Target Avg reg Loss : 0.0141\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0367\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 690, Total Avg Loss : 1.4242\n",
      "Source Avg Loss : 0.7143\n",
      "Target Avg Loss : 0.7099\n",
      "Source Avg reg Loss : 0.0220\n",
      "Target Avg reg Loss : 0.0161\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0382\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 691, Total Avg Loss : 1.4225\n",
      "Source Avg Loss : 0.7146\n",
      "Target Avg Loss : 0.7080\n",
      "Source Avg reg Loss : 0.0222\n",
      "Target Avg reg Loss : 0.0143\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0365\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 692, Total Avg Loss : 1.4297\n",
      "Source Avg Loss : 0.7169\n",
      "Target Avg Loss : 0.7129\n",
      "Source Avg reg Loss : 0.0246\n",
      "Target Avg reg Loss : 0.0191\n",
      "Source Avg cla Loss : 0.6923\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0437\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 693, Total Avg Loss : 1.4165\n",
      "Source Avg Loss : 0.7113\n",
      "Target Avg Loss : 0.7052\n",
      "Source Avg reg Loss : 0.0191\n",
      "Target Avg reg Loss : 0.0116\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0307\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 694, Total Avg Loss : 1.4228\n",
      "Source Avg Loss : 0.7138\n",
      "Target Avg Loss : 0.7090\n",
      "Source Avg reg Loss : 0.0216\n",
      "Target Avg reg Loss : 0.0153\n",
      "Source Avg cla Loss : 0.6921\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0369\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 695, Total Avg Loss : 1.4246\n",
      "Source Avg Loss : 0.7159\n",
      "Target Avg Loss : 0.7087\n",
      "Source Avg reg Loss : 0.0238\n",
      "Target Avg reg Loss : 0.0151\n",
      "Source Avg cla Loss : 0.6921\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0388\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 696, Total Avg Loss : 1.4273\n",
      "Source Avg Loss : 0.7158\n",
      "Target Avg Loss : 0.7115\n",
      "Source Avg reg Loss : 0.0238\n",
      "Target Avg reg Loss : 0.0178\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0416\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 697, Total Avg Loss : 1.4164\n",
      "Source Avg Loss : 0.7109\n",
      "Target Avg Loss : 0.7054\n",
      "Source Avg reg Loss : 0.0190\n",
      "Target Avg reg Loss : 0.0118\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0308\n",
      "Avg Classification Loss : 1.3856\n",
      "\n",
      "Epoch : 698, Total Avg Loss : 1.4270\n",
      "Source Avg Loss : 0.7157\n",
      "Target Avg Loss : 0.7113\n",
      "Source Avg reg Loss : 0.0240\n",
      "Target Avg reg Loss : 0.0176\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0416\n",
      "Avg Classification Loss : 1.3854\n",
      "\n",
      "Epoch : 699, Total Avg Loss : 1.4117\n",
      "Source Avg Loss : 0.7070\n",
      "Target Avg Loss : 0.7047\n",
      "Source Avg reg Loss : 0.0154\n",
      "Target Avg reg Loss : 0.0109\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0263\n",
      "Avg Classification Loss : 1.3853\n",
      "\n",
      "Epoch : 700, Total Avg Loss : 1.4255\n",
      "Source Avg Loss : 0.7150\n",
      "Target Avg Loss : 0.7105\n",
      "Source Avg reg Loss : 0.0235\n",
      "Target Avg reg Loss : 0.0167\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0402\n",
      "Avg Classification Loss : 1.3853\n",
      "\n",
      "Epoch : 701, Total Avg Loss : 1.4210\n",
      "Source Avg Loss : 0.7132\n",
      "Target Avg Loss : 0.7079\n",
      "Source Avg reg Loss : 0.0221\n",
      "Target Avg reg Loss : 0.0139\n",
      "Source Avg cla Loss : 0.6911\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0360\n",
      "Avg Classification Loss : 1.3851\n",
      "\n",
      "Epoch : 702, Total Avg Loss : 1.4244\n",
      "Source Avg Loss : 0.7132\n",
      "Target Avg Loss : 0.7112\n",
      "Source Avg reg Loss : 0.0224\n",
      "Target Avg reg Loss : 0.0172\n",
      "Source Avg cla Loss : 0.6907\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.0396\n",
      "Avg Classification Loss : 1.3847\n",
      "\n",
      "Epoch : 703, Total Avg Loss : 1.4134\n",
      "Source Avg Loss : 0.7084\n",
      "Target Avg Loss : 0.7050\n",
      "Source Avg reg Loss : 0.0178\n",
      "Target Avg reg Loss : 0.0110\n",
      "Source Avg cla Loss : 0.6906\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.0288\n",
      "Avg Classification Loss : 1.3846\n",
      "\n",
      "Epoch : 704, Total Avg Loss : 1.4257\n",
      "Source Avg Loss : 0.7138\n",
      "Target Avg Loss : 0.7119\n",
      "Source Avg reg Loss : 0.0237\n",
      "Target Avg reg Loss : 0.0175\n",
      "Source Avg cla Loss : 0.6901\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.0413\n",
      "Avg Classification Loss : 1.3844\n",
      "\n",
      "Epoch : 705, Total Avg Loss : 1.4071\n",
      "Source Avg Loss : 0.7032\n",
      "Target Avg Loss : 0.7039\n",
      "Source Avg reg Loss : 0.0138\n",
      "Target Avg reg Loss : 0.0095\n",
      "Source Avg cla Loss : 0.6894\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0233\n",
      "Avg Classification Loss : 1.3838\n",
      "\n",
      "Epoch : 706, Total Avg Loss : 1.4224\n",
      "Source Avg Loss : 0.7129\n",
      "Target Avg Loss : 0.7095\n",
      "Source Avg reg Loss : 0.0244\n",
      "Target Avg reg Loss : 0.0147\n",
      "Source Avg cla Loss : 0.6886\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0391\n",
      "Avg Classification Loss : 1.3834\n",
      "\n",
      "Epoch : 707, Total Avg Loss : 1.4113\n",
      "Source Avg Loss : 0.7042\n",
      "Target Avg Loss : 0.7071\n",
      "Source Avg reg Loss : 0.0182\n",
      "Target Avg reg Loss : 0.0111\n",
      "Source Avg cla Loss : 0.6860\n",
      "Target Avg cla Loss : 0.6960\n",
      "Avg Regression Loss : 0.0293\n",
      "Avg Classification Loss : 1.3820\n",
      "\n",
      "Epoch : 708, Total Avg Loss : 1.4198\n",
      "Source Avg Loss : 0.7040\n",
      "Target Avg Loss : 0.7158\n",
      "Source Avg reg Loss : 0.0242\n",
      "Target Avg reg Loss : 0.0160\n",
      "Source Avg cla Loss : 0.6798\n",
      "Target Avg cla Loss : 0.6997\n",
      "Avg Regression Loss : 0.0403\n",
      "Avg Classification Loss : 1.3795\n",
      "\n",
      "Epoch : 709, Total Avg Loss : 1.4124\n",
      "Source Avg Loss : 0.6851\n",
      "Target Avg Loss : 0.7273\n",
      "Source Avg reg Loss : 0.0214\n",
      "Target Avg reg Loss : 0.0121\n",
      "Source Avg cla Loss : 0.6637\n",
      "Target Avg cla Loss : 0.7152\n",
      "Avg Regression Loss : 0.0335\n",
      "Avg Classification Loss : 1.3789\n",
      "\n",
      "Epoch : 710, Total Avg Loss : 1.4362\n",
      "Source Avg Loss : 0.6681\n",
      "Target Avg Loss : 0.7681\n",
      "Source Avg reg Loss : 0.0228\n",
      "Target Avg reg Loss : 0.0147\n",
      "Source Avg cla Loss : 0.6454\n",
      "Target Avg cla Loss : 0.7534\n",
      "Avg Regression Loss : 0.0374\n",
      "Avg Classification Loss : 1.3988\n",
      "\n",
      "Epoch : 711, Total Avg Loss : 1.5030\n",
      "Source Avg Loss : 0.6635\n",
      "Target Avg Loss : 0.8396\n",
      "Source Avg reg Loss : 0.0161\n",
      "Target Avg reg Loss : 0.0178\n",
      "Source Avg cla Loss : 0.6474\n",
      "Target Avg cla Loss : 0.8218\n",
      "Avg Regression Loss : 0.0339\n",
      "Avg Classification Loss : 1.4692\n",
      "\n",
      "Epoch : 712, Total Avg Loss : 1.5338\n",
      "Source Avg Loss : 0.6724\n",
      "Target Avg Loss : 0.8614\n",
      "Source Avg reg Loss : 0.0205\n",
      "Target Avg reg Loss : 0.0295\n",
      "Source Avg cla Loss : 0.6519\n",
      "Target Avg cla Loss : 0.8319\n",
      "Avg Regression Loss : 0.0500\n",
      "Avg Classification Loss : 1.4838\n",
      "\n",
      "Epoch : 713, Total Avg Loss : 1.4745\n",
      "Source Avg Loss : 0.7103\n",
      "Target Avg Loss : 0.7642\n",
      "Source Avg reg Loss : 0.0228\n",
      "Target Avg reg Loss : 0.0289\n",
      "Source Avg cla Loss : 0.6874\n",
      "Target Avg cla Loss : 0.7354\n",
      "Avg Regression Loss : 0.0517\n",
      "Avg Classification Loss : 1.4228\n",
      "\n",
      "Epoch : 714, Total Avg Loss : 1.4678\n",
      "Source Avg Loss : 0.7078\n",
      "Target Avg Loss : 0.7600\n",
      "Source Avg reg Loss : 0.0200\n",
      "Target Avg reg Loss : 0.0240\n",
      "Source Avg cla Loss : 0.6877\n",
      "Target Avg cla Loss : 0.7360\n",
      "Avg Regression Loss : 0.0440\n",
      "Avg Classification Loss : 1.4237\n",
      "\n",
      "Epoch : 715, Total Avg Loss : 1.4584\n",
      "Source Avg Loss : 0.7076\n",
      "Target Avg Loss : 0.7508\n",
      "Source Avg reg Loss : 0.0215\n",
      "Target Avg reg Loss : 0.0219\n",
      "Source Avg cla Loss : 0.6861\n",
      "Target Avg cla Loss : 0.7289\n",
      "Avg Regression Loss : 0.0434\n",
      "Avg Classification Loss : 1.4149\n",
      "\n",
      "Epoch : 716, Total Avg Loss : 1.4314\n",
      "Source Avg Loss : 0.7048\n",
      "Target Avg Loss : 0.7266\n",
      "Source Avg reg Loss : 0.0217\n",
      "Target Avg reg Loss : 0.0228\n",
      "Source Avg cla Loss : 0.6831\n",
      "Target Avg cla Loss : 0.7037\n",
      "Avg Regression Loss : 0.0446\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 717, Total Avg Loss : 1.4206\n",
      "Source Avg Loss : 0.7055\n",
      "Target Avg Loss : 0.7151\n",
      "Source Avg reg Loss : 0.0217\n",
      "Target Avg reg Loss : 0.0188\n",
      "Source Avg cla Loss : 0.6838\n",
      "Target Avg cla Loss : 0.6963\n",
      "Avg Regression Loss : 0.0405\n",
      "Avg Classification Loss : 1.3801\n",
      "\n",
      "Epoch : 718, Total Avg Loss : 1.4226\n",
      "Source Avg Loss : 0.7109\n",
      "Target Avg Loss : 0.7118\n",
      "Source Avg reg Loss : 0.0215\n",
      "Target Avg reg Loss : 0.0165\n",
      "Source Avg cla Loss : 0.6894\n",
      "Target Avg cla Loss : 0.6953\n",
      "Avg Regression Loss : 0.0379\n",
      "Avg Classification Loss : 1.3847\n",
      "\n",
      "Epoch : 719, Total Avg Loss : 1.4166\n",
      "Source Avg Loss : 0.7086\n",
      "Target Avg Loss : 0.7081\n",
      "Source Avg reg Loss : 0.0176\n",
      "Target Avg reg Loss : 0.0130\n",
      "Source Avg cla Loss : 0.6910\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0306\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 720, Total Avg Loss : 1.4210\n",
      "Source Avg Loss : 0.7111\n",
      "Target Avg Loss : 0.7100\n",
      "Source Avg reg Loss : 0.0198\n",
      "Target Avg reg Loss : 0.0151\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0349\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 721, Total Avg Loss : 1.4170\n",
      "Source Avg Loss : 0.7100\n",
      "Target Avg Loss : 0.7071\n",
      "Source Avg reg Loss : 0.0185\n",
      "Target Avg reg Loss : 0.0122\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0307\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 722, Total Avg Loss : 1.4216\n",
      "Source Avg Loss : 0.7123\n",
      "Target Avg Loss : 0.7093\n",
      "Source Avg reg Loss : 0.0207\n",
      "Target Avg reg Loss : 0.0146\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0353\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 723, Total Avg Loss : 1.4157\n",
      "Source Avg Loss : 0.7099\n",
      "Target Avg Loss : 0.7058\n",
      "Source Avg reg Loss : 0.0182\n",
      "Target Avg reg Loss : 0.0112\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0294\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 724, Total Avg Loss : 1.4197\n",
      "Source Avg Loss : 0.7123\n",
      "Target Avg Loss : 0.7075\n",
      "Source Avg reg Loss : 0.0206\n",
      "Target Avg reg Loss : 0.0129\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0335\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 725, Total Avg Loss : 1.4133\n",
      "Source Avg Loss : 0.7084\n",
      "Target Avg Loss : 0.7049\n",
      "Source Avg reg Loss : 0.0167\n",
      "Target Avg reg Loss : 0.0103\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0270\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 726, Total Avg Loss : 1.4138\n",
      "Source Avg Loss : 0.7086\n",
      "Target Avg Loss : 0.7051\n",
      "Source Avg reg Loss : 0.0169\n",
      "Target Avg reg Loss : 0.0106\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0275\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 727, Total Avg Loss : 1.4056\n",
      "Source Avg Loss : 0.7028\n",
      "Target Avg Loss : 0.7028\n",
      "Source Avg reg Loss : 0.0110\n",
      "Target Avg reg Loss : 0.0083\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0193\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 728, Total Avg Loss : 1.4061\n",
      "Source Avg Loss : 0.7032\n",
      "Target Avg Loss : 0.7030\n",
      "Source Avg reg Loss : 0.0113\n",
      "Target Avg reg Loss : 0.0085\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0198\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 729, Total Avg Loss : 1.4091\n",
      "Source Avg Loss : 0.7055\n",
      "Target Avg Loss : 0.7036\n",
      "Source Avg reg Loss : 0.0136\n",
      "Target Avg reg Loss : 0.0092\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0227\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 730, Total Avg Loss : 1.4216\n",
      "Source Avg Loss : 0.7129\n",
      "Target Avg Loss : 0.7087\n",
      "Source Avg reg Loss : 0.0210\n",
      "Target Avg reg Loss : 0.0143\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0353\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 731, Total Avg Loss : 1.4169\n",
      "Source Avg Loss : 0.7115\n",
      "Target Avg Loss : 0.7054\n",
      "Source Avg reg Loss : 0.0197\n",
      "Target Avg reg Loss : 0.0109\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0306\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 732, Total Avg Loss : 1.4167\n",
      "Source Avg Loss : 0.7114\n",
      "Target Avg Loss : 0.7053\n",
      "Source Avg reg Loss : 0.0196\n",
      "Target Avg reg Loss : 0.0109\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0305\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 733, Total Avg Loss : 1.4058\n",
      "Source Avg Loss : 0.7030\n",
      "Target Avg Loss : 0.7029\n",
      "Source Avg reg Loss : 0.0111\n",
      "Target Avg reg Loss : 0.0084\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0195\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 734, Total Avg Loss : 1.4065\n",
      "Source Avg Loss : 0.7040\n",
      "Target Avg Loss : 0.7025\n",
      "Source Avg reg Loss : 0.0123\n",
      "Target Avg reg Loss : 0.0081\n",
      "Source Avg cla Loss : 0.6917\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0203\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 735, Total Avg Loss : 1.4092\n",
      "Source Avg Loss : 0.7060\n",
      "Target Avg Loss : 0.7032\n",
      "Source Avg reg Loss : 0.0142\n",
      "Target Avg reg Loss : 0.0088\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0230\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 736, Total Avg Loss : 1.4060\n",
      "Source Avg Loss : 0.7037\n",
      "Target Avg Loss : 0.7023\n",
      "Source Avg reg Loss : 0.0119\n",
      "Target Avg reg Loss : 0.0079\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0198\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 737, Total Avg Loss : 1.4151\n",
      "Source Avg Loss : 0.7103\n",
      "Target Avg Loss : 0.7048\n",
      "Source Avg reg Loss : 0.0185\n",
      "Target Avg reg Loss : 0.0104\n",
      "Source Avg cla Loss : 0.6918\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0289\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 738, Total Avg Loss : 1.4136\n",
      "Source Avg Loss : 0.7094\n",
      "Target Avg Loss : 0.7042\n",
      "Source Avg reg Loss : 0.0178\n",
      "Target Avg reg Loss : 0.0097\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0275\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 739, Total Avg Loss : 1.4156\n",
      "Source Avg Loss : 0.7106\n",
      "Target Avg Loss : 0.7050\n",
      "Source Avg reg Loss : 0.0189\n",
      "Target Avg reg Loss : 0.0105\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0295\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 740, Total Avg Loss : 1.4096\n",
      "Source Avg Loss : 0.7062\n",
      "Target Avg Loss : 0.7033\n",
      "Source Avg reg Loss : 0.0147\n",
      "Target Avg reg Loss : 0.0089\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0235\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 741, Total Avg Loss : 1.4068\n",
      "Source Avg Loss : 0.7038\n",
      "Target Avg Loss : 0.7030\n",
      "Source Avg reg Loss : 0.0123\n",
      "Target Avg reg Loss : 0.0085\n",
      "Source Avg cla Loss : 0.6915\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0209\n",
      "Avg Classification Loss : 1.3859\n",
      "\n",
      "Epoch : 742, Total Avg Loss : 1.4172\n",
      "Source Avg Loss : 0.7113\n",
      "Target Avg Loss : 0.7059\n",
      "Source Avg reg Loss : 0.0199\n",
      "Target Avg reg Loss : 0.0114\n",
      "Source Avg cla Loss : 0.6914\n",
      "Target Avg cla Loss : 0.6945\n",
      "Avg Regression Loss : 0.0313\n",
      "Avg Classification Loss : 1.3859\n",
      "\n",
      "Epoch : 743, Total Avg Loss : 1.4151\n",
      "Source Avg Loss : 0.7104\n",
      "Target Avg Loss : 0.7047\n",
      "Source Avg reg Loss : 0.0194\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.6910\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0295\n",
      "Avg Classification Loss : 1.3856\n",
      "\n",
      "Epoch : 744, Total Avg Loss : 1.4207\n",
      "Source Avg Loss : 0.7130\n",
      "Target Avg Loss : 0.7077\n",
      "Source Avg reg Loss : 0.0222\n",
      "Target Avg reg Loss : 0.0130\n",
      "Source Avg cla Loss : 0.6908\n",
      "Target Avg cla Loss : 0.6947\n",
      "Avg Regression Loss : 0.0352\n",
      "Avg Classification Loss : 1.3856\n",
      "\n",
      "Epoch : 745, Total Avg Loss : 1.4125\n",
      "Source Avg Loss : 0.7078\n",
      "Target Avg Loss : 0.7047\n",
      "Source Avg reg Loss : 0.0175\n",
      "Target Avg reg Loss : 0.0099\n",
      "Source Avg cla Loss : 0.6903\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0274\n",
      "Avg Classification Loss : 1.3851\n",
      "\n",
      "Epoch : 746, Total Avg Loss : 1.4151\n",
      "Source Avg Loss : 0.7090\n",
      "Target Avg Loss : 0.7061\n",
      "Source Avg reg Loss : 0.0193\n",
      "Target Avg reg Loss : 0.0110\n",
      "Source Avg cla Loss : 0.6897\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0303\n",
      "Avg Classification Loss : 1.3848\n",
      "\n",
      "Epoch : 747, Total Avg Loss : 1.4053\n",
      "Source Avg Loss : 0.7017\n",
      "Target Avg Loss : 0.7036\n",
      "Source Avg reg Loss : 0.0137\n",
      "Target Avg reg Loss : 0.0079\n",
      "Source Avg cla Loss : 0.6880\n",
      "Target Avg cla Loss : 0.6957\n",
      "Avg Regression Loss : 0.0216\n",
      "Avg Classification Loss : 1.3837\n",
      "\n",
      "Epoch : 748, Total Avg Loss : 1.4022\n",
      "Source Avg Loss : 0.6960\n",
      "Target Avg Loss : 0.7063\n",
      "Source Avg reg Loss : 0.0121\n",
      "Target Avg reg Loss : 0.0088\n",
      "Source Avg cla Loss : 0.6839\n",
      "Target Avg cla Loss : 0.6974\n",
      "Avg Regression Loss : 0.0209\n",
      "Avg Classification Loss : 1.3813\n",
      "\n",
      "Epoch : 749, Total Avg Loss : 1.3993\n",
      "Source Avg Loss : 0.6829\n",
      "Target Avg Loss : 0.7164\n",
      "Source Avg reg Loss : 0.0165\n",
      "Target Avg reg Loss : 0.0097\n",
      "Source Avg cla Loss : 0.6664\n",
      "Target Avg cla Loss : 0.7066\n",
      "Avg Regression Loss : 0.0262\n",
      "Avg Classification Loss : 1.3731\n",
      "\n",
      "Epoch : 750, Total Avg Loss : 1.3748\n",
      "Source Avg Loss : 0.6196\n",
      "Target Avg Loss : 0.7552\n",
      "Source Avg reg Loss : 0.0126\n",
      "Target Avg reg Loss : 0.0081\n",
      "Source Avg cla Loss : 0.6070\n",
      "Target Avg cla Loss : 0.7471\n",
      "Avg Regression Loss : 0.0207\n",
      "Avg Classification Loss : 1.3541\n",
      "\n",
      "Epoch : 751, Total Avg Loss : 1.3768\n",
      "Source Avg Loss : 0.5391\n",
      "Target Avg Loss : 0.8376\n",
      "Source Avg reg Loss : 0.0144\n",
      "Target Avg reg Loss : 0.0111\n",
      "Source Avg cla Loss : 0.5247\n",
      "Target Avg cla Loss : 0.8266\n",
      "Avg Regression Loss : 0.0255\n",
      "Avg Classification Loss : 1.3513\n",
      "\n",
      "Epoch : 752, Total Avg Loss : 1.3982\n",
      "Source Avg Loss : 0.5079\n",
      "Target Avg Loss : 0.8904\n",
      "Source Avg reg Loss : 0.0218\n",
      "Target Avg reg Loss : 0.0130\n",
      "Source Avg cla Loss : 0.4860\n",
      "Target Avg cla Loss : 0.8774\n",
      "Avg Regression Loss : 0.0348\n",
      "Avg Classification Loss : 1.3634\n",
      "\n",
      "Epoch : 753, Total Avg Loss : 1.4745\n",
      "Source Avg Loss : 0.5437\n",
      "Target Avg Loss : 0.9308\n",
      "Source Avg reg Loss : 0.0332\n",
      "Target Avg reg Loss : 0.0209\n",
      "Source Avg cla Loss : 0.5105\n",
      "Target Avg cla Loss : 0.9099\n",
      "Avg Regression Loss : 0.0541\n",
      "Avg Classification Loss : 1.4204\n",
      "\n",
      "Epoch : 754, Total Avg Loss : 1.5670\n",
      "Source Avg Loss : 0.6169\n",
      "Target Avg Loss : 0.9501\n",
      "Source Avg reg Loss : 0.0401\n",
      "Target Avg reg Loss : 0.0391\n",
      "Source Avg cla Loss : 0.5768\n",
      "Target Avg cla Loss : 0.9110\n",
      "Avg Regression Loss : 0.0791\n",
      "Avg Classification Loss : 1.4879\n",
      "\n",
      "Epoch : 755, Total Avg Loss : 1.5626\n",
      "Source Avg Loss : 0.6795\n",
      "Target Avg Loss : 0.8831\n",
      "Source Avg reg Loss : 0.0408\n",
      "Target Avg reg Loss : 0.0574\n",
      "Source Avg cla Loss : 0.6387\n",
      "Target Avg cla Loss : 0.8257\n",
      "Avg Regression Loss : 0.0983\n",
      "Avg Classification Loss : 1.4643\n",
      "\n",
      "Epoch : 756, Total Avg Loss : 1.4814\n",
      "Source Avg Loss : 0.7174\n",
      "Target Avg Loss : 0.7640\n",
      "Source Avg reg Loss : 0.0377\n",
      "Target Avg reg Loss : 0.0363\n",
      "Source Avg cla Loss : 0.6797\n",
      "Target Avg cla Loss : 0.7277\n",
      "Avg Regression Loss : 0.0740\n",
      "Avg Classification Loss : 1.4074\n",
      "\n",
      "Epoch : 757, Total Avg Loss : 1.4480\n",
      "Source Avg Loss : 0.7190\n",
      "Target Avg Loss : 0.7291\n",
      "Source Avg reg Loss : 0.0335\n",
      "Target Avg reg Loss : 0.0188\n",
      "Source Avg cla Loss : 0.6855\n",
      "Target Avg cla Loss : 0.7103\n",
      "Avg Regression Loss : 0.0522\n",
      "Avg Classification Loss : 1.3958\n",
      "\n",
      "Epoch : 758, Total Avg Loss : 1.4332\n",
      "Source Avg Loss : 0.7125\n",
      "Target Avg Loss : 0.7207\n",
      "Source Avg reg Loss : 0.0259\n",
      "Target Avg reg Loss : 0.0143\n",
      "Source Avg cla Loss : 0.6866\n",
      "Target Avg cla Loss : 0.7064\n",
      "Avg Regression Loss : 0.0401\n",
      "Avg Classification Loss : 1.3930\n",
      "\n",
      "Epoch : 759, Total Avg Loss : 1.4272\n",
      "Source Avg Loss : 0.7111\n",
      "Target Avg Loss : 0.7161\n",
      "Source Avg reg Loss : 0.0233\n",
      "Target Avg reg Loss : 0.0124\n",
      "Source Avg cla Loss : 0.6879\n",
      "Target Avg cla Loss : 0.7037\n",
      "Avg Regression Loss : 0.0357\n",
      "Avg Classification Loss : 1.3916\n",
      "\n",
      "Epoch : 760, Total Avg Loss : 1.4223\n",
      "Source Avg Loss : 0.7101\n",
      "Target Avg Loss : 0.7122\n",
      "Source Avg reg Loss : 0.0218\n",
      "Target Avg reg Loss : 0.0107\n",
      "Source Avg cla Loss : 0.6883\n",
      "Target Avg cla Loss : 0.7015\n",
      "Avg Regression Loss : 0.0324\n",
      "Avg Classification Loss : 1.3899\n",
      "\n",
      "Epoch : 761, Total Avg Loss : 1.4190\n",
      "Source Avg Loss : 0.7087\n",
      "Target Avg Loss : 0.7103\n",
      "Source Avg reg Loss : 0.0196\n",
      "Target Avg reg Loss : 0.0107\n",
      "Source Avg cla Loss : 0.6891\n",
      "Target Avg cla Loss : 0.6996\n",
      "Avg Regression Loss : 0.0303\n",
      "Avg Classification Loss : 1.3887\n",
      "\n",
      "Epoch : 762, Total Avg Loss : 1.4158\n",
      "Source Avg Loss : 0.7076\n",
      "Target Avg Loss : 0.7083\n",
      "Source Avg reg Loss : 0.0182\n",
      "Target Avg reg Loss : 0.0094\n",
      "Source Avg cla Loss : 0.6894\n",
      "Target Avg cla Loss : 0.6988\n",
      "Avg Regression Loss : 0.0276\n",
      "Avg Classification Loss : 1.3882\n",
      "\n",
      "Epoch : 763, Total Avg Loss : 1.4183\n",
      "Source Avg Loss : 0.7081\n",
      "Target Avg Loss : 0.7102\n",
      "Source Avg reg Loss : 0.0189\n",
      "Target Avg reg Loss : 0.0099\n",
      "Source Avg cla Loss : 0.6892\n",
      "Target Avg cla Loss : 0.7003\n",
      "Avg Regression Loss : 0.0288\n",
      "Avg Classification Loss : 1.3895\n",
      "\n",
      "Epoch : 764, Total Avg Loss : 1.4287\n",
      "Source Avg Loss : 0.7089\n",
      "Target Avg Loss : 0.7198\n",
      "Source Avg reg Loss : 0.0206\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.6882\n",
      "Target Avg cla Loss : 0.7096\n",
      "Avg Regression Loss : 0.0308\n",
      "Avg Classification Loss : 1.3979\n",
      "\n",
      "Epoch : 765, Total Avg Loss : 1.4414\n",
      "Source Avg Loss : 0.6983\n",
      "Target Avg Loss : 0.7430\n",
      "Source Avg reg Loss : 0.0174\n",
      "Target Avg reg Loss : 0.0099\n",
      "Source Avg cla Loss : 0.6809\n",
      "Target Avg cla Loss : 0.7332\n",
      "Avg Regression Loss : 0.0273\n",
      "Avg Classification Loss : 1.4141\n",
      "\n",
      "Epoch : 766, Total Avg Loss : 1.4570\n",
      "Source Avg Loss : 0.6934\n",
      "Target Avg Loss : 0.7635\n",
      "Source Avg reg Loss : 0.0175\n",
      "Target Avg reg Loss : 0.0118\n",
      "Source Avg cla Loss : 0.6759\n",
      "Target Avg cla Loss : 0.7517\n",
      "Avg Regression Loss : 0.0293\n",
      "Avg Classification Loss : 1.4277\n",
      "\n",
      "Epoch : 767, Total Avg Loss : 1.4354\n",
      "Source Avg Loss : 0.7012\n",
      "Target Avg Loss : 0.7342\n",
      "Source Avg reg Loss : 0.0174\n",
      "Target Avg reg Loss : 0.0130\n",
      "Source Avg cla Loss : 0.6837\n",
      "Target Avg cla Loss : 0.7213\n",
      "Avg Regression Loss : 0.0304\n",
      "Avg Classification Loss : 1.4050\n",
      "\n",
      "Epoch : 768, Total Avg Loss : 1.4166\n",
      "Source Avg Loss : 0.7062\n",
      "Target Avg Loss : 0.7103\n",
      "Source Avg reg Loss : 0.0173\n",
      "Target Avg reg Loss : 0.0117\n",
      "Source Avg cla Loss : 0.6889\n",
      "Target Avg cla Loss : 0.6986\n",
      "Avg Regression Loss : 0.0291\n",
      "Avg Classification Loss : 1.3875\n",
      "\n",
      "Epoch : 769, Total Avg Loss : 1.4088\n",
      "Source Avg Loss : 0.7038\n",
      "Target Avg Loss : 0.7050\n",
      "Source Avg reg Loss : 0.0151\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.6887\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0253\n",
      "Avg Classification Loss : 1.3834\n",
      "\n",
      "Epoch : 770, Total Avg Loss : 1.4059\n",
      "Source Avg Loss : 0.7023\n",
      "Target Avg Loss : 0.7036\n",
      "Source Avg reg Loss : 0.0137\n",
      "Target Avg reg Loss : 0.0092\n",
      "Source Avg cla Loss : 0.6886\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.0229\n",
      "Avg Classification Loss : 1.3830\n",
      "\n",
      "Epoch : 771, Total Avg Loss : 1.4084\n",
      "Source Avg Loss : 0.7045\n",
      "Target Avg Loss : 0.7039\n",
      "Source Avg reg Loss : 0.0154\n",
      "Target Avg reg Loss : 0.0097\n",
      "Source Avg cla Loss : 0.6891\n",
      "Target Avg cla Loss : 0.6942\n",
      "Avg Regression Loss : 0.0251\n",
      "Avg Classification Loss : 1.3833\n",
      "\n",
      "Epoch : 772, Total Avg Loss : 1.4062\n",
      "Source Avg Loss : 0.7035\n",
      "Target Avg Loss : 0.7028\n",
      "Source Avg reg Loss : 0.0133\n",
      "Target Avg reg Loss : 0.0087\n",
      "Source Avg cla Loss : 0.6902\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0219\n",
      "Avg Classification Loss : 1.3843\n",
      "\n",
      "Epoch : 773, Total Avg Loss : 1.4053\n",
      "Source Avg Loss : 0.7027\n",
      "Target Avg Loss : 0.7026\n",
      "Source Avg reg Loss : 0.0120\n",
      "Target Avg reg Loss : 0.0085\n",
      "Source Avg cla Loss : 0.6907\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0205\n",
      "Avg Classification Loss : 1.3848\n",
      "\n",
      "Epoch : 774, Total Avg Loss : 1.4049\n",
      "Source Avg Loss : 0.7025\n",
      "Target Avg Loss : 0.7024\n",
      "Source Avg reg Loss : 0.0126\n",
      "Target Avg reg Loss : 0.0083\n",
      "Source Avg cla Loss : 0.6900\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0208\n",
      "Avg Classification Loss : 1.3841\n",
      "\n",
      "Epoch : 775, Total Avg Loss : 1.4134\n",
      "Source Avg Loss : 0.7085\n",
      "Target Avg Loss : 0.7050\n",
      "Source Avg reg Loss : 0.0188\n",
      "Target Avg reg Loss : 0.0108\n",
      "Source Avg cla Loss : 0.6897\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0296\n",
      "Avg Classification Loss : 1.3838\n",
      "\n",
      "Epoch : 776, Total Avg Loss : 1.4158\n",
      "Source Avg Loss : 0.7090\n",
      "Target Avg Loss : 0.7067\n",
      "Source Avg reg Loss : 0.0202\n",
      "Target Avg reg Loss : 0.0124\n",
      "Source Avg cla Loss : 0.6889\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.0325\n",
      "Avg Classification Loss : 1.3832\n",
      "\n",
      "Epoch : 777, Total Avg Loss : 1.4138\n",
      "Source Avg Loss : 0.7058\n",
      "Target Avg Loss : 0.7080\n",
      "Source Avg reg Loss : 0.0187\n",
      "Target Avg reg Loss : 0.0131\n",
      "Source Avg cla Loss : 0.6871\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0317\n",
      "Avg Classification Loss : 1.3820\n",
      "\n",
      "Epoch : 778, Total Avg Loss : 1.4101\n",
      "Source Avg Loss : 0.7030\n",
      "Target Avg Loss : 0.7071\n",
      "Source Avg reg Loss : 0.0176\n",
      "Target Avg reg Loss : 0.0122\n",
      "Source Avg cla Loss : 0.6854\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0298\n",
      "Avg Classification Loss : 1.3803\n",
      "\n",
      "Epoch : 779, Total Avg Loss : 1.4097\n",
      "Source Avg Loss : 0.7015\n",
      "Target Avg Loss : 0.7082\n",
      "Source Avg reg Loss : 0.0176\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.6839\n",
      "Target Avg cla Loss : 0.6957\n",
      "Avg Regression Loss : 0.0301\n",
      "Avg Classification Loss : 1.3796\n",
      "\n",
      "Epoch : 780, Total Avg Loss : 1.4080\n",
      "Source Avg Loss : 0.6988\n",
      "Target Avg Loss : 0.7093\n",
      "Source Avg reg Loss : 0.0171\n",
      "Target Avg reg Loss : 0.0112\n",
      "Source Avg cla Loss : 0.6816\n",
      "Target Avg cla Loss : 0.6981\n",
      "Avg Regression Loss : 0.0283\n",
      "Avg Classification Loss : 1.3797\n",
      "\n",
      "Epoch : 781, Total Avg Loss : 1.4123\n",
      "Source Avg Loss : 0.6978\n",
      "Target Avg Loss : 0.7146\n",
      "Source Avg reg Loss : 0.0166\n",
      "Target Avg reg Loss : 0.0128\n",
      "Source Avg cla Loss : 0.6811\n",
      "Target Avg cla Loss : 0.7018\n",
      "Avg Regression Loss : 0.0294\n",
      "Avg Classification Loss : 1.3829\n",
      "\n",
      "Epoch : 782, Total Avg Loss : 1.4101\n",
      "Source Avg Loss : 0.6965\n",
      "Target Avg Loss : 0.7136\n",
      "Source Avg reg Loss : 0.0150\n",
      "Target Avg reg Loss : 0.0111\n",
      "Source Avg cla Loss : 0.6815\n",
      "Target Avg cla Loss : 0.7025\n",
      "Avg Regression Loss : 0.0261\n",
      "Avg Classification Loss : 1.3840\n",
      "\n",
      "Epoch : 783, Total Avg Loss : 1.4150\n",
      "Source Avg Loss : 0.6997\n",
      "Target Avg Loss : 0.7153\n",
      "Source Avg reg Loss : 0.0172\n",
      "Target Avg reg Loss : 0.0126\n",
      "Source Avg cla Loss : 0.6825\n",
      "Target Avg cla Loss : 0.7028\n",
      "Avg Regression Loss : 0.0298\n",
      "Avg Classification Loss : 1.3852\n",
      "\n",
      "Epoch : 784, Total Avg Loss : 1.4099\n",
      "Source Avg Loss : 0.6952\n",
      "Target Avg Loss : 0.7147\n",
      "Source Avg reg Loss : 0.0133\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.6819\n",
      "Target Avg cla Loss : 0.7045\n",
      "Avg Regression Loss : 0.0235\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 785, Total Avg Loss : 1.4143\n",
      "Source Avg Loss : 0.6974\n",
      "Target Avg Loss : 0.7170\n",
      "Source Avg reg Loss : 0.0150\n",
      "Target Avg reg Loss : 0.0105\n",
      "Source Avg cla Loss : 0.6824\n",
      "Target Avg cla Loss : 0.7064\n",
      "Avg Regression Loss : 0.0255\n",
      "Avg Classification Loss : 1.3888\n",
      "\n",
      "Epoch : 786, Total Avg Loss : 1.4165\n",
      "Source Avg Loss : 0.6948\n",
      "Target Avg Loss : 0.7217\n",
      "Source Avg reg Loss : 0.0114\n",
      "Target Avg reg Loss : 0.0098\n",
      "Source Avg cla Loss : 0.6834\n",
      "Target Avg cla Loss : 0.7119\n",
      "Avg Regression Loss : 0.0212\n",
      "Avg Classification Loss : 1.3953\n",
      "\n",
      "Epoch : 787, Total Avg Loss : 1.4457\n",
      "Source Avg Loss : 0.6936\n",
      "Target Avg Loss : 0.7521\n",
      "Source Avg reg Loss : 0.0117\n",
      "Target Avg reg Loss : 0.0149\n",
      "Source Avg cla Loss : 0.6820\n",
      "Target Avg cla Loss : 0.7372\n",
      "Avg Regression Loss : 0.0266\n",
      "Avg Classification Loss : 1.4192\n",
      "\n",
      "Epoch : 788, Total Avg Loss : 1.4992\n",
      "Source Avg Loss : 0.6980\n",
      "Target Avg Loss : 0.8011\n",
      "Source Avg reg Loss : 0.0155\n",
      "Target Avg reg Loss : 0.0261\n",
      "Source Avg cla Loss : 0.6825\n",
      "Target Avg cla Loss : 0.7751\n",
      "Avg Regression Loss : 0.0416\n",
      "Avg Classification Loss : 1.4576\n",
      "\n",
      "Epoch : 789, Total Avg Loss : 1.5397\n",
      "Source Avg Loss : 0.7047\n",
      "Target Avg Loss : 0.8350\n",
      "Source Avg reg Loss : 0.0203\n",
      "Target Avg reg Loss : 0.0337\n",
      "Source Avg cla Loss : 0.6844\n",
      "Target Avg cla Loss : 0.8013\n",
      "Avg Regression Loss : 0.0540\n",
      "Avg Classification Loss : 1.4857\n",
      "\n",
      "Epoch : 790, Total Avg Loss : 1.5489\n",
      "Source Avg Loss : 0.7077\n",
      "Target Avg Loss : 0.8412\n",
      "Source Avg reg Loss : 0.0204\n",
      "Target Avg reg Loss : 0.0362\n",
      "Source Avg cla Loss : 0.6872\n",
      "Target Avg cla Loss : 0.8050\n",
      "Avg Regression Loss : 0.0567\n",
      "Avg Classification Loss : 1.4922\n",
      "\n",
      "Epoch : 791, Total Avg Loss : 1.4953\n",
      "Source Avg Loss : 0.7111\n",
      "Target Avg Loss : 0.7842\n",
      "Source Avg reg Loss : 0.0206\n",
      "Target Avg reg Loss : 0.0377\n",
      "Source Avg cla Loss : 0.6906\n",
      "Target Avg cla Loss : 0.7465\n",
      "Avg Regression Loss : 0.0582\n",
      "Avg Classification Loss : 1.4371\n",
      "\n",
      "Epoch : 792, Total Avg Loss : 1.4469\n",
      "Source Avg Loss : 0.7124\n",
      "Target Avg Loss : 0.7345\n",
      "Source Avg reg Loss : 0.0197\n",
      "Target Avg reg Loss : 0.0268\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.7077\n",
      "Avg Regression Loss : 0.0465\n",
      "Avg Classification Loss : 1.4004\n",
      "\n",
      "Epoch : 793, Total Avg Loss : 1.4347\n",
      "Source Avg Loss : 0.7119\n",
      "Target Avg Loss : 0.7227\n",
      "Source Avg reg Loss : 0.0190\n",
      "Target Avg reg Loss : 0.0226\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.7001\n",
      "Avg Regression Loss : 0.0416\n",
      "Avg Classification Loss : 1.3930\n",
      "\n",
      "Epoch : 794, Total Avg Loss : 1.4238\n",
      "Source Avg Loss : 0.7091\n",
      "Target Avg Loss : 0.7147\n",
      "Source Avg reg Loss : 0.0161\n",
      "Target Avg reg Loss : 0.0169\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6977\n",
      "Avg Regression Loss : 0.0331\n",
      "Avg Classification Loss : 1.3907\n",
      "\n",
      "Epoch : 795, Total Avg Loss : 1.4201\n",
      "Source Avg Loss : 0.7082\n",
      "Target Avg Loss : 0.7119\n",
      "Source Avg reg Loss : 0.0152\n",
      "Target Avg reg Loss : 0.0155\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6964\n",
      "Avg Regression Loss : 0.0307\n",
      "Avg Classification Loss : 1.3894\n",
      "\n",
      "Epoch : 796, Total Avg Loss : 1.4169\n",
      "Source Avg Loss : 0.7087\n",
      "Target Avg Loss : 0.7083\n",
      "Source Avg reg Loss : 0.0157\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6957\n",
      "Avg Regression Loss : 0.0282\n",
      "Avg Classification Loss : 1.3887\n",
      "\n",
      "Epoch : 797, Total Avg Loss : 1.4172\n",
      "Source Avg Loss : 0.7087\n",
      "Target Avg Loss : 0.7085\n",
      "Source Avg reg Loss : 0.0157\n",
      "Target Avg reg Loss : 0.0134\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6951\n",
      "Avg Regression Loss : 0.0290\n",
      "Avg Classification Loss : 1.3881\n",
      "\n",
      "Epoch : 798, Total Avg Loss : 1.4130\n",
      "Source Avg Loss : 0.7071\n",
      "Target Avg Loss : 0.7059\n",
      "Source Avg reg Loss : 0.0141\n",
      "Target Avg reg Loss : 0.0110\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6949\n",
      "Avg Regression Loss : 0.0251\n",
      "Avg Classification Loss : 1.3879\n",
      "\n",
      "Epoch : 799, Total Avg Loss : 1.4147\n",
      "Source Avg Loss : 0.7084\n",
      "Target Avg Loss : 0.7064\n",
      "Source Avg reg Loss : 0.0153\n",
      "Target Avg reg Loss : 0.0118\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6946\n",
      "Avg Regression Loss : 0.0271\n",
      "Avg Classification Loss : 1.3876\n",
      "\n",
      "Epoch : 800, Total Avg Loss : 1.4085\n",
      "Source Avg Loss : 0.7039\n",
      "Target Avg Loss : 0.7046\n",
      "Source Avg reg Loss : 0.0109\n",
      "Target Avg reg Loss : 0.0102\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0211\n",
      "Avg Classification Loss : 1.3874\n",
      "\n",
      "Epoch : 801, Total Avg Loss : 1.4061\n",
      "Source Avg Loss : 0.7025\n",
      "Target Avg Loss : 0.7036\n",
      "Source Avg reg Loss : 0.0095\n",
      "Target Avg reg Loss : 0.0093\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6943\n",
      "Avg Regression Loss : 0.0188\n",
      "Avg Classification Loss : 1.3873\n",
      "\n",
      "Epoch : 802, Total Avg Loss : 1.4081\n",
      "Source Avg Loss : 0.7044\n",
      "Target Avg Loss : 0.7037\n",
      "Source Avg reg Loss : 0.0114\n",
      "Target Avg reg Loss : 0.0096\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0209\n",
      "Avg Classification Loss : 1.3872\n",
      "\n",
      "Epoch : 803, Total Avg Loss : 1.4103\n",
      "Source Avg Loss : 0.7063\n",
      "Target Avg Loss : 0.7040\n",
      "Source Avg reg Loss : 0.0133\n",
      "Target Avg reg Loss : 0.0100\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6940\n",
      "Avg Regression Loss : 0.0233\n",
      "Avg Classification Loss : 1.3870\n",
      "\n",
      "Epoch : 804, Total Avg Loss : 1.4141\n",
      "Source Avg Loss : 0.7102\n",
      "Target Avg Loss : 0.7040\n",
      "Source Avg reg Loss : 0.0171\n",
      "Target Avg reg Loss : 0.0100\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0272\n",
      "Avg Classification Loss : 1.3870\n",
      "\n",
      "Epoch : 805, Total Avg Loss : 1.4144\n",
      "Source Avg Loss : 0.7088\n",
      "Target Avg Loss : 0.7056\n",
      "Source Avg reg Loss : 0.0157\n",
      "Target Avg reg Loss : 0.0118\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0275\n",
      "Avg Classification Loss : 1.3869\n",
      "\n",
      "Epoch : 806, Total Avg Loss : 1.4101\n",
      "Source Avg Loss : 0.7072\n",
      "Target Avg Loss : 0.7029\n",
      "Source Avg reg Loss : 0.0141\n",
      "Target Avg reg Loss : 0.0091\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0232\n",
      "Avg Classification Loss : 1.3868\n",
      "\n",
      "Epoch : 807, Total Avg Loss : 1.4120\n",
      "Source Avg Loss : 0.7079\n",
      "Target Avg Loss : 0.7041\n",
      "Source Avg reg Loss : 0.0149\n",
      "Target Avg reg Loss : 0.0103\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0252\n",
      "Avg Classification Loss : 1.3867\n",
      "\n",
      "Epoch : 808, Total Avg Loss : 1.4074\n",
      "Source Avg Loss : 0.7056\n",
      "Target Avg Loss : 0.7019\n",
      "Source Avg reg Loss : 0.0125\n",
      "Target Avg reg Loss : 0.0082\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0207\n",
      "Avg Classification Loss : 1.3867\n",
      "\n",
      "Epoch : 809, Total Avg Loss : 1.4083\n",
      "Source Avg Loss : 0.7061\n",
      "Target Avg Loss : 0.7022\n",
      "Source Avg reg Loss : 0.0131\n",
      "Target Avg reg Loss : 0.0086\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0217\n",
      "Avg Classification Loss : 1.3867\n",
      "\n",
      "Epoch : 810, Total Avg Loss : 1.4021\n",
      "Source Avg Loss : 0.7014\n",
      "Target Avg Loss : 0.7007\n",
      "Source Avg reg Loss : 0.0084\n",
      "Target Avg reg Loss : 0.0071\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6936\n",
      "Avg Regression Loss : 0.0155\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 811, Total Avg Loss : 1.4024\n",
      "Source Avg Loss : 0.7017\n",
      "Target Avg Loss : 0.7007\n",
      "Source Avg reg Loss : 0.0087\n",
      "Target Avg reg Loss : 0.0072\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0158\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 812, Total Avg Loss : 1.4067\n",
      "Source Avg Loss : 0.7050\n",
      "Target Avg Loss : 0.7016\n",
      "Source Avg reg Loss : 0.0120\n",
      "Target Avg reg Loss : 0.0081\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0201\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 813, Total Avg Loss : 1.4166\n",
      "Source Avg Loss : 0.7107\n",
      "Target Avg Loss : 0.7060\n",
      "Source Avg reg Loss : 0.0176\n",
      "Target Avg reg Loss : 0.0125\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0301\n",
      "Avg Classification Loss : 1.3866\n",
      "\n",
      "Epoch : 814, Total Avg Loss : 1.4045\n",
      "Source Avg Loss : 0.7040\n",
      "Target Avg Loss : 0.7005\n",
      "Source Avg reg Loss : 0.0110\n",
      "Target Avg reg Loss : 0.0070\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0180\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 815, Total Avg Loss : 1.4060\n",
      "Source Avg Loss : 0.7048\n",
      "Target Avg Loss : 0.7012\n",
      "Source Avg reg Loss : 0.0118\n",
      "Target Avg reg Loss : 0.0078\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0196\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 816, Total Avg Loss : 1.4020\n",
      "Source Avg Loss : 0.7020\n",
      "Target Avg Loss : 0.7000\n",
      "Source Avg reg Loss : 0.0089\n",
      "Target Avg reg Loss : 0.0066\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0155\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 817, Total Avg Loss : 1.4012\n",
      "Source Avg Loss : 0.7014\n",
      "Target Avg Loss : 0.6998\n",
      "Source Avg reg Loss : 0.0083\n",
      "Target Avg reg Loss : 0.0064\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0148\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 818, Total Avg Loss : 1.4005\n",
      "Source Avg Loss : 0.7010\n",
      "Target Avg Loss : 0.6994\n",
      "Source Avg reg Loss : 0.0080\n",
      "Target Avg reg Loss : 0.0060\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0140\n",
      "Avg Classification Loss : 1.3865\n",
      "\n",
      "Epoch : 819, Total Avg Loss : 1.4026\n",
      "Source Avg Loss : 0.7024\n",
      "Target Avg Loss : 0.7002\n",
      "Source Avg reg Loss : 0.0093\n",
      "Target Avg reg Loss : 0.0068\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0162\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 820, Total Avg Loss : 1.4050\n",
      "Source Avg Loss : 0.7041\n",
      "Target Avg Loss : 0.7010\n",
      "Source Avg reg Loss : 0.0110\n",
      "Target Avg reg Loss : 0.0076\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0186\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 821, Total Avg Loss : 1.4130\n",
      "Source Avg Loss : 0.7088\n",
      "Target Avg Loss : 0.7043\n",
      "Source Avg reg Loss : 0.0157\n",
      "Target Avg reg Loss : 0.0109\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0266\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 822, Total Avg Loss : 1.4052\n",
      "Source Avg Loss : 0.7050\n",
      "Target Avg Loss : 0.7002\n",
      "Source Avg reg Loss : 0.0119\n",
      "Target Avg reg Loss : 0.0069\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0188\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 823, Total Avg Loss : 1.4057\n",
      "Source Avg Loss : 0.7048\n",
      "Target Avg Loss : 0.7010\n",
      "Source Avg reg Loss : 0.0117\n",
      "Target Avg reg Loss : 0.0076\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0193\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 824, Total Avg Loss : 1.4039\n",
      "Source Avg Loss : 0.7034\n",
      "Target Avg Loss : 0.7005\n",
      "Source Avg reg Loss : 0.0104\n",
      "Target Avg reg Loss : 0.0071\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0175\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 825, Total Avg Loss : 1.4123\n",
      "Source Avg Loss : 0.7096\n",
      "Target Avg Loss : 0.7027\n",
      "Source Avg reg Loss : 0.0166\n",
      "Target Avg reg Loss : 0.0094\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0259\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 826, Total Avg Loss : 1.4068\n",
      "Source Avg Loss : 0.7055\n",
      "Target Avg Loss : 0.7013\n",
      "Source Avg reg Loss : 0.0125\n",
      "Target Avg reg Loss : 0.0080\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0204\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 827, Total Avg Loss : 1.4059\n",
      "Source Avg Loss : 0.7058\n",
      "Target Avg Loss : 0.7001\n",
      "Source Avg reg Loss : 0.0127\n",
      "Target Avg reg Loss : 0.0068\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0195\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 828, Total Avg Loss : 1.4025\n",
      "Source Avg Loss : 0.7026\n",
      "Target Avg Loss : 0.6999\n",
      "Source Avg reg Loss : 0.0095\n",
      "Target Avg reg Loss : 0.0066\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0162\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 829, Total Avg Loss : 1.4065\n",
      "Source Avg Loss : 0.7056\n",
      "Target Avg Loss : 0.7009\n",
      "Source Avg reg Loss : 0.0126\n",
      "Target Avg reg Loss : 0.0076\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0202\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 830, Total Avg Loss : 1.4113\n",
      "Source Avg Loss : 0.7092\n",
      "Target Avg Loss : 0.7021\n",
      "Source Avg reg Loss : 0.0161\n",
      "Target Avg reg Loss : 0.0088\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0249\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 831, Total Avg Loss : 1.4061\n",
      "Source Avg Loss : 0.7048\n",
      "Target Avg Loss : 0.7012\n",
      "Source Avg reg Loss : 0.0118\n",
      "Target Avg reg Loss : 0.0079\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0197\n",
      "Avg Classification Loss : 1.3864\n",
      "\n",
      "Epoch : 832, Total Avg Loss : 1.4085\n",
      "Source Avg Loss : 0.7077\n",
      "Target Avg Loss : 0.7009\n",
      "Source Avg reg Loss : 0.0146\n",
      "Target Avg reg Loss : 0.0076\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0222\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 833, Total Avg Loss : 1.4086\n",
      "Source Avg Loss : 0.7073\n",
      "Target Avg Loss : 0.7013\n",
      "Source Avg reg Loss : 0.0143\n",
      "Target Avg reg Loss : 0.0080\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0223\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 834, Total Avg Loss : 1.4106\n",
      "Source Avg Loss : 0.7089\n",
      "Target Avg Loss : 0.7017\n",
      "Source Avg reg Loss : 0.0158\n",
      "Target Avg reg Loss : 0.0085\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0243\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 835, Total Avg Loss : 1.4075\n",
      "Source Avg Loss : 0.7059\n",
      "Target Avg Loss : 0.7015\n",
      "Source Avg reg Loss : 0.0129\n",
      "Target Avg reg Loss : 0.0082\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0211\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 836, Total Avg Loss : 1.4100\n",
      "Source Avg Loss : 0.7084\n",
      "Target Avg Loss : 0.7017\n",
      "Source Avg reg Loss : 0.0153\n",
      "Target Avg reg Loss : 0.0084\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0237\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 837, Total Avg Loss : 1.4094\n",
      "Source Avg Loss : 0.7070\n",
      "Target Avg Loss : 0.7024\n",
      "Source Avg reg Loss : 0.0140\n",
      "Target Avg reg Loss : 0.0091\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0231\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 838, Total Avg Loss : 1.4059\n",
      "Source Avg Loss : 0.7052\n",
      "Target Avg Loss : 0.7007\n",
      "Source Avg reg Loss : 0.0122\n",
      "Target Avg reg Loss : 0.0074\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0196\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 839, Total Avg Loss : 1.4079\n",
      "Source Avg Loss : 0.7066\n",
      "Target Avg Loss : 0.7013\n",
      "Source Avg reg Loss : 0.0136\n",
      "Target Avg reg Loss : 0.0080\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0216\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 840, Total Avg Loss : 1.4024\n",
      "Source Avg Loss : 0.7025\n",
      "Target Avg Loss : 0.6998\n",
      "Source Avg reg Loss : 0.0095\n",
      "Target Avg reg Loss : 0.0066\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0160\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 841, Total Avg Loss : 1.4017\n",
      "Source Avg Loss : 0.7025\n",
      "Target Avg Loss : 0.6992\n",
      "Source Avg reg Loss : 0.0094\n",
      "Target Avg reg Loss : 0.0060\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0154\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 842, Total Avg Loss : 1.3997\n",
      "Source Avg Loss : 0.7009\n",
      "Target Avg Loss : 0.6988\n",
      "Source Avg reg Loss : 0.0078\n",
      "Target Avg reg Loss : 0.0056\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0134\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 843, Total Avg Loss : 1.4020\n",
      "Source Avg Loss : 0.7027\n",
      "Target Avg Loss : 0.6993\n",
      "Source Avg reg Loss : 0.0096\n",
      "Target Avg reg Loss : 0.0061\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0157\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 844, Total Avg Loss : 1.4027\n",
      "Source Avg Loss : 0.7032\n",
      "Target Avg Loss : 0.6994\n",
      "Source Avg reg Loss : 0.0102\n",
      "Target Avg reg Loss : 0.0062\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0164\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 845, Total Avg Loss : 1.4013\n",
      "Source Avg Loss : 0.7021\n",
      "Target Avg Loss : 0.6992\n",
      "Source Avg reg Loss : 0.0091\n",
      "Target Avg reg Loss : 0.0059\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0150\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 846, Total Avg Loss : 1.3985\n",
      "Source Avg Loss : 0.7002\n",
      "Target Avg Loss : 0.6983\n",
      "Source Avg reg Loss : 0.0072\n",
      "Target Avg reg Loss : 0.0051\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0122\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 847, Total Avg Loss : 1.4000\n",
      "Source Avg Loss : 0.7012\n",
      "Target Avg Loss : 0.6988\n",
      "Source Avg reg Loss : 0.0082\n",
      "Target Avg reg Loss : 0.0055\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0137\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 848, Total Avg Loss : 1.4042\n",
      "Source Avg Loss : 0.7042\n",
      "Target Avg Loss : 0.7001\n",
      "Source Avg reg Loss : 0.0111\n",
      "Target Avg reg Loss : 0.0068\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0179\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 849, Total Avg Loss : 1.4107\n",
      "Source Avg Loss : 0.7075\n",
      "Target Avg Loss : 0.7032\n",
      "Source Avg reg Loss : 0.0144\n",
      "Target Avg reg Loss : 0.0100\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0244\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 850, Total Avg Loss : 1.3997\n",
      "Source Avg Loss : 0.7013\n",
      "Target Avg Loss : 0.6984\n",
      "Source Avg reg Loss : 0.0082\n",
      "Target Avg reg Loss : 0.0052\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0134\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 851, Total Avg Loss : 1.3987\n",
      "Source Avg Loss : 0.7003\n",
      "Target Avg Loss : 0.6984\n",
      "Source Avg reg Loss : 0.0073\n",
      "Target Avg reg Loss : 0.0052\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0124\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 852, Total Avg Loss : 1.4061\n",
      "Source Avg Loss : 0.7053\n",
      "Target Avg Loss : 0.7007\n",
      "Source Avg reg Loss : 0.0123\n",
      "Target Avg reg Loss : 0.0075\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0198\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 853, Total Avg Loss : 1.4035\n",
      "Source Avg Loss : 0.7041\n",
      "Target Avg Loss : 0.6994\n",
      "Source Avg reg Loss : 0.0111\n",
      "Target Avg reg Loss : 0.0062\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0172\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 854, Total Avg Loss : 1.4032\n",
      "Source Avg Loss : 0.7037\n",
      "Target Avg Loss : 0.6995\n",
      "Source Avg reg Loss : 0.0106\n",
      "Target Avg reg Loss : 0.0063\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0169\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 855, Total Avg Loss : 1.3988\n",
      "Source Avg Loss : 0.7004\n",
      "Target Avg Loss : 0.6984\n",
      "Source Avg reg Loss : 0.0073\n",
      "Target Avg reg Loss : 0.0051\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0125\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 856, Total Avg Loss : 1.4005\n",
      "Source Avg Loss : 0.7015\n",
      "Target Avg Loss : 0.6990\n",
      "Source Avg reg Loss : 0.0085\n",
      "Target Avg reg Loss : 0.0057\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0142\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 857, Total Avg Loss : 1.4011\n",
      "Source Avg Loss : 0.7022\n",
      "Target Avg Loss : 0.6989\n",
      "Source Avg reg Loss : 0.0091\n",
      "Target Avg reg Loss : 0.0057\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0148\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 858, Total Avg Loss : 1.3975\n",
      "Source Avg Loss : 0.6996\n",
      "Target Avg Loss : 0.6979\n",
      "Source Avg reg Loss : 0.0065\n",
      "Target Avg reg Loss : 0.0047\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0113\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 859, Total Avg Loss : 1.3992\n",
      "Source Avg Loss : 0.7007\n",
      "Target Avg Loss : 0.6985\n",
      "Source Avg reg Loss : 0.0076\n",
      "Target Avg reg Loss : 0.0052\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0129\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 860, Total Avg Loss : 1.4003\n",
      "Source Avg Loss : 0.7015\n",
      "Target Avg Loss : 0.6988\n",
      "Source Avg reg Loss : 0.0084\n",
      "Target Avg reg Loss : 0.0056\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0140\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 861, Total Avg Loss : 1.4069\n",
      "Source Avg Loss : 0.7053\n",
      "Target Avg Loss : 0.7017\n",
      "Source Avg reg Loss : 0.0122\n",
      "Target Avg reg Loss : 0.0084\n",
      "Source Avg cla Loss : 0.6931\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0207\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 862, Total Avg Loss : 1.3987\n",
      "Source Avg Loss : 0.7007\n",
      "Target Avg Loss : 0.6980\n",
      "Source Avg reg Loss : 0.0077\n",
      "Target Avg reg Loss : 0.0048\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0124\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 863, Total Avg Loss : 1.3987\n",
      "Source Avg Loss : 0.7003\n",
      "Target Avg Loss : 0.6985\n",
      "Source Avg reg Loss : 0.0072\n",
      "Target Avg reg Loss : 0.0052\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0125\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 864, Total Avg Loss : 1.4043\n",
      "Source Avg Loss : 0.7040\n",
      "Target Avg Loss : 0.7003\n",
      "Source Avg reg Loss : 0.0110\n",
      "Target Avg reg Loss : 0.0070\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0180\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 865, Total Avg Loss : 1.4091\n",
      "Source Avg Loss : 0.7074\n",
      "Target Avg Loss : 0.7017\n",
      "Source Avg reg Loss : 0.0144\n",
      "Target Avg reg Loss : 0.0084\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0228\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 866, Total Avg Loss : 1.4041\n",
      "Source Avg Loss : 0.7033\n",
      "Target Avg Loss : 0.7008\n",
      "Source Avg reg Loss : 0.0102\n",
      "Target Avg reg Loss : 0.0076\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0178\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 867, Total Avg Loss : 1.4067\n",
      "Source Avg Loss : 0.7057\n",
      "Target Avg Loss : 0.7010\n",
      "Source Avg reg Loss : 0.0126\n",
      "Target Avg reg Loss : 0.0078\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0204\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 868, Total Avg Loss : 1.4028\n",
      "Source Avg Loss : 0.7028\n",
      "Target Avg Loss : 0.7000\n",
      "Source Avg reg Loss : 0.0097\n",
      "Target Avg reg Loss : 0.0068\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0165\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 869, Total Avg Loss : 1.4025\n",
      "Source Avg Loss : 0.7034\n",
      "Target Avg Loss : 0.6991\n",
      "Source Avg reg Loss : 0.0104\n",
      "Target Avg reg Loss : 0.0058\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0162\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 870, Total Avg Loss : 1.4013\n",
      "Source Avg Loss : 0.7027\n",
      "Target Avg Loss : 0.6987\n",
      "Source Avg reg Loss : 0.0096\n",
      "Target Avg reg Loss : 0.0054\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0151\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 871, Total Avg Loss : 1.3970\n",
      "Source Avg Loss : 0.6993\n",
      "Target Avg Loss : 0.6977\n",
      "Source Avg reg Loss : 0.0063\n",
      "Target Avg reg Loss : 0.0044\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0107\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 872, Total Avg Loss : 1.4035\n",
      "Source Avg Loss : 0.7040\n",
      "Target Avg Loss : 0.6995\n",
      "Source Avg reg Loss : 0.0109\n",
      "Target Avg reg Loss : 0.0063\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0172\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 873, Total Avg Loss : 1.4041\n",
      "Source Avg Loss : 0.7039\n",
      "Target Avg Loss : 0.7002\n",
      "Source Avg reg Loss : 0.0109\n",
      "Target Avg reg Loss : 0.0070\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0178\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 874, Total Avg Loss : 1.3987\n",
      "Source Avg Loss : 0.7006\n",
      "Target Avg Loss : 0.6981\n",
      "Source Avg reg Loss : 0.0076\n",
      "Target Avg reg Loss : 0.0049\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0125\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 875, Total Avg Loss : 1.3991\n",
      "Source Avg Loss : 0.7010\n",
      "Target Avg Loss : 0.6981\n",
      "Source Avg reg Loss : 0.0080\n",
      "Target Avg reg Loss : 0.0049\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0128\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 876, Total Avg Loss : 1.3980\n",
      "Source Avg Loss : 0.7002\n",
      "Target Avg Loss : 0.6978\n",
      "Source Avg reg Loss : 0.0072\n",
      "Target Avg reg Loss : 0.0046\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0117\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 877, Total Avg Loss : 1.3964\n",
      "Source Avg Loss : 0.6989\n",
      "Target Avg Loss : 0.6975\n",
      "Source Avg reg Loss : 0.0059\n",
      "Target Avg reg Loss : 0.0043\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0102\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 878, Total Avg Loss : 1.3956\n",
      "Source Avg Loss : 0.6984\n",
      "Target Avg Loss : 0.6971\n",
      "Source Avg reg Loss : 0.0054\n",
      "Target Avg reg Loss : 0.0039\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0093\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 879, Total Avg Loss : 1.3952\n",
      "Source Avg Loss : 0.6981\n",
      "Target Avg Loss : 0.6971\n",
      "Source Avg reg Loss : 0.0051\n",
      "Target Avg reg Loss : 0.0039\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0090\n",
      "Avg Classification Loss : 1.3863\n",
      "\n",
      "Epoch : 880, Total Avg Loss : 1.3960\n",
      "Source Avg Loss : 0.6989\n",
      "Target Avg Loss : 0.6972\n",
      "Source Avg reg Loss : 0.0059\n",
      "Target Avg reg Loss : 0.0039\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0098\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 881, Total Avg Loss : 1.3960\n",
      "Source Avg Loss : 0.6987\n",
      "Target Avg Loss : 0.6973\n",
      "Source Avg reg Loss : 0.0057\n",
      "Target Avg reg Loss : 0.0041\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0098\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 882, Total Avg Loss : 1.4008\n",
      "Source Avg Loss : 0.7022\n",
      "Target Avg Loss : 0.6986\n",
      "Source Avg reg Loss : 0.0092\n",
      "Target Avg reg Loss : 0.0053\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0146\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 883, Total Avg Loss : 1.3998\n",
      "Source Avg Loss : 0.7008\n",
      "Target Avg Loss : 0.6989\n",
      "Source Avg reg Loss : 0.0078\n",
      "Target Avg reg Loss : 0.0057\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0135\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 884, Total Avg Loss : 1.3986\n",
      "Source Avg Loss : 0.7003\n",
      "Target Avg Loss : 0.6984\n",
      "Source Avg reg Loss : 0.0072\n",
      "Target Avg reg Loss : 0.0051\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0124\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 885, Total Avg Loss : 1.4008\n",
      "Source Avg Loss : 0.7017\n",
      "Target Avg Loss : 0.6990\n",
      "Source Avg reg Loss : 0.0087\n",
      "Target Avg reg Loss : 0.0058\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0145\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 886, Total Avg Loss : 1.4057\n",
      "Source Avg Loss : 0.7052\n",
      "Target Avg Loss : 0.7006\n",
      "Source Avg reg Loss : 0.0121\n",
      "Target Avg reg Loss : 0.0073\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0195\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 887, Total Avg Loss : 1.4003\n",
      "Source Avg Loss : 0.7014\n",
      "Target Avg Loss : 0.6989\n",
      "Source Avg reg Loss : 0.0084\n",
      "Target Avg reg Loss : 0.0057\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0141\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 888, Total Avg Loss : 1.4040\n",
      "Source Avg Loss : 0.7042\n",
      "Target Avg Loss : 0.6998\n",
      "Source Avg reg Loss : 0.0112\n",
      "Target Avg reg Loss : 0.0065\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0177\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 889, Total Avg Loss : 1.3986\n",
      "Source Avg Loss : 0.7006\n",
      "Target Avg Loss : 0.6980\n",
      "Source Avg reg Loss : 0.0076\n",
      "Target Avg reg Loss : 0.0048\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0123\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 890, Total Avg Loss : 1.3967\n",
      "Source Avg Loss : 0.6991\n",
      "Target Avg Loss : 0.6975\n",
      "Source Avg reg Loss : 0.0061\n",
      "Target Avg reg Loss : 0.0043\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0104\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 891, Total Avg Loss : 1.4007\n",
      "Source Avg Loss : 0.7018\n",
      "Target Avg Loss : 0.6988\n",
      "Source Avg reg Loss : 0.0088\n",
      "Target Avg reg Loss : 0.0056\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0144\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 892, Total Avg Loss : 1.4025\n",
      "Source Avg Loss : 0.7030\n",
      "Target Avg Loss : 0.6996\n",
      "Source Avg reg Loss : 0.0100\n",
      "Target Avg reg Loss : 0.0063\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0163\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 893, Total Avg Loss : 1.3967\n",
      "Source Avg Loss : 0.6990\n",
      "Target Avg Loss : 0.6977\n",
      "Source Avg reg Loss : 0.0060\n",
      "Target Avg reg Loss : 0.0045\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0105\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 894, Total Avg Loss : 1.3958\n",
      "Source Avg Loss : 0.6985\n",
      "Target Avg Loss : 0.6972\n",
      "Source Avg reg Loss : 0.0056\n",
      "Target Avg reg Loss : 0.0040\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0096\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 895, Total Avg Loss : 1.3960\n",
      "Source Avg Loss : 0.6987\n",
      "Target Avg Loss : 0.6974\n",
      "Source Avg reg Loss : 0.0057\n",
      "Target Avg reg Loss : 0.0042\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0098\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 896, Total Avg Loss : 1.4001\n",
      "Source Avg Loss : 0.7014\n",
      "Target Avg Loss : 0.6987\n",
      "Source Avg reg Loss : 0.0084\n",
      "Target Avg reg Loss : 0.0054\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0139\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 897, Total Avg Loss : 1.4029\n",
      "Source Avg Loss : 0.7035\n",
      "Target Avg Loss : 0.6994\n",
      "Source Avg reg Loss : 0.0106\n",
      "Target Avg reg Loss : 0.0061\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0167\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 898, Total Avg Loss : 1.3998\n",
      "Source Avg Loss : 0.7010\n",
      "Target Avg Loss : 0.6988\n",
      "Source Avg reg Loss : 0.0081\n",
      "Target Avg reg Loss : 0.0055\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0136\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 899, Total Avg Loss : 1.4005\n",
      "Source Avg Loss : 0.7021\n",
      "Target Avg Loss : 0.6985\n",
      "Source Avg reg Loss : 0.0091\n",
      "Target Avg reg Loss : 0.0052\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0143\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 900, Total Avg Loss : 1.3986\n",
      "Source Avg Loss : 0.7003\n",
      "Target Avg Loss : 0.6983\n",
      "Source Avg reg Loss : 0.0073\n",
      "Target Avg reg Loss : 0.0051\n",
      "Source Avg cla Loss : 0.6930\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0124\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 901, Total Avg Loss : 1.3989\n",
      "Source Avg Loss : 0.7010\n",
      "Target Avg Loss : 0.6980\n",
      "Source Avg reg Loss : 0.0080\n",
      "Target Avg reg Loss : 0.0047\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0128\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 902, Total Avg Loss : 1.3978\n",
      "Source Avg Loss : 0.7001\n",
      "Target Avg Loss : 0.6976\n",
      "Source Avg reg Loss : 0.0072\n",
      "Target Avg reg Loss : 0.0044\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0116\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 903, Total Avg Loss : 1.3950\n",
      "Source Avg Loss : 0.6980\n",
      "Target Avg Loss : 0.6970\n",
      "Source Avg reg Loss : 0.0051\n",
      "Target Avg reg Loss : 0.0037\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0088\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 904, Total Avg Loss : 1.3994\n",
      "Source Avg Loss : 0.7011\n",
      "Target Avg Loss : 0.6982\n",
      "Source Avg reg Loss : 0.0082\n",
      "Target Avg reg Loss : 0.0050\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0132\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 905, Total Avg Loss : 1.3998\n",
      "Source Avg Loss : 0.7013\n",
      "Target Avg Loss : 0.6984\n",
      "Source Avg reg Loss : 0.0084\n",
      "Target Avg reg Loss : 0.0052\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6932\n",
      "Avg Regression Loss : 0.0136\n",
      "Avg Classification Loss : 1.3862\n",
      "\n",
      "Epoch : 906, Total Avg Loss : 1.3959\n",
      "Source Avg Loss : 0.6988\n",
      "Target Avg Loss : 0.6972\n",
      "Source Avg reg Loss : 0.0059\n",
      "Target Avg reg Loss : 0.0039\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0098\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 907, Total Avg Loss : 1.3957\n",
      "Source Avg Loss : 0.6984\n",
      "Target Avg Loss : 0.6973\n",
      "Source Avg reg Loss : 0.0055\n",
      "Target Avg reg Loss : 0.0040\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0096\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 908, Total Avg Loss : 1.3978\n",
      "Source Avg Loss : 0.6999\n",
      "Target Avg Loss : 0.6978\n",
      "Source Avg reg Loss : 0.0070\n",
      "Target Avg reg Loss : 0.0046\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0116\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 909, Total Avg Loss : 1.3983\n",
      "Source Avg Loss : 0.7004\n",
      "Target Avg Loss : 0.6978\n",
      "Source Avg reg Loss : 0.0076\n",
      "Target Avg reg Loss : 0.0046\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0121\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 910, Total Avg Loss : 1.3976\n",
      "Source Avg Loss : 0.6998\n",
      "Target Avg Loss : 0.6977\n",
      "Source Avg reg Loss : 0.0069\n",
      "Target Avg reg Loss : 0.0045\n",
      "Source Avg cla Loss : 0.6929\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0114\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 911, Total Avg Loss : 1.3947\n",
      "Source Avg Loss : 0.6979\n",
      "Target Avg Loss : 0.6968\n",
      "Source Avg reg Loss : 0.0051\n",
      "Target Avg reg Loss : 0.0035\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0086\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 912, Total Avg Loss : 1.3960\n",
      "Source Avg Loss : 0.6987\n",
      "Target Avg Loss : 0.6973\n",
      "Source Avg reg Loss : 0.0059\n",
      "Target Avg reg Loss : 0.0040\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0099\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 913, Total Avg Loss : 1.3965\n",
      "Source Avg Loss : 0.6991\n",
      "Target Avg Loss : 0.6975\n",
      "Source Avg reg Loss : 0.0062\n",
      "Target Avg reg Loss : 0.0042\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0104\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 914, Total Avg Loss : 1.3940\n",
      "Source Avg Loss : 0.6973\n",
      "Target Avg Loss : 0.6967\n",
      "Source Avg reg Loss : 0.0045\n",
      "Target Avg reg Loss : 0.0034\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0079\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 915, Total Avg Loss : 1.3947\n",
      "Source Avg Loss : 0.6978\n",
      "Target Avg Loss : 0.6969\n",
      "Source Avg reg Loss : 0.0051\n",
      "Target Avg reg Loss : 0.0036\n",
      "Source Avg cla Loss : 0.6928\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0087\n",
      "Avg Classification Loss : 1.3861\n",
      "\n",
      "Epoch : 916, Total Avg Loss : 1.3949\n",
      "Source Avg Loss : 0.6978\n",
      "Target Avg Loss : 0.6971\n",
      "Source Avg reg Loss : 0.0051\n",
      "Target Avg reg Loss : 0.0038\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0089\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 917, Total Avg Loss : 1.3977\n",
      "Source Avg Loss : 0.6997\n",
      "Target Avg Loss : 0.6980\n",
      "Source Avg reg Loss : 0.0070\n",
      "Target Avg reg Loss : 0.0047\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0117\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 918, Total Avg Loss : 1.3973\n",
      "Source Avg Loss : 0.6997\n",
      "Target Avg Loss : 0.6976\n",
      "Source Avg reg Loss : 0.0070\n",
      "Target Avg reg Loss : 0.0043\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0113\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 919, Total Avg Loss : 1.3970\n",
      "Source Avg Loss : 0.6993\n",
      "Target Avg Loss : 0.6977\n",
      "Source Avg reg Loss : 0.0066\n",
      "Target Avg reg Loss : 0.0044\n",
      "Source Avg cla Loss : 0.6927\n",
      "Target Avg cla Loss : 0.6933\n",
      "Avg Regression Loss : 0.0110\n",
      "Avg Classification Loss : 1.3860\n",
      "\n",
      "Epoch : 920, Total Avg Loss : 1.3938\n",
      "Source Avg Loss : 0.6972\n",
      "Target Avg Loss : 0.6966\n",
      "Source Avg reg Loss : 0.0046\n",
      "Target Avg reg Loss : 0.0033\n",
      "Source Avg cla Loss : 0.6926\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0078\n",
      "Avg Classification Loss : 1.3859\n",
      "\n",
      "Epoch : 921, Total Avg Loss : 1.3935\n",
      "Source Avg Loss : 0.6969\n",
      "Target Avg Loss : 0.6966\n",
      "Source Avg reg Loss : 0.0044\n",
      "Target Avg reg Loss : 0.0032\n",
      "Source Avg cla Loss : 0.6925\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0076\n",
      "Avg Classification Loss : 1.3859\n",
      "\n",
      "Epoch : 922, Total Avg Loss : 1.3937\n",
      "Source Avg Loss : 0.6970\n",
      "Target Avg Loss : 0.6968\n",
      "Source Avg reg Loss : 0.0045\n",
      "Target Avg reg Loss : 0.0034\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0079\n",
      "Avg Classification Loss : 1.3859\n",
      "\n",
      "Epoch : 923, Total Avg Loss : 1.3945\n",
      "Source Avg Loss : 0.6976\n",
      "Target Avg Loss : 0.6969\n",
      "Source Avg reg Loss : 0.0052\n",
      "Target Avg reg Loss : 0.0035\n",
      "Source Avg cla Loss : 0.6924\n",
      "Target Avg cla Loss : 0.6934\n",
      "Avg Regression Loss : 0.0087\n",
      "Avg Classification Loss : 1.3858\n",
      "\n",
      "Epoch : 924, Total Avg Loss : 1.3932\n",
      "Source Avg Loss : 0.6965\n",
      "Target Avg Loss : 0.6967\n",
      "Source Avg reg Loss : 0.0043\n",
      "Target Avg reg Loss : 0.0032\n",
      "Source Avg cla Loss : 0.6922\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0075\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 925, Total Avg Loss : 1.3935\n",
      "Source Avg Loss : 0.6966\n",
      "Target Avg Loss : 0.6969\n",
      "Source Avg reg Loss : 0.0046\n",
      "Target Avg reg Loss : 0.0033\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6935\n",
      "Avg Regression Loss : 0.0079\n",
      "Avg Classification Loss : 1.3856\n",
      "\n",
      "Epoch : 926, Total Avg Loss : 1.3945\n",
      "Source Avg Loss : 0.6974\n",
      "Target Avg Loss : 0.6972\n",
      "Source Avg reg Loss : 0.0054\n",
      "Target Avg reg Loss : 0.0035\n",
      "Source Avg cla Loss : 0.6920\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0089\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 927, Total Avg Loss : 1.3951\n",
      "Source Avg Loss : 0.6973\n",
      "Target Avg Loss : 0.6977\n",
      "Source Avg reg Loss : 0.0054\n",
      "Target Avg reg Loss : 0.0039\n",
      "Source Avg cla Loss : 0.6919\n",
      "Target Avg cla Loss : 0.6938\n",
      "Avg Regression Loss : 0.0094\n",
      "Avg Classification Loss : 1.3857\n",
      "\n",
      "Epoch : 928, Total Avg Loss : 1.3934\n",
      "Source Avg Loss : 0.6963\n",
      "Target Avg Loss : 0.6971\n",
      "Source Avg reg Loss : 0.0046\n",
      "Target Avg reg Loss : 0.0034\n",
      "Source Avg cla Loss : 0.6916\n",
      "Target Avg cla Loss : 0.6937\n",
      "Avg Regression Loss : 0.0080\n",
      "Avg Classification Loss : 1.3854\n",
      "\n",
      "Epoch : 929, Total Avg Loss : 1.3964\n",
      "Source Avg Loss : 0.6978\n",
      "Target Avg Loss : 0.6985\n",
      "Source Avg reg Loss : 0.0066\n",
      "Target Avg reg Loss : 0.0046\n",
      "Source Avg cla Loss : 0.6912\n",
      "Target Avg cla Loss : 0.6939\n",
      "Avg Regression Loss : 0.0112\n",
      "Avg Classification Loss : 1.3852\n",
      "\n",
      "Epoch : 930, Total Avg Loss : 1.3960\n",
      "Source Avg Loss : 0.6977\n",
      "Target Avg Loss : 0.6983\n",
      "Source Avg reg Loss : 0.0070\n",
      "Target Avg reg Loss : 0.0042\n",
      "Source Avg cla Loss : 0.6906\n",
      "Target Avg cla Loss : 0.6941\n",
      "Avg Regression Loss : 0.0112\n",
      "Avg Classification Loss : 1.3847\n",
      "\n",
      "Epoch : 931, Total Avg Loss : 1.3944\n",
      "Source Avg Loss : 0.6958\n",
      "Target Avg Loss : 0.6986\n",
      "Source Avg reg Loss : 0.0055\n",
      "Target Avg reg Loss : 0.0042\n",
      "Source Avg cla Loss : 0.6902\n",
      "Target Avg cla Loss : 0.6944\n",
      "Avg Regression Loss : 0.0097\n",
      "Avg Classification Loss : 1.3847\n",
      "\n",
      "Epoch : 932, Total Avg Loss : 1.3947\n",
      "Source Avg Loss : 0.6959\n",
      "Target Avg Loss : 0.6988\n",
      "Source Avg reg Loss : 0.0065\n",
      "Target Avg reg Loss : 0.0040\n",
      "Source Avg cla Loss : 0.6894\n",
      "Target Avg cla Loss : 0.6948\n",
      "Avg Regression Loss : 0.0105\n",
      "Avg Classification Loss : 1.3842\n",
      "\n",
      "Epoch : 933, Total Avg Loss : 1.3935\n",
      "Source Avg Loss : 0.6940\n",
      "Target Avg Loss : 0.6995\n",
      "Source Avg reg Loss : 0.0054\n",
      "Target Avg reg Loss : 0.0041\n",
      "Source Avg cla Loss : 0.6885\n",
      "Target Avg cla Loss : 0.6955\n",
      "Avg Regression Loss : 0.0095\n",
      "Avg Classification Loss : 1.3840\n",
      "\n",
      "Epoch : 934, Total Avg Loss : 1.3926\n",
      "Source Avg Loss : 0.6925\n",
      "Target Avg Loss : 0.7001\n",
      "Source Avg reg Loss : 0.0056\n",
      "Target Avg reg Loss : 0.0037\n",
      "Source Avg cla Loss : 0.6869\n",
      "Target Avg cla Loss : 0.6964\n",
      "Avg Regression Loss : 0.0093\n",
      "Avg Classification Loss : 1.3833\n",
      "\n",
      "Epoch : 935, Total Avg Loss : 1.3916\n",
      "Source Avg Loss : 0.6903\n",
      "Target Avg Loss : 0.7013\n",
      "Source Avg reg Loss : 0.0050\n",
      "Target Avg reg Loss : 0.0037\n",
      "Source Avg cla Loss : 0.6853\n",
      "Target Avg cla Loss : 0.6976\n",
      "Avg Regression Loss : 0.0087\n",
      "Avg Classification Loss : 1.3829\n",
      "\n",
      "Epoch : 936, Total Avg Loss : 1.3898\n",
      "Source Avg Loss : 0.6880\n",
      "Target Avg Loss : 0.7018\n",
      "Source Avg reg Loss : 0.0045\n",
      "Target Avg reg Loss : 0.0032\n",
      "Source Avg cla Loss : 0.6835\n",
      "Target Avg cla Loss : 0.6986\n",
      "Avg Regression Loss : 0.0077\n",
      "Avg Classification Loss : 1.3821\n",
      "\n",
      "Epoch : 937, Total Avg Loss : 1.3893\n",
      "Source Avg Loss : 0.6853\n",
      "Target Avg Loss : 0.7040\n",
      "Source Avg reg Loss : 0.0046\n",
      "Target Avg reg Loss : 0.0034\n",
      "Source Avg cla Loss : 0.6807\n",
      "Target Avg cla Loss : 0.7006\n",
      "Avg Regression Loss : 0.0081\n",
      "Avg Classification Loss : 1.3812\n",
      "\n",
      "Epoch : 938, Total Avg Loss : 1.3890\n",
      "Source Avg Loss : 0.6823\n",
      "Target Avg Loss : 0.7067\n",
      "Source Avg reg Loss : 0.0048\n",
      "Target Avg reg Loss : 0.0036\n",
      "Source Avg cla Loss : 0.6774\n",
      "Target Avg cla Loss : 0.7031\n",
      "Avg Regression Loss : 0.0084\n",
      "Avg Classification Loss : 1.3806\n",
      "\n",
      "Epoch : 939, Total Avg Loss : 1.3878\n",
      "Source Avg Loss : 0.6785\n",
      "Target Avg Loss : 0.7093\n",
      "Source Avg reg Loss : 0.0053\n",
      "Target Avg reg Loss : 0.0038\n",
      "Source Avg cla Loss : 0.6732\n",
      "Target Avg cla Loss : 0.7056\n",
      "Avg Regression Loss : 0.0091\n",
      "Avg Classification Loss : 1.3787\n",
      "\n",
      "Epoch : 940, Total Avg Loss : 1.3878\n",
      "Source Avg Loss : 0.6737\n",
      "Target Avg Loss : 0.7141\n",
      "Source Avg reg Loss : 0.0046\n",
      "Target Avg reg Loss : 0.0036\n",
      "Source Avg cla Loss : 0.6691\n",
      "Target Avg cla Loss : 0.7105\n",
      "Avg Regression Loss : 0.0082\n",
      "Avg Classification Loss : 1.3796\n",
      "\n",
      "Epoch : 941, Total Avg Loss : 1.3872\n",
      "Source Avg Loss : 0.6691\n",
      "Target Avg Loss : 0.7182\n",
      "Source Avg reg Loss : 0.0050\n",
      "Target Avg reg Loss : 0.0037\n",
      "Source Avg cla Loss : 0.6641\n",
      "Target Avg cla Loss : 0.7144\n",
      "Avg Regression Loss : 0.0087\n",
      "Avg Classification Loss : 1.3785\n",
      "\n",
      "Epoch : 942, Total Avg Loss : 1.3866\n",
      "Source Avg Loss : 0.6603\n",
      "Target Avg Loss : 0.7263\n",
      "Source Avg reg Loss : 0.0052\n",
      "Target Avg reg Loss : 0.0039\n",
      "Source Avg cla Loss : 0.6550\n",
      "Target Avg cla Loss : 0.7225\n",
      "Avg Regression Loss : 0.0091\n",
      "Avg Classification Loss : 1.3775\n",
      "\n",
      "Epoch : 943, Total Avg Loss : 1.3849\n",
      "Source Avg Loss : 0.6507\n",
      "Target Avg Loss : 0.7342\n",
      "Source Avg reg Loss : 0.0052\n",
      "Target Avg reg Loss : 0.0040\n",
      "Source Avg cla Loss : 0.6455\n",
      "Target Avg cla Loss : 0.7302\n",
      "Avg Regression Loss : 0.0092\n",
      "Avg Classification Loss : 1.3757\n",
      "\n",
      "Epoch : 944, Total Avg Loss : 1.3842\n",
      "Source Avg Loss : 0.6416\n",
      "Target Avg Loss : 0.7426\n",
      "Source Avg reg Loss : 0.0052\n",
      "Target Avg reg Loss : 0.0039\n",
      "Source Avg cla Loss : 0.6364\n",
      "Target Avg cla Loss : 0.7388\n",
      "Avg Regression Loss : 0.0091\n",
      "Avg Classification Loss : 1.3751\n",
      "\n",
      "Epoch : 945, Total Avg Loss : 1.3857\n",
      "Source Avg Loss : 0.6322\n",
      "Target Avg Loss : 0.7535\n",
      "Source Avg reg Loss : 0.0055\n",
      "Target Avg reg Loss : 0.0041\n",
      "Source Avg cla Loss : 0.6267\n",
      "Target Avg cla Loss : 0.7494\n",
      "Avg Regression Loss : 0.0096\n",
      "Avg Classification Loss : 1.3761\n",
      "\n",
      "Epoch : 946, Total Avg Loss : 1.3882\n",
      "Source Avg Loss : 0.6243\n",
      "Target Avg Loss : 0.7639\n",
      "Source Avg reg Loss : 0.0060\n",
      "Target Avg reg Loss : 0.0044\n",
      "Source Avg cla Loss : 0.6183\n",
      "Target Avg cla Loss : 0.7595\n",
      "Avg Regression Loss : 0.0104\n",
      "Avg Classification Loss : 1.3778\n",
      "\n",
      "Epoch : 947, Total Avg Loss : 1.3940\n",
      "Source Avg Loss : 0.6214\n",
      "Target Avg Loss : 0.7726\n",
      "Source Avg reg Loss : 0.0062\n",
      "Target Avg reg Loss : 0.0046\n",
      "Source Avg cla Loss : 0.6152\n",
      "Target Avg cla Loss : 0.7681\n",
      "Avg Regression Loss : 0.0107\n",
      "Avg Classification Loss : 1.3833\n",
      "\n",
      "Epoch : 948, Total Avg Loss : 1.4000\n",
      "Source Avg Loss : 0.6201\n",
      "Target Avg Loss : 0.7799\n",
      "Source Avg reg Loss : 0.0071\n",
      "Target Avg reg Loss : 0.0050\n",
      "Source Avg cla Loss : 0.6131\n",
      "Target Avg cla Loss : 0.7749\n",
      "Avg Regression Loss : 0.0121\n",
      "Avg Classification Loss : 1.3880\n",
      "\n",
      "Epoch : 949, Total Avg Loss : 1.4078\n",
      "Source Avg Loss : 0.6193\n",
      "Target Avg Loss : 0.7884\n",
      "Source Avg reg Loss : 0.0076\n",
      "Target Avg reg Loss : 0.0056\n",
      "Source Avg cla Loss : 0.6118\n",
      "Target Avg cla Loss : 0.7828\n",
      "Avg Regression Loss : 0.0132\n",
      "Avg Classification Loss : 1.3946\n",
      "\n",
      "Epoch : 950, Total Avg Loss : 1.4164\n",
      "Source Avg Loss : 0.6206\n",
      "Target Avg Loss : 0.7958\n",
      "Source Avg reg Loss : 0.0086\n",
      "Target Avg reg Loss : 0.0061\n",
      "Source Avg cla Loss : 0.6120\n",
      "Target Avg cla Loss : 0.7898\n",
      "Avg Regression Loss : 0.0146\n",
      "Avg Classification Loss : 1.4017\n",
      "\n",
      "Epoch : 951, Total Avg Loss : 1.4273\n",
      "Source Avg Loss : 0.6236\n",
      "Target Avg Loss : 0.8037\n",
      "Source Avg reg Loss : 0.0093\n",
      "Target Avg reg Loss : 0.0069\n",
      "Source Avg cla Loss : 0.6143\n",
      "Target Avg cla Loss : 0.7968\n",
      "Avg Regression Loss : 0.0162\n",
      "Avg Classification Loss : 1.4111\n",
      "\n",
      "Epoch : 952, Total Avg Loss : 1.4406\n",
      "Source Avg Loss : 0.6283\n",
      "Target Avg Loss : 0.8123\n",
      "Source Avg reg Loss : 0.0107\n",
      "Target Avg reg Loss : 0.0078\n",
      "Source Avg cla Loss : 0.6176\n",
      "Target Avg cla Loss : 0.8045\n",
      "Avg Regression Loss : 0.0185\n",
      "Avg Classification Loss : 1.4220\n",
      "\n",
      "Epoch : 953, Total Avg Loss : 1.4527\n",
      "Source Avg Loss : 0.6329\n",
      "Target Avg Loss : 0.8198\n",
      "Source Avg reg Loss : 0.0114\n",
      "Target Avg reg Loss : 0.0090\n",
      "Source Avg cla Loss : 0.6215\n",
      "Target Avg cla Loss : 0.8108\n",
      "Avg Regression Loss : 0.0204\n",
      "Avg Classification Loss : 1.4323\n",
      "\n",
      "Epoch : 954, Total Avg Loss : 1.4676\n",
      "Source Avg Loss : 0.6400\n",
      "Target Avg Loss : 0.8276\n",
      "Source Avg reg Loss : 0.0126\n",
      "Target Avg reg Loss : 0.0100\n",
      "Source Avg cla Loss : 0.6274\n",
      "Target Avg cla Loss : 0.8176\n",
      "Avg Regression Loss : 0.0226\n",
      "Avg Classification Loss : 1.4450\n",
      "\n",
      "Epoch : 955, Total Avg Loss : 1.4768\n",
      "Source Avg Loss : 0.6453\n",
      "Target Avg Loss : 0.8314\n",
      "Source Avg reg Loss : 0.0128\n",
      "Target Avg reg Loss : 0.0108\n",
      "Source Avg cla Loss : 0.6326\n",
      "Target Avg cla Loss : 0.8207\n",
      "Avg Regression Loss : 0.0235\n",
      "Avg Classification Loss : 1.4532\n",
      "\n",
      "Epoch : 956, Total Avg Loss : 1.4881\n",
      "Source Avg Loss : 0.6523\n",
      "Target Avg Loss : 0.8359\n",
      "Source Avg reg Loss : 0.0127\n",
      "Target Avg reg Loss : 0.0113\n",
      "Source Avg cla Loss : 0.6396\n",
      "Target Avg cla Loss : 0.8245\n",
      "Avg Regression Loss : 0.0241\n",
      "Avg Classification Loss : 1.4641\n",
      "\n",
      "Epoch : 957, Total Avg Loss : 1.4928\n",
      "Source Avg Loss : 0.6567\n",
      "Target Avg Loss : 0.8361\n",
      "Source Avg reg Loss : 0.0127\n",
      "Target Avg reg Loss : 0.0120\n",
      "Source Avg cla Loss : 0.6440\n",
      "Target Avg cla Loss : 0.8240\n",
      "Avg Regression Loss : 0.0247\n",
      "Avg Classification Loss : 1.4680\n",
      "\n",
      "Epoch : 958, Total Avg Loss : 1.4931\n",
      "Source Avg Loss : 0.6616\n",
      "Target Avg Loss : 0.8315\n",
      "Source Avg reg Loss : 0.0126\n",
      "Target Avg reg Loss : 0.0132\n",
      "Source Avg cla Loss : 0.6490\n",
      "Target Avg cla Loss : 0.8184\n",
      "Avg Regression Loss : 0.0258\n",
      "Avg Classification Loss : 1.4673\n",
      "\n",
      "Epoch : 959, Total Avg Loss : 1.4967\n",
      "Source Avg Loss : 0.6671\n",
      "Target Avg Loss : 0.8296\n",
      "Source Avg reg Loss : 0.0124\n",
      "Target Avg reg Loss : 0.0145\n",
      "Source Avg cla Loss : 0.6547\n",
      "Target Avg cla Loss : 0.8152\n",
      "Avg Regression Loss : 0.0268\n",
      "Avg Classification Loss : 1.4699\n",
      "\n",
      "Epoch : 960, Total Avg Loss : 1.4951\n",
      "Source Avg Loss : 0.6699\n",
      "Target Avg Loss : 0.8253\n",
      "Source Avg reg Loss : 0.0122\n",
      "Target Avg reg Loss : 0.0160\n",
      "Source Avg cla Loss : 0.6577\n",
      "Target Avg cla Loss : 0.8093\n",
      "Avg Regression Loss : 0.0282\n",
      "Avg Classification Loss : 1.4669\n",
      "\n",
      "Epoch : 961, Total Avg Loss : 1.4962\n",
      "Source Avg Loss : 0.6739\n",
      "Target Avg Loss : 0.8224\n",
      "Source Avg reg Loss : 0.0123\n",
      "Target Avg reg Loss : 0.0170\n",
      "Source Avg cla Loss : 0.6615\n",
      "Target Avg cla Loss : 0.8053\n",
      "Avg Regression Loss : 0.0294\n",
      "Avg Classification Loss : 1.4668\n",
      "\n",
      "Epoch : 962, Total Avg Loss : 1.4933\n",
      "Source Avg Loss : 0.6763\n",
      "Target Avg Loss : 0.8171\n",
      "Source Avg reg Loss : 0.0127\n",
      "Target Avg reg Loss : 0.0182\n",
      "Source Avg cla Loss : 0.6636\n",
      "Target Avg cla Loss : 0.7989\n",
      "Avg Regression Loss : 0.0308\n",
      "Avg Classification Loss : 1.4625\n",
      "\n",
      "Epoch : 963, Total Avg Loss : 1.4920\n",
      "Source Avg Loss : 0.6783\n",
      "Target Avg Loss : 0.8138\n",
      "Source Avg reg Loss : 0.0127\n",
      "Target Avg reg Loss : 0.0185\n",
      "Source Avg cla Loss : 0.6656\n",
      "Target Avg cla Loss : 0.7952\n",
      "Avg Regression Loss : 0.0312\n",
      "Avg Classification Loss : 1.4608\n",
      "\n",
      "Epoch : 964, Total Avg Loss : 1.4909\n",
      "Source Avg Loss : 0.6810\n",
      "Target Avg Loss : 0.8099\n",
      "Source Avg reg Loss : 0.0130\n",
      "Target Avg reg Loss : 0.0196\n",
      "Source Avg cla Loss : 0.6679\n",
      "Target Avg cla Loss : 0.7903\n",
      "Avg Regression Loss : 0.0326\n",
      "Avg Classification Loss : 1.4583\n",
      "\n",
      "Epoch : 965, Total Avg Loss : 1.4918\n",
      "Source Avg Loss : 0.6827\n",
      "Target Avg Loss : 0.8091\n",
      "Source Avg reg Loss : 0.0131\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.6696\n",
      "Target Avg cla Loss : 0.7893\n",
      "Avg Regression Loss : 0.0329\n",
      "Avg Classification Loss : 1.4589\n",
      "\n",
      "Epoch : 966, Total Avg Loss : 1.4907\n",
      "Source Avg Loss : 0.6848\n",
      "Target Avg Loss : 0.8059\n",
      "Source Avg reg Loss : 0.0134\n",
      "Target Avg reg Loss : 0.0202\n",
      "Source Avg cla Loss : 0.6714\n",
      "Target Avg cla Loss : 0.7857\n",
      "Avg Regression Loss : 0.0336\n",
      "Avg Classification Loss : 1.4571\n",
      "\n",
      "Epoch : 967, Total Avg Loss : 1.4904\n",
      "Source Avg Loss : 0.6868\n",
      "Target Avg Loss : 0.8036\n",
      "Source Avg reg Loss : 0.0138\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.6730\n",
      "Target Avg cla Loss : 0.7837\n",
      "Avg Regression Loss : 0.0336\n",
      "Avg Classification Loss : 1.4567\n",
      "\n",
      "Epoch : 968, Total Avg Loss : 1.4884\n",
      "Source Avg Loss : 0.6872\n",
      "Target Avg Loss : 0.8012\n",
      "Source Avg reg Loss : 0.0135\n",
      "Target Avg reg Loss : 0.0199\n",
      "Source Avg cla Loss : 0.6737\n",
      "Target Avg cla Loss : 0.7813\n",
      "Avg Regression Loss : 0.0334\n",
      "Avg Classification Loss : 1.4550\n",
      "\n",
      "Epoch : 969, Total Avg Loss : 1.4886\n",
      "Source Avg Loss : 0.6884\n",
      "Target Avg Loss : 0.8001\n",
      "Source Avg reg Loss : 0.0131\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6753\n",
      "Target Avg cla Loss : 0.7802\n",
      "Avg Regression Loss : 0.0331\n",
      "Avg Classification Loss : 1.4555\n",
      "\n",
      "Epoch : 970, Total Avg Loss : 1.4874\n",
      "Source Avg Loss : 0.6898\n",
      "Target Avg Loss : 0.7975\n",
      "Source Avg reg Loss : 0.0136\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6762\n",
      "Target Avg cla Loss : 0.7775\n",
      "Avg Regression Loss : 0.0336\n",
      "Avg Classification Loss : 1.4538\n",
      "\n",
      "Epoch : 971, Total Avg Loss : 1.4867\n",
      "Source Avg Loss : 0.6906\n",
      "Target Avg Loss : 0.7961\n",
      "Source Avg reg Loss : 0.0129\n",
      "Target Avg reg Loss : 0.0203\n",
      "Source Avg cla Loss : 0.6777\n",
      "Target Avg cla Loss : 0.7759\n",
      "Avg Regression Loss : 0.0332\n",
      "Avg Classification Loss : 1.4535\n",
      "\n",
      "Epoch : 972, Total Avg Loss : 1.4847\n",
      "Source Avg Loss : 0.6913\n",
      "Target Avg Loss : 0.7934\n",
      "Source Avg reg Loss : 0.0130\n",
      "Target Avg reg Loss : 0.0197\n",
      "Source Avg cla Loss : 0.6783\n",
      "Target Avg cla Loss : 0.7737\n",
      "Avg Regression Loss : 0.0327\n",
      "Avg Classification Loss : 1.4520\n",
      "\n",
      "Epoch : 973, Total Avg Loss : 1.4805\n",
      "Source Avg Loss : 0.6922\n",
      "Target Avg Loss : 0.7883\n",
      "Source Avg reg Loss : 0.0129\n",
      "Target Avg reg Loss : 0.0202\n",
      "Source Avg cla Loss : 0.6793\n",
      "Target Avg cla Loss : 0.7681\n",
      "Avg Regression Loss : 0.0331\n",
      "Avg Classification Loss : 1.4474\n",
      "\n",
      "Epoch : 974, Total Avg Loss : 1.4781\n",
      "Source Avg Loss : 0.6932\n",
      "Target Avg Loss : 0.7850\n",
      "Source Avg reg Loss : 0.0125\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6806\n",
      "Target Avg cla Loss : 0.7650\n",
      "Avg Regression Loss : 0.0325\n",
      "Avg Classification Loss : 1.4456\n",
      "\n",
      "Epoch : 975, Total Avg Loss : 1.4736\n",
      "Source Avg Loss : 0.6937\n",
      "Target Avg Loss : 0.7798\n",
      "Source Avg reg Loss : 0.0125\n",
      "Target Avg reg Loss : 0.0200\n",
      "Source Avg cla Loss : 0.6812\n",
      "Target Avg cla Loss : 0.7599\n",
      "Avg Regression Loss : 0.0325\n",
      "Avg Classification Loss : 1.4411\n",
      "\n",
      "Epoch : 976, Total Avg Loss : 1.4725\n",
      "Source Avg Loss : 0.6951\n",
      "Target Avg Loss : 0.7774\n",
      "Source Avg reg Loss : 0.0129\n",
      "Target Avg reg Loss : 0.0199\n",
      "Source Avg cla Loss : 0.6823\n",
      "Target Avg cla Loss : 0.7575\n",
      "Avg Regression Loss : 0.0327\n",
      "Avg Classification Loss : 1.4398\n",
      "\n",
      "Epoch : 977, Total Avg Loss : 1.4665\n",
      "Source Avg Loss : 0.6950\n",
      "Target Avg Loss : 0.7715\n",
      "Source Avg reg Loss : 0.0123\n",
      "Target Avg reg Loss : 0.0197\n",
      "Source Avg cla Loss : 0.6827\n",
      "Target Avg cla Loss : 0.7518\n",
      "Avg Regression Loss : 0.0320\n",
      "Avg Classification Loss : 1.4345\n",
      "\n",
      "Epoch : 978, Total Avg Loss : 1.4616\n",
      "Source Avg Loss : 0.6954\n",
      "Target Avg Loss : 0.7662\n",
      "Source Avg reg Loss : 0.0123\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.6831\n",
      "Target Avg cla Loss : 0.7464\n",
      "Avg Regression Loss : 0.0321\n",
      "Avg Classification Loss : 1.4295\n",
      "\n",
      "Epoch : 979, Total Avg Loss : 1.4587\n",
      "Source Avg Loss : 0.6951\n",
      "Target Avg Loss : 0.7636\n",
      "Source Avg reg Loss : 0.0122\n",
      "Target Avg reg Loss : 0.0196\n",
      "Source Avg cla Loss : 0.6829\n",
      "Target Avg cla Loss : 0.7439\n",
      "Avg Regression Loss : 0.0319\n",
      "Avg Classification Loss : 1.4268\n",
      "\n",
      "Epoch : 980, Total Avg Loss : 1.4559\n",
      "Source Avg Loss : 0.6961\n",
      "Target Avg Loss : 0.7598\n",
      "Source Avg reg Loss : 0.0121\n",
      "Target Avg reg Loss : 0.0197\n",
      "Source Avg cla Loss : 0.6840\n",
      "Target Avg cla Loss : 0.7401\n",
      "Avg Regression Loss : 0.0317\n",
      "Avg Classification Loss : 1.4242\n",
      "\n",
      "Epoch : 981, Total Avg Loss : 1.4510\n",
      "Source Avg Loss : 0.6957\n",
      "Target Avg Loss : 0.7553\n",
      "Source Avg reg Loss : 0.0116\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.6841\n",
      "Target Avg cla Loss : 0.7359\n",
      "Avg Regression Loss : 0.0311\n",
      "Avg Classification Loss : 1.4200\n",
      "\n",
      "Epoch : 982, Total Avg Loss : 1.4479\n",
      "Source Avg Loss : 0.6957\n",
      "Target Avg Loss : 0.7522\n",
      "Source Avg reg Loss : 0.0113\n",
      "Target Avg reg Loss : 0.0195\n",
      "Source Avg cla Loss : 0.6844\n",
      "Target Avg cla Loss : 0.7327\n",
      "Avg Regression Loss : 0.0308\n",
      "Avg Classification Loss : 1.4172\n",
      "\n",
      "Epoch : 983, Total Avg Loss : 1.4474\n",
      "Source Avg Loss : 0.6958\n",
      "Target Avg Loss : 0.7516\n",
      "Source Avg reg Loss : 0.0111\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.6848\n",
      "Target Avg cla Loss : 0.7318\n",
      "Avg Regression Loss : 0.0309\n",
      "Avg Classification Loss : 1.4165\n",
      "\n",
      "Epoch : 984, Total Avg Loss : 1.4453\n",
      "Source Avg Loss : 0.6961\n",
      "Target Avg Loss : 0.7492\n",
      "Source Avg reg Loss : 0.0107\n",
      "Target Avg reg Loss : 0.0194\n",
      "Source Avg cla Loss : 0.6854\n",
      "Target Avg cla Loss : 0.7297\n",
      "Avg Regression Loss : 0.0302\n",
      "Avg Classification Loss : 1.4151\n",
      "\n",
      "Epoch : 985, Total Avg Loss : 1.4445\n",
      "Source Avg Loss : 0.6961\n",
      "Target Avg Loss : 0.7484\n",
      "Source Avg reg Loss : 0.0106\n",
      "Target Avg reg Loss : 0.0198\n",
      "Source Avg cla Loss : 0.6855\n",
      "Target Avg cla Loss : 0.7286\n",
      "Avg Regression Loss : 0.0304\n",
      "Avg Classification Loss : 1.4140\n",
      "\n",
      "Epoch : 986, Total Avg Loss : 1.4430\n",
      "Source Avg Loss : 0.6961\n",
      "Target Avg Loss : 0.7469\n",
      "Source Avg reg Loss : 0.0106\n",
      "Target Avg reg Loss : 0.0195\n",
      "Source Avg cla Loss : 0.6855\n",
      "Target Avg cla Loss : 0.7275\n",
      "Avg Regression Loss : 0.0300\n",
      "Avg Classification Loss : 1.4130\n",
      "\n",
      "Epoch : 987, Total Avg Loss : 1.4413\n",
      "Source Avg Loss : 0.6957\n",
      "Target Avg Loss : 0.7456\n",
      "Source Avg reg Loss : 0.0104\n",
      "Target Avg reg Loss : 0.0191\n",
      "Source Avg cla Loss : 0.6853\n",
      "Target Avg cla Loss : 0.7264\n",
      "Avg Regression Loss : 0.0296\n",
      "Avg Classification Loss : 1.4117\n",
      "\n",
      "Epoch : 988, Total Avg Loss : 1.4405\n",
      "Source Avg Loss : 0.6966\n",
      "Target Avg Loss : 0.7440\n",
      "Source Avg reg Loss : 0.0101\n",
      "Target Avg reg Loss : 0.0193\n",
      "Source Avg cla Loss : 0.6865\n",
      "Target Avg cla Loss : 0.7247\n",
      "Avg Regression Loss : 0.0294\n",
      "Avg Classification Loss : 1.4112\n",
      "\n",
      "Epoch : 989, Total Avg Loss : 1.4381\n",
      "Source Avg Loss : 0.6962\n",
      "Target Avg Loss : 0.7420\n",
      "Source Avg reg Loss : 0.0102\n",
      "Target Avg reg Loss : 0.0185\n",
      "Source Avg cla Loss : 0.6859\n",
      "Target Avg cla Loss : 0.7234\n",
      "Avg Regression Loss : 0.0288\n",
      "Avg Classification Loss : 1.4094\n",
      "\n",
      "Epoch : 990, Total Avg Loss : 1.4388\n",
      "Source Avg Loss : 0.6966\n",
      "Target Avg Loss : 0.7422\n",
      "Source Avg reg Loss : 0.0100\n",
      "Target Avg reg Loss : 0.0187\n",
      "Source Avg cla Loss : 0.6867\n",
      "Target Avg cla Loss : 0.7234\n",
      "Avg Regression Loss : 0.0287\n",
      "Avg Classification Loss : 1.4101\n",
      "\n",
      "Epoch : 991, Total Avg Loss : 1.4375\n",
      "Source Avg Loss : 0.6964\n",
      "Target Avg Loss : 0.7411\n",
      "Source Avg reg Loss : 0.0098\n",
      "Target Avg reg Loss : 0.0188\n",
      "Source Avg cla Loss : 0.6866\n",
      "Target Avg cla Loss : 0.7222\n",
      "Avg Regression Loss : 0.0286\n",
      "Avg Classification Loss : 1.4088\n",
      "\n",
      "Epoch : 992, Total Avg Loss : 1.4357\n",
      "Source Avg Loss : 0.6962\n",
      "Target Avg Loss : 0.7395\n",
      "Source Avg reg Loss : 0.0097\n",
      "Target Avg reg Loss : 0.0185\n",
      "Source Avg cla Loss : 0.6865\n",
      "Target Avg cla Loss : 0.7209\n",
      "Avg Regression Loss : 0.0282\n",
      "Avg Classification Loss : 1.4075\n",
      "\n",
      "Epoch : 993, Total Avg Loss : 1.4354\n",
      "Source Avg Loss : 0.6963\n",
      "Target Avg Loss : 0.7391\n",
      "Source Avg reg Loss : 0.0098\n",
      "Target Avg reg Loss : 0.0185\n",
      "Source Avg cla Loss : 0.6865\n",
      "Target Avg cla Loss : 0.7207\n",
      "Avg Regression Loss : 0.0283\n",
      "Avg Classification Loss : 1.4071\n",
      "\n",
      "Epoch : 994, Total Avg Loss : 1.4348\n",
      "Source Avg Loss : 0.6964\n",
      "Target Avg Loss : 0.7384\n",
      "Source Avg reg Loss : 0.0098\n",
      "Target Avg reg Loss : 0.0186\n",
      "Source Avg cla Loss : 0.6865\n",
      "Target Avg cla Loss : 0.7198\n",
      "Avg Regression Loss : 0.0285\n",
      "Avg Classification Loss : 1.4063\n",
      "\n",
      "Epoch : 995, Total Avg Loss : 1.4334\n",
      "Source Avg Loss : 0.6964\n",
      "Target Avg Loss : 0.7370\n",
      "Source Avg reg Loss : 0.0097\n",
      "Target Avg reg Loss : 0.0180\n",
      "Source Avg cla Loss : 0.6867\n",
      "Target Avg cla Loss : 0.7190\n",
      "Avg Regression Loss : 0.0277\n",
      "Avg Classification Loss : 1.4057\n",
      "\n",
      "Epoch : 996, Total Avg Loss : 1.4335\n",
      "Source Avg Loss : 0.6964\n",
      "Target Avg Loss : 0.7370\n",
      "Source Avg reg Loss : 0.0095\n",
      "Target Avg reg Loss : 0.0180\n",
      "Source Avg cla Loss : 0.6870\n",
      "Target Avg cla Loss : 0.7191\n",
      "Avg Regression Loss : 0.0274\n",
      "Avg Classification Loss : 1.4060\n",
      "\n",
      "Epoch : 997, Total Avg Loss : 1.4330\n",
      "Source Avg Loss : 0.6966\n",
      "Target Avg Loss : 0.7364\n",
      "Source Avg reg Loss : 0.0095\n",
      "Target Avg reg Loss : 0.0180\n",
      "Source Avg cla Loss : 0.6871\n",
      "Target Avg cla Loss : 0.7184\n",
      "Avg Regression Loss : 0.0275\n",
      "Avg Classification Loss : 1.4055\n",
      "\n",
      "Epoch : 998, Total Avg Loss : 1.4326\n",
      "Source Avg Loss : 0.6965\n",
      "Target Avg Loss : 0.7361\n",
      "Source Avg reg Loss : 0.0094\n",
      "Target Avg reg Loss : 0.0179\n",
      "Source Avg cla Loss : 0.6870\n",
      "Target Avg cla Loss : 0.7183\n",
      "Avg Regression Loss : 0.0273\n",
      "Avg Classification Loss : 1.4053\n",
      "\n",
      "Epoch : 999, Total Avg Loss : 1.4326\n",
      "Source Avg Loss : 0.6966\n",
      "Target Avg Loss : 0.7360\n",
      "Source Avg reg Loss : 0.0094\n",
      "Target Avg reg Loss : 0.0182\n",
      "Source Avg cla Loss : 0.6872\n",
      "Target Avg cla Loss : 0.7178\n",
      "Avg Regression Loss : 0.0276\n",
      "Avg Classification Loss : 1.4050\n",
      "\n",
      "Epoch : 1000, Total Avg Loss : 1.4324\n",
      "Source Avg Loss : 0.6965\n",
      "Target Avg Loss : 0.7359\n",
      "Source Avg reg Loss : 0.0091\n",
      "Target Avg reg Loss : 0.0180\n",
      "Source Avg cla Loss : 0.6873\n",
      "Target Avg cla Loss : 0.7179\n",
      "Avg Regression Loss : 0.0271\n",
      "Avg Classification Loss : 1.4053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "alpha = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "model.train() # 훈련 모드 설정\n",
    "\n",
    "optimizer= torch.optim.Adamax(model.parameters(), lr=0.01)#, weight_decay=0.01)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters = epochs * len(NELEC_train))\n",
    "\n",
    "patience = 200  # Early stopping을 위한 기다리는 최대 epoch 수\n",
    "best_loss = float('inf')\n",
    "best_source_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "counter = 10  # Early stopping 카운터\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    source_total_loss = 0\n",
    "    target_total_loss = 0\n",
    "    src_reg_total = 0\n",
    "    src_cla_total = 0\n",
    "    tar_reg_total = 0\n",
    "    tar_cal_total = 0\n",
    "    reg_loss_total = 0\n",
    "    cla_loss_total = 0\n",
    "    for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "        source_x = source_data[0].to(device)\n",
    "        source_y = source_data[1].to(device)\n",
    "        target_x = target_data[0].to(device)\n",
    "        target_y = target_data[1].to(device)\n",
    "        #print(source_x.shape)\n",
    "        #print(source_y.shape)\n",
    "        #print(target_x.shape)\n",
    "        #print(target_y.shape)\n",
    "        # 순전파\n",
    "        source_result = model(source_x)\n",
    "        target_result = model(target_x)\n",
    "        #print(source_result[0].shape)\n",
    "        #print(target_result[0].shape)\n",
    "        #print(source_result[1].shape)\n",
    "        #print(target_result[1].shape)\n",
    "        # 순전파 loss\n",
    "        source_loss, source_reg_loss, source_cla_loss = loss_fn(source_result, source_y, 1, alpha=alpha) # 소스 도메인 레이블 1\n",
    "        target_loss, target_reg_loss, target_cla_loss = loss_fn(target_result, target_y, 0, alpha=alpha) # 타겟 도메인 레이블 0\n",
    "        \n",
    "        loss = source_loss + target_loss\n",
    "        reg_loss = source_reg_loss + target_reg_loss\n",
    "        cla_loss = source_cla_loss + target_cla_loss\n",
    "        \n",
    "        src_reg_total += source_reg_loss.item()\n",
    "        src_cla_total += source_cla_loss.item()\n",
    "        tar_reg_total += target_reg_loss.item()\n",
    "        tar_cal_total += target_cla_loss.item()\n",
    "        \n",
    "        source_total_loss += source_loss.item()\n",
    "        target_total_loss += target_loss.item()\n",
    "        \n",
    "        reg_loss_total += reg_loss.item()\n",
    "        cla_loss_total += cla_loss.item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 그래디언트 계산 및 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping 확인\n",
    "    if reg_loss_total / len(NELEC_train) < best_loss:\n",
    "        best_loss = reg_loss_total / len(NELEC_train)\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        counter = 0  # Counter 초기화\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {i}')\n",
    "            break\n",
    "\n",
    "    print('Epoch : %d, Total Avg Loss : %.4f' % (i, total_loss / len(NELEC_train)))     \n",
    "    print('Source Avg Loss : %.4f' % (source_total_loss / len(NELEC_train)))\n",
    "    print('Target Avg Loss : %.4f' % (target_total_loss / len(NELEC_train)))\n",
    "    print('Source Avg reg Loss : %.4f' % (src_reg_total / len(NELEC_train)))\n",
    "    print('Target Avg reg Loss : %.4f' % (tar_reg_total / len(NELEC_train)))\n",
    "    print('Source Avg cla Loss : %.4f' % (src_cla_total / len(NELEC_train)))\n",
    "    print('Target Avg cla Loss : %.4f' % (tar_cal_total / len(NELEC_train)))\n",
    "    print('Avg Regression Loss : %.4f' % (reg_loss_total / len(NELEC_train)))\n",
    "    print('Avg Classification Loss : %.4f' % (cla_loss_total / len(NELEC_train)))\n",
    "    print('')\n",
    "    \n",
    "    # source_loss가 가장 낮은 경우 모델 가중치 저장\n",
    "    if reg_loss_total / len(NELEC_train) < best_source_loss:\n",
    "        best_source_loss = reg_loss_total / len(NELEC_train)\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "# 최상의 모델을 파일로 저장\n",
    "if best_model_state_dict is not None:\n",
    "    torch.save(best_model_state_dict, '0810_test.pth')\n",
    "    #torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, (512,3,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '0810hoxyhoxy2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 저장된 최상의 소스 모델 불러오기\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_source_model2 \u001b[39m=\u001b[39m dann(my_lstm)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m best_source_model2\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m0810hoxyhoxy2.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m best_source_model2\u001b[39m.\u001b[39meval()  \u001b[39m# 평가 모드로 설정\u001b[39;00m\n\u001b[0;32m      6\u001b[0m s_pred \u001b[39m=\u001b[39m []  \u001b[39m# MNIST 데이터셋의 예측값을 저장할 리스트\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '0810hoxyhoxy2.pth'"
     ]
    }
   ],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('0810_test.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "s_d_pred = []\n",
    "t_d_pred = []\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    source_domain_label_pred, source_pred = best_source_model2(sourcex)\n",
    "    target_domain_label_pred, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "    s_d_pred.extend(source_domain_label_pred.detach().cpu().numpy())\n",
    "    t_d_pred.extend(target_domain_label_pred.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('0810_test.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_test, DHW_test)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    _, source_pred = best_source_model2(sourcex)\n",
    "    _, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Embedding Space 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 앞 batch의 250개씩의 데이터만 샘플링\n",
    "source_tsne = DataLoader(nelec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_tsne = DataLoader(dhw_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_tsne2 = next(iter(source_tsne))\n",
    "target_tsne2 = next(iter(target_tsne))\n",
    "\n",
    "source_x_TNSE = source_tsne2[0].to(device)\n",
    "source_y_TNSE = source_tsne2[1].to(device)\n",
    "\n",
    "target_x_TNSE = target_tsne2[0].to(device)\n",
    "target_y_TNSE = target_tsne2[1].to(device)\n",
    "\n",
    "# 학습된 모델의 LSTM 부분만 활용 (100차원 임베딩 벡터를 받아오는 과정)\n",
    "source_vector = model.lstm(source_x_TNSE)\n",
    "target_vector = model.lstm(target_x_TNSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([source_vector.cpu().detach().numpy(), target_vector.cpu().detach().numpy()]))\n",
    "\n",
    "tsne_np = TSNE(n_components=2).fit_transform(df)\n",
    "tsne_df = pd.DataFrame(tsne_np, columns=['component 0', 'component 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_0 = tsne_df.loc[:1000]\n",
    "tsne_df_1 = tsne_df.loc[1000:]\n",
    "\n",
    "plt.scatter(tsne_df_0['component 0'], tsne_df_0['component 1'], color='green', label='NELEC', alpha=0.5)\n",
    "plt.scatter(tsne_df_1['component 0'], tsne_df_1['component 1'], color='black', label='DHW', alpha=0.5)\n",
    "\n",
    "plt.title('alpha = '+ str(alpha))\n",
    "plt.xlabel('component 0')\n",
    "plt.ylabel('component 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
