{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data as data_util\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec = pd.read_csv('nelec_082323.csv')\n",
    "dhw = pd.read_csv('dhw_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec = nelec.drop(['YEAR'], axis=1)\n",
    "dhw = dhw.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nelec = nelec.iloc[:-1,].reset_index(drop=True)\n",
    "output_nelec = nelec[['n_elec']].iloc[1:].reset_index(drop=True)\n",
    "output_nelec.columns = ['nelec']\n",
    "\n",
    "input_dhw = dhw.iloc[:-1,].reset_index(drop=True)\n",
    "output_dhw = dhw[['DHW']].iloc[1:].reset_index(drop=True)\n",
    "output_dhw.columns = ['dhw']\n",
    "\n",
    "nelec = pd.concat([input_nelec, output_nelec], axis=1)\n",
    "dhw = pd.concat([input_dhw, output_dhw], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_nelec = MinMaxScaler()\n",
    "scaler_dhw = MinMaxScaler()\n",
    "\n",
    "scaler_nelec.fit(nelec)\n",
    "scaler_dhw.fit(dhw)\n",
    "\n",
    "scaled_nelec = scaler_nelec.transform(nelec)\n",
    "scaled_dhw = scaler_dhw.transform(dhw)\n",
    "\n",
    "nelec = pd.DataFrame(scaled_nelec, index=nelec.index, columns=nelec.columns)\n",
    "dhw = pd.DataFrame(scaled_dhw, index=dhw.index, columns=dhw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_nelec = int(len(nelec) * 0.7)\n",
    "standard_dhw = int(len(dhw) * 0.7)\n",
    "\n",
    "nelec_train = nelec.iloc[:standard_nelec]\n",
    "nelec_test = nelec.iloc[standard_nelec:].reset_index(drop=True)\n",
    "\n",
    "dhw_train = dhw.iloc[:standard_dhw]\n",
    "dhw_test = dhw.iloc[standard_dhw:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_trainx = nelec_train.drop(['nelec'], axis=1)\n",
    "nelec_trainy = nelec_train[['nelec']]\n",
    "\n",
    "nelec_testx = nelec_test.drop(['nelec'], axis=1)\n",
    "nelec_testy = nelec_test[['nelec']]\n",
    "\n",
    "dhw_trainx = dhw_train.drop(['dhw'], axis=1)\n",
    "dhw_trainy = dhw_train[['dhw']]\n",
    "\n",
    "dhw_testx = dhw_test.drop(['dhw'], axis=1)\n",
    "dhw_testy = dhw_test[['dhw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)\n",
    "\n",
    "nelec_trainx, nelec_trainy = buildDataSet(nelec_trainx, nelec_trainy, 7)\n",
    "nelec_testx, nelec_testy = buildDataSet(nelec_testx, nelec_testy, 7)\n",
    "\n",
    "dhw_trainx, dhw_trainy = buildDataSet(dhw_trainx, dhw_trainy, 7)\n",
    "dhw_testx, dhw_testy = buildDataSet(dhw_testx, dhw_testy, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nelec_trainx.shape)\n",
    "print(nelec_trainy.shape)\n",
    "print(nelec_testx.shape)\n",
    "print(nelec_testy.shape)\n",
    "print(dhw_trainx.shape)\n",
    "print(dhw_trainy.shape)\n",
    "print(dhw_testx.shape)\n",
    "print(dhw_testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "nelec_trainx_tensor = torch.FloatTensor(nelec_trainx)\n",
    "nelec_trainy_tensor = torch.FloatTensor(nelec_trainy)\n",
    "\n",
    "nelec_testx_tensor = torch.FloatTensor(nelec_testx)\n",
    "nelec_testy_tensor = torch.FloatTensor(nelec_testy)\n",
    "\n",
    "dhw_trainx_tensor = torch.FloatTensor(dhw_trainx)\n",
    "dhw_trainy_tensor = torch.FloatTensor(dhw_trainy)\n",
    "\n",
    "dhw_testx_tensor = torch.FloatTensor(dhw_testx)\n",
    "dhw_testy_tensor = torch.FloatTensor(dhw_testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 형태로 데이터 정의\n",
    "nelec_train = TensorDataset(nelec_trainx_tensor, nelec_trainy_tensor)\n",
    "nelec_test = TensorDataset(nelec_testx_tensor, nelec_testy_tensor)\n",
    "\n",
    "dhw_train = TensorDataset(dhw_trainx_tensor, dhw_trainy_tensor)\n",
    "dhw_test = TensorDataset(dhw_testx_tensor, dhw_testy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_for_nelec_train = 512\n",
    "batch_for_nelec_test = 512\n",
    "batch_for_dhw_train = 512\n",
    "batch_for_dhw_test = 512\n",
    "\n",
    "NELEC_train = DataLoader(nelec_train,\n",
    "                        batch_size=batch_for_nelec_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "NELEC_test = DataLoader(nelec_test,\n",
    "                        batch_size=batch_for_nelec_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "DHW_train = DataLoader(dhw_train,\n",
    "                        batch_size=batch_for_dhw_train,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "DHW_test = DataLoader(dhw_test,\n",
    "                        batch_size=batch_for_dhw_test,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋의 개수를 가정하여 변수에 저장\n",
    "# nelec_train_size = len(nelec_train)\n",
    "# nelec_test_size = len(nelec_test)\n",
    "# dhw_train_size = len(dhw_train)\n",
    "# dhw_test_size = len(dhw_test)\n",
    "\n",
    "# # 각 DataLoader에서 생성되는 배치의 개수 계산\n",
    "# nelec_train_batches = nelec_train_size // batch_for_nelec_train\n",
    "# nelec_test_batches = nelec_test_size // batch_for_nelec_test\n",
    "# dhw_train_batches = dhw_train_size // batch_for_dhw_train\n",
    "# dhw_test_batches = dhw_test_size // batch_for_dhw_test\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"nelec_train batches:\", nelec_train_batches)\n",
    "# print(\"nelec_test batches:\", nelec_test_batches)\n",
    "# print(\"dhw_train batches:\", dhw_train_batches)\n",
    "# print(\"dhw_test batches:\", dhw_test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientReversalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return (grad_output * -1), None\n",
    "\n",
    "class GradientReversalLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(12, 64, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(64, 64, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(64, 64, batch_first=True)\n",
    "        \n",
    "        self.bn7 = nn.BatchNorm1d(7)\n",
    "        self.bn64 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.swish = Swish()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (h0, c0) = self.lstm1(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x, (h1, c1) = self.lstm2(x, (h0, c0))\n",
    "        x = self.bn7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x, _ = self.lstm3(x, (h1, c1))\n",
    "        x = self.bn64(x[:,-1])\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hoxy = lstm()\n",
    "# inputx = torch.randn(512,7,9)\n",
    "# hoxy2 = hoxy(inputx)\n",
    "# print(hoxy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_regression, self).__init__()\n",
    "        self.regression_layer1 = nn.Linear(64, 100)\n",
    "        self.regression_layer2 = nn.Linear(100, 100)\n",
    "        #self.regression_layer3 = nn.Linear(100, 100)\n",
    "        self.regression_layer4 = nn.Linear(100, 1)\n",
    "        self.swish = Swish()\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.regression_layer1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(self.regression_layer2(x))\n",
    "        x = self.relu(x)\n",
    "        #x = self.bn1(self.regression_layer3(x))\n",
    "        #x = self.relu(x)\n",
    "        x = self.regression_layer4(x)#self.relu(self.regression_layer4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_classfication(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classfication, self).__init__()\n",
    "        self.classification_layer0 = GradientReversalLayer()\n",
    "        self.classification_layer1 = nn.Linear(64, 100)\n",
    "        #self.classification_layer2 = nn.Linear(100,100)\n",
    "        #self.classification_layer3 = nn.Linear(100,100)\n",
    "        self.classification_layer4 = nn.Linear(100, 1)\n",
    "        self.swish = Swish()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.classification_layer0(x)\n",
    "        x = self.bn1(self.classification_layer1(x))\n",
    "        x = self.relu(x)\n",
    "        #x = self.bn1(self.classification_layer2(x))\n",
    "        #x = self.relu(x)\n",
    "        #x = self.bn1(self.classification_layer3(x))\n",
    "        #x = self.relu(x)\n",
    "        x = self.classification_layer4(x)#self.sigmoid(self.classification_layer4(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super(dann, self).__init__()\n",
    "        self.lstm = lstm\n",
    "        self.regression = domain_regression()\n",
    "        self.classification = domain_classfication()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feature = self.lstm(x)\n",
    "        reg_output = self.regression(feature)\n",
    "        cla_output = self.classification(feature)\n",
    "        \n",
    "        return reg_output, cla_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dann_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dann_loss, self).__init__()\n",
    "        self.reg = nn.L1Loss()\n",
    "        self.cla = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, result, reg_real, domain_num, alpha=1):\n",
    "        reg_output, cla_output = result\n",
    "        batch_size = reg_output.shape[0]\n",
    "        cla_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        \n",
    "        reg_loss = self.reg(reg_output, reg_real)\n",
    "        cla_loss = self.cla(cla_output, cla_target)\n",
    "        cla_loss2 = cla_loss * alpha\n",
    "        \n",
    "        loss = reg_loss + cla_loss2\n",
    "        \n",
    "        return loss, reg_loss, cla_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lstm = lstm().to(device)\n",
    "model = dann(my_lstm).to(device)\n",
    "loss_fn = dann_loss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "alpha = 1.0\n",
    "epochs = 200\n",
    "\n",
    "model.train() # 훈련 모드 설정\n",
    "\n",
    "optimizer= torch.optim.Adamax(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters = epochs * len(NELEC_train))\n",
    "\n",
    "patience = 100  # Early stopping을 위한 기다리는 최대 epoch 수\n",
    "best_loss = float('inf')\n",
    "best_source_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "counter = 10  # Early stopping 카운터\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    source_total_loss = 0\n",
    "    target_total_loss = 0\n",
    "    src_reg_total = 0\n",
    "    src_cla_total = 0\n",
    "    tar_reg_total = 0\n",
    "    tar_cal_total = 0\n",
    "    reg_loss_total = 0\n",
    "    cla_loss_total = 0\n",
    "    for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "        source_x = source_data[0].to(device)\n",
    "        source_y = source_data[1].to(device)\n",
    "        target_x = target_data[0].to(device)\n",
    "        target_y = target_data[1].to(device)\n",
    "        #print(source_x.shape)\n",
    "        #print(source_y.shape)\n",
    "        #print(target_x.shape)\n",
    "        #print(target_y.shape)\n",
    "        # 순전파\n",
    "        source_result = model(source_x)\n",
    "        target_result = model(target_x)\n",
    "        #print(source_result[0].shape)\n",
    "        #print(target_result[0].shape)\n",
    "        #print(source_result[1].shape)\n",
    "        #print(target_result[1].shape)\n",
    "        # 순전파 loss\n",
    "        source_loss, source_reg_loss, source_cla_loss = loss_fn(source_result, source_y, 1, alpha=alpha) # 소스 도메인 레이블 0\n",
    "        target_loss, target_reg_loss, target_cla_loss = loss_fn(target_result, target_y, 0, alpha=alpha) # 타겟 도메인 레이블 1\n",
    "        \n",
    "        loss = source_loss + target_loss\n",
    "        reg_loss = source_reg_loss + target_reg_loss\n",
    "        cla_loss = source_cla_loss + target_cla_loss\n",
    "        \n",
    "        #src_reg_total += source_reg_loss.item()\n",
    "        #src_cla_total += source_cla_loss.item()\n",
    "        #tar_reg_total += target_reg_loss.item()\n",
    "        #tar_cal_total += target_cla_loss.item()\n",
    "        \n",
    "        #source_total_loss += source_loss.item()\n",
    "        #target_total_loss += target_loss.item()\n",
    "        \n",
    "        reg_loss_total += reg_loss.item()\n",
    "        cla_loss_total += cla_loss.item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 그래디언트 계산 및 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping 확인\n",
    "    if reg_loss_total / len(NELEC_train) < best_loss:\n",
    "        best_loss = reg_loss_total / len(NELEC_train)\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        counter = 0  # Counter 초기화\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {i}')\n",
    "            break\n",
    "\n",
    "    print('Epoch : %d, Total Avg Loss : %.4f' % (i, total_loss / len(NELEC_train)))     \n",
    "    #print('Source Avg Loss : %.4f' % (source_total_loss / len(NELEC_train)))\n",
    "    #print('Target Avg Loss : %.4f' % (target_total_loss / len(NELEC_train)))\n",
    "    #print('Source Avg reg Loss : %.4f' % (src_reg_total / len(NELEC_train)))\n",
    "    #print('Target Avg reg Loss : %.4f' % (tar_reg_total / len(NELEC_train)))\n",
    "    #print('Source Avg cla Loss : %.4f' % (src_cla_total / len(NELEC_train)))\n",
    "    #print('Target Avg cla Loss : %.4f' % (tar_cal_total / len(NELEC_train)))\n",
    "    print('Avg Regression Loss : %.4f' % (reg_loss_total / len(NELEC_train)))\n",
    "    print('Avg Classification Loss : %.4f' % (cla_loss_total / len(NELEC_train)))\n",
    "    print('')\n",
    "    \n",
    "    # source_loss가 가장 낮은 경우 모델 가중치 저장\n",
    "    if reg_loss / len(NELEC_train) < best_source_loss:\n",
    "        best_source_loss = reg_loss / len(NELEC_train)\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "# 최상의 모델을 파일로 저장\n",
    "if best_model_state_dict is not None:\n",
    "    torch.save(best_model_state_dict, '090523_in_the_lab.pth')\n",
    "    #torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model, (512,3,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('090523_in_the_lab.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "s_d_pred = []\n",
    "t_d_pred = []\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    source_domain_label_pred, source_pred = best_source_model2(sourcex)\n",
    "    target_domain_label_pred, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "    s_d_pred.extend(source_domain_label_pred.detach().cpu().numpy())\n",
    "    t_d_pred.extend(target_domain_label_pred.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('090523_in_the_lab.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_test, DHW_test)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    _, source_pred = best_source_model2(sourcex)\n",
    "    _, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Embedding Space 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 앞 batch의 250개씩의 데이터만 샘플링\n",
    "source_tsne = DataLoader(nelec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_tsne = DataLoader(dhw_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_tsne2 = next(iter(source_tsne))\n",
    "target_tsne2 = next(iter(target_tsne))\n",
    "\n",
    "source_x_TNSE = source_tsne2[0].to(device)\n",
    "source_y_TNSE = source_tsne2[1].to(device)\n",
    "\n",
    "target_x_TNSE = target_tsne2[0].to(device)\n",
    "target_y_TNSE = target_tsne2[1].to(device)\n",
    "\n",
    "# 학습된 모델의 LSTM 부분만 활용 (100차원 임베딩 벡터를 받아오는 과정)\n",
    "source_vector = model.lstm(source_x_TNSE)\n",
    "target_vector = model.lstm(target_x_TNSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([source_vector.cpu().detach().numpy(), target_vector.cpu().detach().numpy()]))\n",
    "\n",
    "tsne_np = TSNE(n_components=2).fit_transform(df)\n",
    "tsne_df = pd.DataFrame(tsne_np, columns=['component 0', 'component 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_0 = tsne_df.loc[:1000]\n",
    "tsne_df_1 = tsne_df.loc[1000:]\n",
    "\n",
    "plt.scatter(tsne_df_0['component 0'], tsne_df_0['component 1'], color='green', label='NELEC', alpha=0.5)\n",
    "plt.scatter(tsne_df_1['component 0'], tsne_df_1['component 1'], color='black', label='DHW', alpha=0.5)\n",
    "\n",
    "plt.title('alpha = '+ str(alpha))\n",
    "plt.xlabel('component 0')\n",
    "plt.ylabel('component 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
