{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout, Layer, LSTM, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_merge = pd.read_csv('dhw_merge.csv')\n",
    "elec_merge  = pd.read_csv('elec_merge.csv')\n",
    "n_elec_merge = pd.read_csv('n_elec_merge.csv')\n",
    "\n",
    "dhw_merge = dhw_merge.drop(['YEAR'], axis=1)\n",
    "elec_merge = elec_merge.drop(['YEAR'], axis=1)\n",
    "n_elec_merge = n_elec_merge.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_X = dhw_merge.drop(['DHW'], axis=1)\n",
    "dhw_Y = dhw_merge[['DHW']]\n",
    "\n",
    "scaler_dhw = MinMaxScaler()\n",
    "scaler_dhw.fit(dhw_X)\n",
    "scaled_dhw_X = scaler_dhw.transform(dhw_X)\n",
    "\n",
    "new_dhw_X = pd.DataFrame(scaled_dhw_X, index=dhw_X.index, columns=dhw_X.columns)\n",
    "new_dhw = pd.concat([new_dhw_X, dhw_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_X = elec_merge.drop(['ELEC'], axis=1)\n",
    "elec_Y = elec_merge[['ELEC']]\n",
    "\n",
    "scaler_elec = MinMaxScaler()\n",
    "scaler_elec.fit(elec_X)\n",
    "scaled_elec_X = scaler_elec.transform(elec_X)\n",
    "\n",
    "new_elec_X = pd.DataFrame(scaled_elec_X, index=elec_X.index, columns=elec_X.columns)\n",
    "new_elec = pd.concat([new_elec_X, elec_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elec_X = n_elec_merge.drop(['n_elec'], axis=1)\n",
    "n_elec_Y = n_elec_merge[['n_elec']]\n",
    "\n",
    "scaler_n_elec = MinMaxScaler()\n",
    "scaler_n_elec.fit(n_elec_X)\n",
    "scaled_n_elec_X = scaler_n_elec.transform(n_elec_X)\n",
    "\n",
    "new_n_elec_X = pd.DataFrame(scaled_n_elec_X, index=n_elec_X.index, columns=n_elec_X.columns)\n",
    "new_n_elec = pd.concat([new_n_elec_X, n_elec_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dhw = new_dhw.iloc[:-1,]\n",
    "target_dhw = new_dhw[['DHW']].iloc[1:]\n",
    "\n",
    "trainX_dhw, testX_dhw, trainY_dhw, testY_dhw = train_test_split(input_dhw,target_dhw,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_elec = new_elec.iloc[:-1,]\n",
    "target_elec = new_elec[['ELEC']].iloc[1:]\n",
    "\n",
    "trainX_elec, testX_elec, trainY_elec, testY_elec = train_test_split(input_elec,target_elec,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = new_n_elec.iloc[:-1,]\n",
    "target = new_n_elec[['n_elec']].iloc[1:]\n",
    "\n",
    "trainX_n_elec, testX_n_elec, trainY_n_elec, testY_n_elec = train_test_split(input,target,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_dhw, trainy_dhw = buildDataSet(trainX_dhw, trainY_dhw, 3)\n",
    "testx_dhw, testy_dhw = buildDataSet(testX_dhw, testY_dhw, 3)\n",
    "trainx_elec, trainy_elec = buildDataSet(trainX_elec, trainY_elec, 3)\n",
    "testx_elec, testy_elec = buildDataSet(testX_elec, testY_elec, 3)\n",
    "trainx_n_elec, trainy_n_elec = buildDataSet(trainX_n_elec, trainY_n_elec, 3)\n",
    "testx_n_elec, testy_n_elec = buildDataSet(testX_n_elec, testY_n_elec, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_train_x = scaler_dhw_x_train.inverse_transform(scaled_dhw_train_x)\n",
    "# reverse_train_y = scaler_dhw_y_train.inverse_transform(scaled_dhw_train_y)\n",
    "# reverse_test_x = scaler_dhw_x_train.inverse_transform(scaled_dhw_test_x)\n",
    "# reverse_test_y = scaler_dhw_y_train.inverse_transform(scaled_dhw_test_y)\n",
    "\n",
    "# rev_dhw_train_x = pd.DataFrame(reverse_train_x, index=dhw_x_train.index, columns=dhw_x_train.columns)\n",
    "# rev_dhw_train_y = pd.DataFrame(reverse_train_y, index=dhw_y_train.index, columns=dhw_y_train.columns)\n",
    "\n",
    "# rev_dhw_test_x = pd.DataFrame(reverse_test_x, index=dhw_x_test.index, columns=dhw_x_test.columns)\n",
    "# rev_dhw_test_y = pd.DataFrame(reverse_test_y, index=dhw_y_test.index, columns=dhw_y_test.columns)\n",
    "\n",
    "# rev_dhw_trainSet = pd.concat([rev_dhw_train_x, rev_dhw_train_y], axis=1)\n",
    "# rev_dhw_testSet = pd.concat([rev_dhw_test_x, rev_dhw_test_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "\n",
    "        # Feature Extractor\n",
    "        self.feature_extractor = Sequential([\n",
    "           LSTM(64, activation='swish', return_sequences=True),\n",
    "           LSTM(64, activation='swish', return_sequences=False)\n",
    "        ])\n",
    "        \n",
    "        # Label Predictor\n",
    "        self.label_predictor = Sequential([\n",
    "            Flatten(),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1) # 에너지 소비량을 예측하기 위한 레이어\n",
    "        ])\n",
    "\n",
    "        self.predict_label = Sequential([\n",
    "\t\t\tself.feature_extractor,\n",
    "\t\t\tself.label_predictor\n",
    "\t\t])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        prediction = self.predict_label(inputs)\n",
    "        #prediction = self.label_predictor(lstm_features)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "dann_model = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EarlyStopping' object has no attribute 'best'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m train_loss \u001b[39m=\u001b[39m loss_object(trainy_n_elec, dann_model(trainx_n_elec))\n\u001b[0;32m     38\u001b[0m \u001b[39m# EarlyStopping과 ModelCheckpoint 콜백 적용\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m early_stopping_callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, {\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m: train_loss})\n\u001b[0;32m     40\u001b[0m checkpoint_callback\u001b[39m.\u001b[39mon_epoch_end(epoch, {\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: train_loss})\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m.\u001b[39mnumpy()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\keras\\callbacks.py:1858\u001b[0m, in \u001b[0;36mEarlyStopping.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1855\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_weights()\n\u001b[0;32m   1857\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1858\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_improvement(current, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest):\n\u001b[0;32m   1859\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest \u001b[39m=\u001b[39m current\n\u001b[0;32m   1860\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_epoch \u001b[39m=\u001b[39m epoch\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EarlyStopping' object has no attribute 'best'"
     ]
    }
   ],
   "source": [
    "# EarlyStopping 콜백을 생성합니다.\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',      # 모니터링할 성능 지표 (여기서는 검증 손실)\n",
    "    patience=25,              # 성능이 향상되지 않더라도 몇 번까지 기다릴지 지정\n",
    "    mode='min',              # 성능 지표가 최소화되어야 하는지 또는 최대화되어야 하는지 지정\n",
    "    verbose=0                # 조기 종료가 적용될 때 메시지를 출력 (1: 출력, 0: 출력하지 않음)\n",
    ")\n",
    "\n",
    "# ModelCheckpoint 콜백을 생성합니다.\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_weights.h5',  # 가중치 파일을 저장할 경로와 이름 지정\n",
    "    save_best_only=True,          # 가장 좋은 성능을 보인 에포크의 가중치만 저장\n",
    "    save_weights_only=True,       # 가중치만 저장 (아키텍처는 저장하지 않음)\n",
    "    monitor='loss',           # 모니터링할 성능 지표 (여기서는 검증 손실)\n",
    "    mode='min',                   # 성능 지표가 최소화되어야 하는지 또는 최대화되어야 하는지 지정\n",
    "    verbose=0                     # 저장되는 과정을 출력 (1: 출력, 0: 출력하지 않음)\n",
    ")\n",
    "\n",
    "# EarlyStopping 콜백의 멤버 변수를 초기화합니다.\n",
    "early_stopping_callback.stopped_epoch = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(trainx_n_elec), batch_size):\n",
    "        batch_x = trainx_n_elec[i:i+batch_size]\n",
    "        batch_y = trainy_n_elec[i:i+batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as Tape:\n",
    "            predictions = dann_model(batch_x)\n",
    "            loss = loss_object(batch_y, predictions)\n",
    "            \n",
    "        gradients = Tape.gradient(loss, dann_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, dann_model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    train_loss = loss_object(trainy_n_elec, dann_model(trainx_n_elec))\n",
    " \n",
    "    # EarlyStopping과 ModelCheckpoint 콜백 적용\n",
    "    early_stopping_callback.on_epoch_end(epoch, {'loss': train_loss})\n",
    "    checkpoint_callback.on_epoch_end(epoch, {'loss': train_loss})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss.numpy():.4f}\")\n",
    "    \n",
    "    ## EarlyStopping 조건 충족 시 학습 중지\n",
    "    #if early_stopping_callback.stopped_epoch is not None:\n",
    "    #    print(\"학습 조기 종료\")\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료되면 테스트 데이터에 대한 예측값을 가져옵니다.\n",
    "predicted_values = dann_model.predict(testx_n_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted values against the actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predicted_values, color='blue', label='Predicted')\n",
    "plt.plot(testy_n_elec, color='red', linestyle='--', label='Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
