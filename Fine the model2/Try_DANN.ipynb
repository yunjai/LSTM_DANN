{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout, Layer, LSTM, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_merge = pd.read_csv('dhw_merge.csv')\n",
    "elec_merge  = pd.read_csv('elec_merge.csv')\n",
    "n_elec_merge = pd.read_csv('n_elec_merge.csv')\n",
    "\n",
    "dhw_merge = dhw_merge.drop(['YEAR'], axis=1)\n",
    "elec_merge = elec_merge.drop(['YEAR'], axis=1)\n",
    "n_elec_merge = n_elec_merge.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_X = dhw_merge.drop(['DHW'], axis=1)\n",
    "dhw_Y = dhw_merge[['DHW']]\n",
    "\n",
    "scaler_dhw = MinMaxScaler()\n",
    "scaler_dhw.fit(dhw_X)\n",
    "scaled_dhw_X = scaler_dhw.transform(dhw_X)\n",
    "\n",
    "new_dhw_X = pd.DataFrame(scaled_dhw_X, index=dhw_X.index, columns=dhw_X.columns)\n",
    "new_dhw = pd.concat([new_dhw_X, dhw_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_X = elec_merge.drop(['ELEC'], axis=1)\n",
    "elec_Y = elec_merge[['ELEC']]\n",
    "\n",
    "scaler_elec = MinMaxScaler()\n",
    "scaler_elec.fit(elec_X)\n",
    "scaled_elec_X = scaler_elec.transform(elec_X)\n",
    "\n",
    "new_elec_X = pd.DataFrame(scaled_elec_X, index=elec_X.index, columns=elec_X.columns)\n",
    "new_elec = pd.concat([new_elec_X, elec_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elec_X = n_elec_merge.drop(['n_elec'], axis=1)\n",
    "n_elec_Y = n_elec_merge[['n_elec']]\n",
    "\n",
    "scaler_n_elec = MinMaxScaler()\n",
    "scaler_n_elec.fit(n_elec_X)\n",
    "scaled_n_elec_X = scaler_n_elec.transform(n_elec_X)\n",
    "\n",
    "new_n_elec_X = pd.DataFrame(scaled_n_elec_X, index=n_elec_X.index, columns=n_elec_X.columns)\n",
    "new_n_elec = pd.concat([new_n_elec_X, n_elec_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dhw = new_dhw.iloc[:-1,]\n",
    "target_dhw = new_dhw[['DHW']].iloc[1:]\n",
    "\n",
    "trainX_dhw, testX_dhw, trainY_dhw, testY_dhw = train_test_split(input_dhw,target_dhw,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_elec = new_elec.iloc[:-1,]\n",
    "target_elec = new_elec[['ELEC']].iloc[1:]\n",
    "\n",
    "trainX_elec, testX_elec, trainY_elec, testY_elec = train_test_split(input_elec,target_elec,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = new_n_elec.iloc[:-1,]\n",
    "target = new_n_elec[['n_elec']].iloc[1:]\n",
    "\n",
    "trainX_n_elec, testX_n_elec, trainY_n_elec, testY_n_elec = train_test_split(input,target,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_dhw, trainy_dhw = buildDataSet(trainX_dhw, trainY_dhw, 3)\n",
    "testx_dhw, testy_dhw = buildDataSet(testX_dhw, testY_dhw, 3)\n",
    "trainx_elec, trainy_elec = buildDataSet(trainX_elec, trainY_elec, 3)\n",
    "testx_elec, testy_elec = buildDataSet(testX_elec, testY_elec, 3)\n",
    "trainx_n_elec, trainy_n_elec = buildDataSet(trainX_n_elec, trainY_n_elec, 3)\n",
    "testx_n_elec, testy_n_elec = buildDataSet(testX_n_elec, testY_n_elec, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_train_x = scaler_dhw_x_train.inverse_transform(scaled_dhw_train_x)\n",
    "# reverse_train_y = scaler_dhw_y_train.inverse_transform(scaled_dhw_train_y)\n",
    "# reverse_test_x = scaler_dhw_x_train.inverse_transform(scaled_dhw_test_x)\n",
    "# reverse_test_y = scaler_dhw_y_train.inverse_transform(scaled_dhw_test_y)\n",
    "\n",
    "# rev_dhw_train_x = pd.DataFrame(reverse_train_x, index=dhw_x_train.index, columns=dhw_x_train.columns)\n",
    "# rev_dhw_train_y = pd.DataFrame(reverse_train_y, index=dhw_y_train.index, columns=dhw_y_train.columns)\n",
    "\n",
    "# rev_dhw_test_x = pd.DataFrame(reverse_test_x, index=dhw_x_test.index, columns=dhw_x_test.columns)\n",
    "# rev_dhw_test_y = pd.DataFrame(reverse_test_y, index=dhw_y_test.index, columns=dhw_y_test.columns)\n",
    "\n",
    "# rev_dhw_trainSet = pd.concat([rev_dhw_train_x, rev_dhw_train_y], axis=1)\n",
    "# rev_dhw_testSet = pd.concat([rev_dhw_test_x, rev_dhw_test_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super(DANN, self).__init__()\n",
    "\n",
    "        # Feature Extractor\n",
    "        self.feature_extractor = Sequential([\n",
    "           LSTM(64, activation='swish', return_sequences=True),\n",
    "           LSTM(64, activation='swish', return_sequences=False)\n",
    "        ])\n",
    "        \n",
    "        # Label Predictor\n",
    "        self.label_predictor = Sequential([\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1) # 에너지 소비량을 예측하기 위한 레이어\n",
    "        ])\n",
    "        \n",
    "        # Domain Predictor\n",
    "        self.domain_predictor = Sequential([\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1, activation='softmax') # 도메인 분류를 위한 레이어 (0:Source, 1:Target)\n",
    "        ])\n",
    "        \n",
    "        self.predict_label = Sequential([\n",
    "\t\t\tself.feature_extractor,\n",
    "\t\t\tself.label_predictor\n",
    "\t\t])\n",
    "\n",
    "\t\tself.classify_domain = Sequential([\n",
    "\t\t\tself.feature_extractor,\n",
    "\t\t\tself.domain_classifier\n",
    "\t\t])\n",
    "  \n",
    "    @tf.function\n",
    "\tdef train(self, x_class, y_class, x_domain):\n",
    "\t\t\n",
    "\t\tdomain_labels = np.concatenate([np.zeros(len(x_class)), np.ones(len(x_domain))])\n",
    "\t\t\n",
    "\t\tx_both = tf.concat([x_class, x_domain], axis = 0)\n",
    "\t\t\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\ty_class_pred = self.predict_label(x_class, training=True)\n",
    "\t\t\tlp_loss = self.loss(y_class, y_class_pred)   \n",
    "\t\tlp_grad = tape.gradient(lp_loss, self.predict_label.trainable_variables)\n",
    "\n",
    "\t\twith tf.GradientTape(persistent = True) as tape:\n",
    "\t\t\ty_domain_pred = self.classify_domain(x_both, training=True)\n",
    "\t\t\tdc_loss = self.loss(domain_labels, y_domain_pred) \n",
    "\t\tfe_grad = tape.gradient(dc_loss, self.feature_extractor.trainable_variables)\n",
    "\t\tdc_grad = tape.gradient(dc_loss, self.domain_classifier.trainable_variables)\n",
    "\t\tdel tape\n",
    "\n",
    "\t\tself.lp_optimizer.apply_gradients(zip(lp_grad, self.predict_label.trainable_variables))\n",
    "\t\t# self.dc_optimizer.apply_gradients(zip(dc_grad, self.classify_domain.trainable_variables))\n",
    "\t\tself.dc_optimizer.apply_gradients(zip(dc_grad, self.domain_classifier.trainable_variables))\n",
    "\t\t\n",
    "\t\tself.fe_optimizer.apply_gradients(zip(fe_grad, self.feature_extractor.trainable_variables))\n",
    "\n",
    "\t\tself.train_lp_loss(lp_loss)\n",
    "\t\tself.train_lp_accuracy(y_class, y_class_pred)\n",
    "\t\t\n",
    "\t\tself.train_dc_loss(dc_loss)\n",
    "\t\tself.train_dc_accuracy(domain_labels, y_domain_pred)\n",
    "\n",
    "\t\treturn\n",
    "\n",
    "    def call(self, inputs, train=False, lamda=1.0):\n",
    "        #model_input = self.model_input(inputs)\n",
    "        lstm_features1 = self.lstm_layer1(inputs)\n",
    "        lstm_features2 = self.lstm_layer2(lstm_features1)\n",
    "        #features = self.feature_extractor(lstm_features2)\n",
    "        power_consumption_prediction = self.label_predictor(lstm_features2)\n",
    "        \n",
    "        if train:\n",
    "            reversed_features = GradientReversalLayer(lamda)(lstm_features2)\n",
    "            domain_prediction = self.domain_predictor(reversed_features)\n",
    "            return power_consumption_prediction, domain_prediction\n",
    "        \n",
    "        else:\n",
    "            return power_consumption_prediction\n",
    "################################################        \n",
    "@tf.custom_gradient\n",
    "def GradientReversalOperator(x):\n",
    "\tdef grad(dy):\n",
    "\t\treturn -1 * dy\n",
    "\treturn x, grad\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(GradientReversalLayer, self).__init__()\n",
    "\t\t\n",
    "\tdef call(self, inputs):\n",
    "\t\treturn GradientReversalOperator(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dann_model = DANN()\n",
    "\n",
    "#predictions = dann_model(trainx_n_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dann_model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dann_model.fit(trainx_n_elec, trainy_n_elec, epochs=100, batch_size=1024, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"gradient_reversal_layer_6\" (type GradientReversalLayer).\n\nAttempt to convert a value (<function identity at 0x00000239BFA00A60>) with an unsupported type (<class 'function'>) to a Tensor.\n\nCall arguments received by layer \"gradient_reversal_layer_6\" (type GradientReversalLayer):\n  • inputs=tf.Tensor(shape=(16, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m batch_elec_target \u001b[39m=\u001b[39m trainy_elec[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m     22\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape(persistent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m     23\u001b[0m     \u001b[39m# 소스 도메인 데이터에 대한 레이블 예측 및 도메인 분류\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     n_elec_power_prediction, n_elec_domain_prediction \u001b[39m=\u001b[39m dann_model(batch_n_elec, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, lamda\u001b[39m=\u001b[39;49mlamda)\n\u001b[0;32m     25\u001b[0m     n_elec_power_loss \u001b[39m=\u001b[39m loss_object(batch_n_elec_target, n_elec_power_prediction)\n\u001b[0;32m     26\u001b[0m     n_elec_domain_loss \u001b[39m=\u001b[39m loss_object(tf\u001b[39m.\u001b[39mzeros_like(n_elec_domain_prediction), n_elec_domain_prediction)\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[62], line 33\u001b[0m, in \u001b[0;36mDANN.call\u001b[1;34m(self, inputs, train, lamda)\u001b[0m\n\u001b[0;32m     30\u001b[0m power_consumption_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_predictor(lstm_features2)\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m train:\n\u001b[1;32m---> 33\u001b[0m     reversed_features \u001b[39m=\u001b[39m GradientReversalLayer(lamda)(lstm_features2)\n\u001b[0;32m     34\u001b[0m     domain_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdomain_predictor(reversed_features)\n\u001b[0;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m power_consumption_prediction, domain_prediction\n",
      "Cell \u001b[1;32mIn[62], line 46\u001b[0m, in \u001b[0;36mGradientReversalLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m,inputs):\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m GradientReversalLayerFunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlamda)(inputs)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"gradient_reversal_layer_6\" (type GradientReversalLayer).\n\nAttempt to convert a value (<function identity at 0x00000239BFA00A60>) with an unsupported type (<class 'function'>) to a Tensor.\n\nCall arguments received by layer \"gradient_reversal_layer_6\" (type GradientReversalLayer):\n  • inputs=tf.Tensor(shape=(16, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 및 손실 함수 설정\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# 학습 설정\n",
    "epochs = 100\n",
    "lamda = 1.0\n",
    "batch_size = 16\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    n_elec_loss = 0.0\n",
    "    elec_loss = 0.0\n",
    "    \n",
    "    # 배치별 학습\n",
    "    for i in range(0, len(trainx_n_elec), batch_size):\n",
    "        batch_n_elec = trainx_n_elec[i:i+batch_size]\n",
    "        batch_n_elec_target = trainy_n_elec[i:i+batch_size]\n",
    "        batch_elec = trainx_elec[i:i+batch_size]\n",
    "        batch_elec_target = trainy_elec[i:i+batch_size]\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # 소스 도메인 데이터에 대한 레이블 예측 및 도메인 분류\n",
    "            n_elec_power_prediction, n_elec_domain_prediction = dann_model(batch_n_elec, train=True, lamda=lamda)\n",
    "            n_elec_power_loss = loss_object(batch_n_elec_target, n_elec_power_prediction)\n",
    "            n_elec_domain_loss = loss_object(tf.zeros_like(n_elec_domain_prediction), n_elec_domain_prediction)\n",
    "            total_n_elec_loss = n_elec_power_loss + n_elec_domain_loss\n",
    "            \n",
    "            # 타겟 도메인 데이터에 대한 레이블 예측 및 도메인 분류\n",
    "            elec_power_prediction, elec_domain_prediction = dann_model(batch_elec, train=True, lamda=lamda)\n",
    "            elec_power_loss = loss_object(batch_elec_target, elec_power_prediction)\n",
    "            elec_domain_loss = loss_object(tf.ones_like(elec_domain_prediction), elec_domain_prediction)\n",
    "            total_elec_loss = elec_power_loss + elec_domain_loss\n",
    "        \n",
    "        # 소스 도메인과 타겟 도메인의 합친 손실을 최소화하는 방향으로 가중치 업데이트\n",
    "        total_loss = total_n_elec_loss + total_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 이전의 모델 에측 결과 시각화\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(range(num_sampels), city_a_data, label='City A Ground Truth')\n",
    "plt.sactter(range(num_samples), dann_model(city_a_data), label='City A predction (Before Adaptation)')\n",
    "plt.scatter(range(num_samples), city_b_data, label='City B Ground Truth')\n",
    "plt.sactter(range(num_samples), dann_model(city_b_data), label='City B prediction (Before Adaptation)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Power Consumption')\n",
    "plt.legend()\n",
    "plt.title('Prediction Before Adaptation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 후의 모델 예측 결과 시각화\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(range(num_samples), city_a_data, label='City A Ground Truth')\n",
    "plt.scatter(range(num_samples), dann_model(city_a_data), label='City A predcition (After Adaptation)')\n",
    "plt.scatter(range(num_samples), city_b_data, label='City B Ground Truth')\n",
    "plt.sactter(range(num_samples), dann_model(city_b_data), label='City B prediction (After Adaptation)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Power Consumption')\n",
    "plt.legend()\n",
    "plt.title('Prediction After Adaptation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# print('MAE : ', round(mean_absolute_error(real, pred),4)) \n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# print('MSE : ', round(mean_squared_error(real, pred),4))\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "# print('R2 : ', round(r2_score(real, pred),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
