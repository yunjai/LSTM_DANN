{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout, Layer, LSTM, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_merge = pd.read_csv('dhw_merge.csv')\n",
    "elec_merge  = pd.read_csv('elec_merge.csv')\n",
    "n_elec_merge = pd.read_csv('n_elec_merge.csv')\n",
    "\n",
    "dhw_merge = dhw_merge.drop(['YEAR'], axis=1)\n",
    "elec_merge = elec_merge.drop(['YEAR'], axis=1)\n",
    "n_elec_merge = n_elec_merge.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhw_X = dhw_merge.drop(['DHW'], axis=1)\n",
    "dhw_Y = dhw_merge[['DHW']]\n",
    "\n",
    "scaler_dhw = MinMaxScaler()\n",
    "scaler_dhw.fit(dhw_X)\n",
    "scaled_dhw_X = scaler_dhw.transform(dhw_X)\n",
    "\n",
    "new_dhw_X = pd.DataFrame(scaled_dhw_X, index=dhw_X.index, columns=dhw_X.columns)\n",
    "new_dhw = pd.concat([new_dhw_X, dhw_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_X = elec_merge.drop(['ELEC'], axis=1)\n",
    "elec_Y = elec_merge[['ELEC']]\n",
    "\n",
    "scaler_elec = MinMaxScaler()\n",
    "scaler_elec.fit(elec_X)\n",
    "scaled_elec_X = scaler_elec.transform(elec_X)\n",
    "\n",
    "new_elec_X = pd.DataFrame(scaled_elec_X, index=elec_X.index, columns=elec_X.columns)\n",
    "new_elec = pd.concat([new_elec_X, elec_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elec_X = n_elec_merge.drop(['n_elec'], axis=1)\n",
    "n_elec_Y = n_elec_merge[['n_elec']]\n",
    "\n",
    "scaler_n_elec = MinMaxScaler()\n",
    "scaler_n_elec.fit(n_elec_X)\n",
    "scaled_n_elec_X = scaler_n_elec.transform(n_elec_X)\n",
    "\n",
    "new_n_elec_X = pd.DataFrame(scaled_n_elec_X, index=n_elec_X.index, columns=n_elec_X.columns)\n",
    "new_n_elec = pd.concat([new_n_elec_X, n_elec_Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dhw = new_dhw.iloc[:-1,]\n",
    "target_dhw = new_dhw[['DHW']].iloc[1:]\n",
    "\n",
    "trainX_dhw, testX_dhw, trainY_dhw, testY_dhw = train_test_split(input_dhw,target_dhw,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_elec = new_elec.iloc[:-1,]\n",
    "target_elec = new_elec[['ELEC']].iloc[1:]\n",
    "\n",
    "trainX_elec, testX_elec, trainY_elec, testY_elec = train_test_split(input_elec,target_elec,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = new_n_elec.iloc[:-1,]\n",
    "target = new_n_elec[['n_elec']].iloc[1:]\n",
    "\n",
    "trainX_n_elec, testX_n_elec, trainY_n_elec, testY_n_elec = train_test_split(input,target,test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_dhw, trainy_dhw = buildDataSet(trainX_dhw, trainY_dhw, 3)\n",
    "testx_dhw, testy_dhw = buildDataSet(testX_dhw, testY_dhw, 3)\n",
    "trainx_elec, trainy_elec = buildDataSet(trainX_elec, trainY_elec, 3)\n",
    "testx_elec, testy_elec = buildDataSet(testX_elec, testY_elec, 3)\n",
    "trainx_n_elec, trainy_n_elec = buildDataSet(trainX_n_elec, trainY_n_elec, 3)\n",
    "testx_n_elec, testy_n_elec = buildDataSet(testX_n_elec, testY_n_elec, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_train_x = scaler_dhw_x_train.inverse_transform(scaled_dhw_train_x)\n",
    "# reverse_train_y = scaler_dhw_y_train.inverse_transform(scaled_dhw_train_y)\n",
    "# reverse_test_x = scaler_dhw_x_train.inverse_transform(scaled_dhw_test_x)\n",
    "# reverse_test_y = scaler_dhw_y_train.inverse_transform(scaled_dhw_test_y)\n",
    "\n",
    "# rev_dhw_train_x = pd.DataFrame(reverse_train_x, index=dhw_x_train.index, columns=dhw_x_train.columns)\n",
    "# rev_dhw_train_y = pd.DataFrame(reverse_train_y, index=dhw_y_train.index, columns=dhw_y_train.columns)\n",
    "\n",
    "# rev_dhw_test_x = pd.DataFrame(reverse_test_x, index=dhw_x_test.index, columns=dhw_x_test.columns)\n",
    "# rev_dhw_test_y = pd.DataFrame(reverse_test_y, index=dhw_y_test.index, columns=dhw_y_test.columns)\n",
    "\n",
    "# rev_dhw_trainSet = pd.concat([rev_dhw_train_x, rev_dhw_train_y], axis=1)\n",
    "# rev_dhw_testSet = pd.concat([rev_dhw_test_x, rev_dhw_test_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(Layer):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self._reverse_gradient(inputs)\n",
    "    \n",
    "    def _reverse_gradient(self, x):\n",
    "        return tf.negative(x) * self.alpha\n",
    "\n",
    "class DANN(Model):\n",
    "    def __init__(self, num_features, num_domains, hidden_units=64):\n",
    "        super(DANN, self).__init__()\n",
    "\n",
    "        # Feature Extractor\n",
    "        self.feature_extractor = Sequential([\n",
    "            LSTM(hidden_units, activation='swish', return_sequences=True),\n",
    "            LSTM(hidden_units, activation='swish', return_sequences=False)\n",
    "        ])\n",
    "        \n",
    "        # Domain Classifier\n",
    "        self.domain_classifier = Sequential([\n",
    "            Dense(hidden_units, activation='relu'),\n",
    "            Dense(num_domains)  # Domain classifier predicts the domain label\n",
    "        ])\n",
    "        \n",
    "        # Label Predictor\n",
    "        self.label_predictor = Sequential([\n",
    "            Dense(hidden_units, activation='relu'),\n",
    "            Dense(1)  # For predicting energy consumption (or any other regression task)\n",
    "        ])\n",
    "\n",
    "        self.grl_layer = GradientReversalLayer()\n",
    "\n",
    "    def call(self, inputs, training=False, alpha=1.0):\n",
    "        features = self.feature_extractor(inputs)\n",
    "        \n",
    "        # Label Prediction\n",
    "        label_prediction = self.label_predictor(features)\n",
    "        \n",
    "        if training:\n",
    "            # Domain Prediction (using the gradient reversal layer)\n",
    "            reversed_features = self.grl_layer(features, alpha=alpha)\n",
    "            domain_prediction = self.domain_classifier(reversed_features)\n",
    "        else:\n",
    "            # During evaluation or inference, use the original features for domain prediction\n",
    "            domain_prediction = self.domain_classifier(features)\n",
    "        \n",
    "        return label_prediction, domain_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DANN model\n",
    "num_features = 12  # Number of features in the input data\n",
    "num_domains = 2    # Number of domains (source and target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "dann_model = DANN(num_features=num_features, num_domains=num_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=         MONTH       DAY      HOUR      TEMP        WS        WD       HUM  \\\n",
      "0     0.909091  0.000000  0.000000  0.455497  0.007634  0.000000  0.931034   \n",
      "1     0.909091  0.000000  0.043478  0.462478  0.076336  0.444444  0.954023   \n",
      "2     0.909091  0.000000  0.086957  0.462478  0.038168  0.388889  0.942529   \n",
      "3     0.909091  0.000000  0.130435  0.460733  0.000000  0.000000  0.965517   \n",
      "4     0.909091  0.000000  0.173913  0.464223  0.030534  0.000000  0.954023   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1019  1.000000  0.733333  0.956522  0.354276  0.015267  0.000000  1.000000   \n",
      "1020  1.000000  0.733333  1.000000  0.361257  0.007634  0.000000  1.000000   \n",
      "1021  1.000000  0.766667  0.000000  0.363002  0.053435  0.444444  1.000000   \n",
      "1022  1.000000  0.766667  0.043478  0.366492  0.045802  0.555556  1.000000   \n",
      "1023  1.000000  0.766667  0.086957  0.378709  0.045802  0.638889  1.000000   \n",
      "\n",
      "            AP       SLP  VISIBILITY     GTEMP  n_elec  \n",
      "0     0.710909  0.704174    0.213153  0.286331  6.1139  \n",
      "1     0.707273  0.700544    0.222437  0.290647  5.7231  \n",
      "2     0.698182  0.691470    0.194197  0.290647  5.4961  \n",
      "3     0.694545  0.687840    0.202321  0.290647  5.3911  \n",
      "4     0.689091  0.682396    0.221277  0.296403  5.3408  \n",
      "...        ...       ...         ...       ...     ...  \n",
      "1019  0.672727  0.667877    0.028240  0.172662  6.2382  \n",
      "1020  0.669091  0.664247    0.029787  0.172662  6.2969  \n",
      "1021  0.667273  0.662432    0.027079  0.172662  6.3302  \n",
      "1022  0.650909  0.646098    0.027853  0.174101  6.1260  \n",
      "1023  0.636364  0.631579    0.039458  0.174101  5.8553  \n",
      "\n",
      "[1024 rows x 12 columns]. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential\" (type Sequential).\n\nInput 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1024, 12)\n\nCall arguments received by layer \"sequential\" (type Sequential):\n  • inputs=         MONTH       DAY      HOUR      TEMP        WS        WD       HUM  \\\n0     0.909091  0.000000  0.000000  0.455497  0.007634  0.000000  0.931034   \n1     0.909091  0.000000  0.043478  0.462478  0.076336  0.444444  0.954023   \n2     0.909091  0.000000  0.086957  0.462478  0.038168  0.388889  0.942529   \n3     0.909091  0.000000  0.130435  0.460733  0.000000  0.000000  0.965517   \n4     0.909091  0.000000  0.173913  0.464223  0.030534  0.000000  0.954023   \n...        ...       ...       ...       ...       ...       ...       ...   \n1019  1.000000  0.733333  0.956522  0.354276  0.015267  0.000000  1.000000   \n1020  1.000000  0.733333  1.000000  0.361257  0.007634  0.000000  1.000000   \n1021  1.000000  0.766667  0.000000  0.363002  0.053435  0.444444  1.000000   \n1022  1.000000  0.766667  0.043478  0.366492  0.045802  0.555556  1.000000   \n1023  1.000000  0.766667  0.086957  0.378709  0.045802  0.638889  1.000000   \n\n            AP       SLP  VISIBILITY     GTEMP  n_elec  \n0     0.710909  0.704174    0.213153  0.286331  6.1139  \n1     0.707273  0.700544    0.222437  0.290647  5.7231  \n2     0.698182  0.691470    0.194197  0.290647  5.4961  \n3     0.694545  0.687840    0.202321  0.290647  5.3911  \n4     0.689091  0.682396    0.221277  0.296403  5.3408  \n...        ...       ...         ...       ...     ...  \n1019  0.672727  0.667877    0.028240  0.172662  6.2382  \n1020  0.669091  0.664247    0.029787  0.172662  6.2969  \n1021  0.667273  0.662432    0.027079  0.172662  6.3302  \n1022  0.650909  0.646098    0.027853  0.174101  6.1260  \n1023  0.636364  0.631579    0.039458  0.174101  5.8553  \n\n[1024 rows x 12 columns]\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m batch_y \u001b[39m=\u001b[39m trainY_n_elec[i:i \u001b[39m+\u001b[39m \u001b[39m1024\u001b[39m]\n\u001b[0;32m     36\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> 37\u001b[0m     label_predictions, domain_predictions \u001b[39m=\u001b[39m dann_model(batch_x, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     38\u001b[0m     \u001b[39m# 손실 함수 계산\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     label_loss \u001b[39m=\u001b[39m loss_object(batch_y, label_predictions)\n",
      "File \u001b[1;32mc:\\Users\\znld3\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m, in \u001b[0;36mDANN.call\u001b[1;34m(self, inputs, training, alpha)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, alpha\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(inputs)\n\u001b[0;32m     39\u001b[0m     \u001b[39m# Label Prediction\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     label_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_predictor(features)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential\" (type Sequential).\n\nInput 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1024, 12)\n\nCall arguments received by layer \"sequential\" (type Sequential):\n  • inputs=         MONTH       DAY      HOUR      TEMP        WS        WD       HUM  \\\n0     0.909091  0.000000  0.000000  0.455497  0.007634  0.000000  0.931034   \n1     0.909091  0.000000  0.043478  0.462478  0.076336  0.444444  0.954023   \n2     0.909091  0.000000  0.086957  0.462478  0.038168  0.388889  0.942529   \n3     0.909091  0.000000  0.130435  0.460733  0.000000  0.000000  0.965517   \n4     0.909091  0.000000  0.173913  0.464223  0.030534  0.000000  0.954023   \n...        ...       ...       ...       ...       ...       ...       ...   \n1019  1.000000  0.733333  0.956522  0.354276  0.015267  0.000000  1.000000   \n1020  1.000000  0.733333  1.000000  0.361257  0.007634  0.000000  1.000000   \n1021  1.000000  0.766667  0.000000  0.363002  0.053435  0.444444  1.000000   \n1022  1.000000  0.766667  0.043478  0.366492  0.045802  0.555556  1.000000   \n1023  1.000000  0.766667  0.086957  0.378709  0.045802  0.638889  1.000000   \n\n            AP       SLP  VISIBILITY     GTEMP  n_elec  \n0     0.710909  0.704174    0.213153  0.286331  6.1139  \n1     0.707273  0.700544    0.222437  0.290647  5.7231  \n2     0.698182  0.691470    0.194197  0.290647  5.4961  \n3     0.694545  0.687840    0.202321  0.290647  5.3911  \n4     0.689091  0.682396    0.221277  0.296403  5.3408  \n...        ...       ...         ...       ...     ...  \n1019  0.672727  0.667877    0.028240  0.172662  6.2382  \n1020  0.669091  0.664247    0.029787  0.172662  6.2969  \n1021  0.667273  0.662432    0.027079  0.172662  6.3302  \n1022  0.650909  0.646098    0.027853  0.174101  6.1260  \n1023  0.636364  0.631579    0.039458  0.174101  5.8553  \n\n[1024 rows x 12 columns]\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# 손실 함수 정의 (예시로 평균 제곱 오차 사용)\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# 최적화를 위한 옵티마이저 정의 (예시로 Adam 사용)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# EarlyStopping 콜백 생성\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # 모니터링할 성능 지표 (여기서는 검증 손실)\n",
    "    patience=5,              # 성능이 향상되지 않더라도 몇 번까지 기다릴지 지정\n",
    "    mode='min',              # 성능 지표가 최소화되어야 하는지 지정\n",
    "    verbose=1                # 조기 종료가 적용될 때 메시지를 출력 (1: 출력, 0: 출력하지 않음)\n",
    ")\n",
    "\n",
    "# ModelCheckpoint 콜백 생성\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_weights.h5',  # 가중치 파일을 저장할 경로와 이름 지정\n",
    "    save_best_only=True,          # 가장 좋은 성능을 보인 에포크의 가중치만 저장\n",
    "    save_weights_only=True,       # 가중치만 저장 (아키텍처는 저장하지 않음)\n",
    "    monitor='val_loss',           # 모니터링할 성능 지표 (여기서는 검증 손실)\n",
    "    mode='min',                   # 성능 지표가 최소화되어야 하는지 지정\n",
    "    verbose=1                     # 저장되는 과정을 출력 (1: 출력, 0: 출력하지 않음)\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "alpha = 1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    # 기존에 가지고 있는 3차원 데이터 사용\n",
    "    for i in range(0, len(trainX_n_elec), 1024):\n",
    "        batch_x = trainX_n_elec[i:i + 1024]\n",
    "        batch_y = trainY_n_elec[i:i + 1024]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            label_predictions, domain_predictions = dann_model(batch_x, training=True)\n",
    "            # 손실 함수 계산\n",
    "            label_loss = loss_object(batch_y, label_predictions)\n",
    "            domain_loss = loss_object(1, domain_predictions)  # 도메인 레이블을 구현해야 합니다.\n",
    "            total_loss = label_loss + domain_loss\n",
    "\n",
    "        # 그래디언트 계산 및 가중치 업데이트\n",
    "        gradients = tape.gradient(total_loss, dann_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, dann_model.trainable_variables))\n",
    "\n",
    "    # # 검증 데이터를 사용하여 성능 평가\n",
    "    # val_total_loss = 0.0\n",
    "    # for i in range(0, len(val_data), batch_size):\n",
    "    #     val_batch_x = val_data[i:i + batch_size]\n",
    "    #     val_batch_y = val_labels[i:i + batch_size]\n",
    "\n",
    "    #     label_predictions, domain_predictions = dann_model(val_batch_x, training=False, alpha=alpha)\n",
    "    #     # 검증 결과 평가 (손실 함수 계산)\n",
    "    #     val_label_loss = loss_object(val_batch_y, label_predictions)\n",
    "    #     val_domain_loss = loss_object(도메인 레이블, domain_predictions)  # 도메인 레이블을 구현해야 합니다.\n",
    "    #     val_total_loss += (val_label_loss + val_domain_loss)\n",
    "\n",
    "    # average_val_loss = val_total_loss / num_val_batches\n",
    "    # print(f\"Epoch {epoch + 1}, Validation Loss: {average_val_loss.numpy():.4f}\")\n",
    "\n",
    "    # EarlyStopping 조건 충족 시 학습 중지\n",
    "    if early_stopping_callback.stopped_epoch is not None:\n",
    "        print(\"학습 조기 종료\")\n",
    "        break\n",
    "\n",
    "# 최종 테스트 데이터에 대한 예측\n",
    "predicted_values = dann_model(testx_n_elec, training=False, alpha=alpha)\n",
    "# 예측 결과를 활용하여 분석 등 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 완료되면 테스트 데이터에 대한 예측값을 가져옵니다.\n",
    "predicted_values = dann_model.predict(testx_n_elec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted values against the actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predicted_values, color='blue', label='Predicted')\n",
    "plt.plot(testy_n_elec, color='red', linestyle='--', label='Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
