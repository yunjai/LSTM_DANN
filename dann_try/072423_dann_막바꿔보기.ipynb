{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout, Layer, LSTM, Input\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dann_data = pd.read_csv('dann_nelec_elec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4274 entries, 0 to 4273\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   YEAR        4274 non-null   int64  \n",
      " 1   MONTH       4274 non-null   int64  \n",
      " 2   DAY         4274 non-null   int64  \n",
      " 3   HOUR        4274 non-null   int64  \n",
      " 4   TEMP        4274 non-null   float64\n",
      " 5   WS          4274 non-null   float64\n",
      " 6   WD          4274 non-null   int64  \n",
      " 7   HUM         4274 non-null   float64\n",
      " 8   AP          4274 non-null   float64\n",
      " 9   SLP         4274 non-null   float64\n",
      " 10  VISIBILITY  4274 non-null   int64  \n",
      " 11  GTEMP       4274 non-null   float64\n",
      " 12  N_ELEC      4274 non-null   float64\n",
      " 13  ELEC        4274 non-null   float64\n",
      "dtypes: float64(8), int64(6)\n",
      "memory usage: 467.6 KB\n"
     ]
    }
   ],
   "source": [
    "dann_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dann_data.iloc[:,:-2]\n",
    "Y = dann_data.iloc[:,-2:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "scaled_x = scaler.transform(X)\n",
    "\n",
    "new_x = pd.DataFrame(scaled_x, index=X.index, columns=X.columns)\n",
    "new_dann_data = pd.concat([new_x, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = new_dann_data.iloc[:,:-2]\n",
    "source_nelec = new_dann_data[['N_ELEC']]\n",
    "target_elec = new_dann_data[['ELEC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source vs Target (Train_Test_Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_for_data_input = int(len(data_input)*0.7)\n",
    "data_input_trainX = data_input[:split_for_data_input]\n",
    "data_input_testX = data_input[split_for_data_input:]\n",
    "\n",
    "split_for_source_nelec = int(len(source_nelec)*0.7)\n",
    "source_nelec_trainX = source_nelec[:split_for_source_nelec]\n",
    "source_nelec_testX = source_nelec[split_for_source_nelec:]\n",
    "\n",
    "split_for_target_elec = int(len(target_elec)*0.7)\n",
    "target_elec_trainX = target_elec[:split_for_target_elec]\n",
    "target_elec_testX = target_elec[split_for_target_elec:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Rows, Window_Size, Column) 3차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_trainx, source_nelec_trainx = buildDataSet(data_input_trainX, source_nelec_trainX, 3)\n",
    "data_input_testx, source_nelec_testx = buildDataSet(data_input_testX, source_nelec_testX, 3)\n",
    "\n",
    "data_input_trainx, target_elec_trainx = buildDataSet(data_input_trainX, target_elec_trainX, 3)\n",
    "data_input_testx, target_elec_testx = buildDataSet(data_input_testX, target_elec_testX, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 3, 12)\n",
      "(1280, 3, 12)\n",
      "(2988, 1)\n",
      "(1280, 1)\n",
      "(2988, 1)\n",
      "(1280, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_input_trainx.shape)\n",
    "print(data_input_testx.shape)\n",
    "print(source_nelec_trainx.shape)\n",
    "print(source_nelec_testx.shape)\n",
    "print(target_elec_trainx.shape)\n",
    "print(target_elec_testx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Datasets\n",
    "BATCH_SIZE = 32\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((data_input_trainx, source_nelec_trainx)).batch(BATCH_SIZE*2, drop_remainder=True)\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((data_input_trainx, source_nelec_trainx, data_input_trainx, target_elec_trainx)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((data_input_testx, target_elec_testx)).batch(BATCH_SIZE*2, drop_remainder=True) #Test Dataset over Target Domain\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((data_input_trainx, target_elec_trainx)).batch(BATCH_SIZE*2, drop_remainder=True) #Test Dataset over Target (used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋의 요소를 numpy 배열로 변환하여 shape 확인\n",
    "# for data_input, source_nelec in source_dataset:\n",
    "#     print(\"data_input shape:\", data_input[0])\n",
    "#     print(\"source_nelec shape:\", source_nelec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)\n",
    "    \n",
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature Extractor\n",
    "        self.feature_extractor_layer0 = LSTM(64, activation='swish', return_sequences=True)\n",
    "        self.feature_extractor_layer1 = Dropout(0.3)\n",
    "        self.feature_extractor_layer2 = LSTM(64, activation='swish', return_sequences=False)\n",
    "        \n",
    "        # Label regression\n",
    "        self.label_predcitor_layer0 = Dense(64, activation='relu')\n",
    "        self.label_predcitor_layer1 = Dense(1)\n",
    "        \n",
    "        # Domain Predictor\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(64, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2)\n",
    "        \n",
    "    def call(self, x,train=False, source_train=True, lamda=1.0):\n",
    "        # Featrue Extractor\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        x = self.feature_extractor_layer1(x, training=train)\n",
    "        feature = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        #feature = tf.reshape(x, [x.shape[0], -1]) ## shape 2차원으로 바꾸는 거\n",
    "        \n",
    "        # Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "            \n",
    "        lp_x = self.label_predcitor_layer0(feature_slice)\n",
    "        l_logits = self.label_predcitor_layer1(lp_x)\n",
    "\n",
    "        # Domain Predictor\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda) #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            return l_logits, d_logits\n",
    "\n",
    "model = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_func(input_logits, target_labels):\n",
    "#     return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=input_logits, labels=target_labels))\n",
    "\n",
    "# def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "#     if d_logits is None:\n",
    "#         return loss_func(l_logits, labels)\n",
    "#     else:\n",
    "#         return loss_func(l_logits, labels) + loss_func(d_logits, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mae_loss(input_logits, target_labels):\n",
    "    mae_loss = tf.reduce_mean(tf.keras.losses.MeanAbsoluteError(input_logits, input_logits))\n",
    "    return mae_loss\n",
    "\n",
    "def domain_accucary(d_logits, domain):\n",
    "    domain_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=d_logits, labels=domain))\n",
    "    return domain_loss\n",
    "\n",
    "def get_loss(l_logits, labels, d_logits=None, domain=None):\n",
    "    if d_logits is None:\n",
    "        return label_mae_loss(l_logits, labels)\n",
    "    else:\n",
    "        return domain_accucary(d_logits, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accuracy1 = tf.keras.metrics.MeanAbsoluteError()\n",
    "epoch_accuracy2 = tf.keras.metrics.CategoricalAccuracy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_source(s_images, s_labels, lamda=1.0):\n",
    "    images = s_images\n",
    "    labels = s_labels\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output, labels)\n",
    "        epoch_accuracy1(output, labels)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_da(s_images, s_labels, t_images=None, t_labels=None, lamda=1.0):\n",
    "    images = tf.concat([s_images, t_images], 0)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, train=True, source_train=False, lamda=lamda)\n",
    "        l_logits, d_logits = output  # Output from the label predictor and domain predictor\n",
    "\n",
    "        model_loss = get_loss(l_logits, s_labels, d_logits, domain_labels)\n",
    "        epoch_accuracy2(l_logits, s_labels)\n",
    "          \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(t_images, t_labels):\n",
    "    images = t_images\n",
    "    labels = t_labels\n",
    "    \n",
    "    output = model(images, train=False, source_train=True)\n",
    "    epoch_accuracy1(output, labels)\n",
    "\n",
    "\n",
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        p = float(epoch) / epochs\n",
    "        lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "        lamda = lamda.astype('float32')\n",
    "\n",
    "        for batch in dataset:\n",
    "            train_func(*batch, lamda=lamda)\n",
    "        \n",
    "        print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_accuracy1.result()), end='  |  ')\n",
    "        acc_list.append(epoch_accuracy1.result())\n",
    "        test()\n",
    "        epoch_accuracy1.reset_states()\n",
    "        epoch_accuracy2.reset_states()\n",
    "\n",
    "def test():\n",
    "    epoch_accuracy1.reset_states()\n",
    "    epoch_accuracy2.reset_states()\n",
    "    #Testing Dataset (Target Domain)\n",
    "    for batch in test_dataset:\n",
    "        test_step(*batch)\n",
    "        \n",
    "    print(\"Testing Accuracy : {:.3%}\".format(epoch_accuracy1.result()), end='  |  ')\n",
    "    test_acc.append(epoch_accuracy1.result())\n",
    "    epoch_accuracy1.reset_states()\n",
    "    \n",
    "    #Target Domain (used for Training)\n",
    "    for batch in test_dataset2:\n",
    "        test_step(*batch)\n",
    "    \n",
    "    print(\"Target Domain Accuracy : {:.3%}\".format(epoch_accuracy2.result()))\n",
    "    test2_acc.append(epoch_accuracy2.result())\n",
    "    epoch_accuracy1.reset_states()\n",
    "    epoch_accuracy2.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dann/dense/kernel:0', 'dann/dense/bias:0', 'dann/dense_1/kernel:0', 'dann/dense_1/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dann/dense/kernel:0', 'dann/dense/bias:0', 'dann/dense_1/kernel:0', 'dann/dense_1/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Training: Epoch 0 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 73.076%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 1 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 72.573%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 2 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 71.219%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 3 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 70.672%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 4 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 70.842%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 5 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 70.236%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 6 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 70.695%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 7 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 70.686%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 8 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 70.572%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 9 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 68.130%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 10 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 36.179%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 11 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 34270.563%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 12 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 168294.043%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 13 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 360773.706%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 14 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 537116.455%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 15 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 996593.457%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 16 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 960216.406%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 17 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 1257195.605%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 18 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 2114022.266%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 19 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 4118416.797%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 20 :\t Source Accuracy : 0.000%  |  Testing Accuracy : 5424046.875%  |  Target Domain Accuracy : 0.000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#train('source', 100)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train(\u001b[39m'\u001b[39;49m\u001b[39mdomain-adaptation\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[19], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_mode, epochs)\u001b[0m\n\u001b[0;32m     56\u001b[0m lamda \u001b[39m=\u001b[39m lamda\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m---> 59\u001b[0m     train_func(\u001b[39m*\u001b[39;49mbatch, lamda\u001b[39m=\u001b[39;49mlamda)\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining: Epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m :\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Source Accuracy : \u001b[39m\u001b[39m{:.3%}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, epoch_accuracy1\u001b[39m.\u001b[39mresult()), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m  |  \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m acc_list\u001b[39m.\u001b[39mappend(epoch_accuracy1\u001b[39m.\u001b[39mresult())\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training\n",
    "#train('source', 100)\n",
    "train('domain-adaptation', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot Results\n",
    "# x_axis = [i for i in range(0, EPOCH)]\n",
    "\n",
    "# plt.plot(x_axis, da_acc, label=\"source accuracy\")\n",
    "# plt.plot(x_axis, test_acc, label=\"testing accuracy\")\n",
    "# plt.plot(x_axis, test2_acc, label=\"target accuracy\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
