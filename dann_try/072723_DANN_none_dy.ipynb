{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout, Layer, LSTM, Input\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dann_data = pd.read_csv('dann_nelec_elec_weired.csv')\n",
    "dann_data = pd.read_csv('dann_nelec_elec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4274 entries, 0 to 4273\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   YEAR        4274 non-null   int64  \n",
      " 1   MONTH       4274 non-null   int64  \n",
      " 2   DAY         4274 non-null   int64  \n",
      " 3   HOUR        4274 non-null   int64  \n",
      " 4   TEMP        4274 non-null   float64\n",
      " 5   WS          4274 non-null   float64\n",
      " 6   WD          4274 non-null   int64  \n",
      " 7   HUM         4274 non-null   float64\n",
      " 8   AP          4274 non-null   float64\n",
      " 9   SLP         4274 non-null   float64\n",
      " 10  VISIBILITY  4274 non-null   int64  \n",
      " 11  GTEMP       4274 non-null   float64\n",
      " 12  N_ELEC      4274 non-null   float64\n",
      " 13  ELEC        4274 non-null   float64\n",
      "dtypes: float64(8), int64(6)\n",
      "memory usage: 467.6 KB\n"
     ]
    }
   ],
   "source": [
    "dann_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dann_data.iloc[:,:-2]\n",
    "Y = dann_data.iloc[:,-2:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "scaled_x = scaler.transform(X)\n",
    "\n",
    "new_x = pd.DataFrame(scaled_x, index=X.index, columns=X.columns)\n",
    "new_dann_data = pd.concat([new_x, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = new_dann_data.iloc[:,:-2]\n",
    "source_nelec = new_dann_data[['N_ELEC']]\n",
    "target_elec = new_dann_data[['ELEC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source vs Target (Train_Test_Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_for_data_input = int(len(data_input)*0.7)\n",
    "data_input_trainX = data_input[:split_for_data_input]\n",
    "data_input_testX = data_input[split_for_data_input:]\n",
    "\n",
    "split_for_source_nelec = int(len(source_nelec)*0.7)\n",
    "source_nelec_trainX = source_nelec[:split_for_source_nelec]\n",
    "source_nelec_testX = source_nelec[split_for_source_nelec:]\n",
    "\n",
    "split_for_target_elec = int(len(target_elec)*0.7)\n",
    "target_elec_trainX = target_elec[:split_for_target_elec]\n",
    "target_elec_testX = target_elec[split_for_target_elec:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Rows, Window_Size, Column) 3차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_trainx, source_nelec_trainx = buildDataSet(data_input_trainX, source_nelec_trainX, 3)\n",
    "data_input_testx, source_nelec_testx = buildDataSet(data_input_testX, source_nelec_testX, 3)\n",
    "\n",
    "data_input_trainx, target_elec_trainx = buildDataSet(data_input_trainX, target_elec_trainX, 3)\n",
    "data_input_testx, target_elec_testx = buildDataSet(data_input_testX, target_elec_testX, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988, 3, 12)\n",
      "(1280, 3, 12)\n",
      "(2988, 1)\n",
      "(1280, 1)\n",
      "(2988, 1)\n",
      "(1280, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_input_trainx.shape)\n",
    "print(data_input_testx.shape)\n",
    "print(source_nelec_trainx.shape)\n",
    "print(source_nelec_testx.shape)\n",
    "print(target_elec_trainx.shape)\n",
    "print(target_elec_testx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Datasets\n",
    "BATCH_SIZE = 32\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((data_input_trainx, source_nelec_trainx)).batch(BATCH_SIZE*2, drop_remainder=True)\n",
    "#source_testset = tf.data.Dataset.from_tensor_slices((data_input_testx, source_nelec_testx)).batch(BATCH_SIZE*2, drop_remainder=True)\n",
    "\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((data_input_trainx, source_nelec_trainx, data_input_trainx, target_elec_trainx)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((data_input_testx, target_elec_testx)).batch(BATCH_SIZE*2, drop_remainder=True) #Test Dataset over Target Domain\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((data_input_trainx, target_elec_trainx)).batch(BATCH_SIZE*2, drop_remainder=True) #Test Dataset over Target (used for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)\n",
    "    \n",
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature Extractor\n",
    "        self.feature_extractor_layer0 = LSTM(64, activation='swish', return_sequences=True)\n",
    "        self.feature_extractor_layer1 = Dropout(0.5)\n",
    "        self.feature_extractor_layer2 = LSTM(64, activation='swish', return_sequences=False)\n",
    "        \n",
    "        # Label regression\n",
    "        self.label_predcitor_layer0 = Dense(64, activation='relu')\n",
    "        self.label_predcitor_layer1 = Dense(1)\n",
    "        \n",
    "        # Domain Predictor\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(128, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(64, activation='relu')\n",
    "        self.domain_predictor_layer3 = Dense(32, activation='relu')\n",
    "        self.domain_predictor_layer4 = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x,train=False, source_train=True, lamda=1.0):\n",
    "        # Featrue Extractor\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        x = self.feature_extractor_layer1(x, training=train)\n",
    "        feature = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        #feature = tf.reshape(x, [x.shape[0], -1]) ## shape 2차원으로 바꾸는 거\n",
    "        \n",
    "        # Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "            \n",
    "        lp_x = self.label_predcitor_layer0(feature_slice)\n",
    "        l_logits = self.label_predcitor_layer1(lp_x)\n",
    "\n",
    "        # Domain Predictor\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda) #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            dp_x = self.domain_predictor_layer2(dp_x)\n",
    "            dp_x = self.domain_predictor_layer3(dp_x)\n",
    "            d_logits = self.domain_predictor_layer4(dp_x)\n",
    "            return l_logits, d_logits\n",
    "\n",
    "model = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mae_loss(true_consumption, pred_consumption):\n",
    "    #mae_loss = tf.reduce_mean(tf.keras.losses.MAE(true_consumption, pred_consumption))\n",
    "    return tf.reduce_mean(tf.keras.losses.MAE(true_consumption, pred_consumption))\n",
    "\n",
    "def domain_accucary(pred_domain, true_domain):\n",
    "    #domain_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_domain, labels=true_domain))\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_domain, labels=true_domain))\n",
    "\n",
    "def get_loss(true_consumption, pred_consumption, pred_domain=None, true_domain=None):\n",
    "    if pred_domain is None:\n",
    "        return label_mae_loss(true_consumption, pred_consumption)\n",
    "    else:\n",
    "        return  label_mae_loss(true_consumption, pred_consumption)*0.2 + domain_accucary(pred_domain, true_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg_optimizer = tf.optimizers.Adamax(learning_rate=3e-4)\n",
    "model_cla_optimizer = tf.optimizers.Adam(learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_labels = np.vstack([np.tile([1.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_reg = tf.keras.metrics.MeanAbsoluteError()\n",
    "epoch_cla = tf.keras.metrics.BinaryAccuracy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy\n",
    "EPOCH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = []\n",
    "\n",
    "@tf.function\n",
    "def train_step_source(source_x, source_y, lamda=1.0):\n",
    "    x = source_x\n",
    "    y = source_y\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(x, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output, y)\n",
    "        epoch_reg(output, y)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_reg_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def train_step_da(source_x, source_y, target_x, target_y=None, lamda=1.0):\n",
    "    cross_domain_x = tf.concat([source_x, target_x], 0)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(cross_domain_x, train=True, source_train=False, lamda=lamda)\n",
    "        l_logits, d_logits = output  # Output from the label predictor and domain predictor\n",
    "        model_loss = get_loss(source_y, l_logits, d_logits, domain_labels)\n",
    "        epoch_cla(d_logits, domain_labels)\n",
    "\n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_cla_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(target_x, target_y):\n",
    "    x = target_x\n",
    "    y = target_y\n",
    "    \n",
    "    output = model(x, train=False, source_train=True)\n",
    "    epoch_reg(output, y)\n",
    "    asdf.append(output)\n",
    "    \n",
    "def train(train_mode, epochs=EPOCH):\n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            p = float(epoch) / epochs\n",
    "            lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "            lamda = lamda.astype('float32')\n",
    "\n",
    "            for batch in dataset:\n",
    "                train_func(*batch, lamda=lamda)\n",
    "\n",
    "            print(\"Training: Epoch {} :\\t Source MAE : {:.3}\".format(epoch, epoch_reg.result()), end='  |  ')\n",
    "            acc_list.append(epoch_reg.result())\n",
    "            test(train_mode)\n",
    "            epoch_reg.reset_states()\n",
    "        \n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            p = float(epoch) / epochs\n",
    "            lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "            lamda = lamda.astype('float32')\n",
    "            \n",
    "            for batch in dataset:\n",
    "                train_func(*batch, lamda=lamda)\n",
    "\n",
    "            print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_cla.result()), end='  |  ')\n",
    "            acc_list.append(epoch_cla.result())\n",
    "            test(train_mode)\n",
    "            epoch_cla.reset_states()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "\n",
    "def test(train_mode):\n",
    "    epoch_reg.reset_states()\n",
    "    epoch_cla.reset_states()\n",
    "    #Testing Dataset (Target Domain)\n",
    "    if train_mode == 'source':\n",
    "        for batch in test_dataset:\n",
    "            test_step(*batch)\n",
    "\n",
    "        print(\"Testing MAE : {:.3}\".format(epoch_reg.result()))\n",
    "        test_acc.append(epoch_reg.result())\n",
    "        epoch_reg.reset_states()\n",
    "\n",
    "    #Target Domain (used for Training)\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        for batch in test_dataset2:\n",
    "            test_step(*batch)\n",
    "\n",
    "        print(\"Target Domain Accuracy : {:.3%}\".format(epoch_cla.result()))\n",
    "        test2_acc.append(epoch_cla.result())\n",
    "        epoch_cla.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 :\t Source MAE : 6.37  |  Testing MAE : 0.606\n",
      "Training: Epoch 1 :\t Source MAE : 6.21  |  Testing MAE : 0.417\n",
      "Training: Epoch 2 :\t Source MAE : 5.95  |  Testing MAE : 0.196\n",
      "Training: Epoch 3 :\t Source MAE : 5.45  |  Testing MAE : 0.627\n",
      "Training: Epoch 4 :\t Source MAE : 4.37  |  Testing MAE : 2.18\n",
      "Training: Epoch 5 :\t Source MAE : 2.0  |  Testing MAE : 5.84\n",
      "Training: Epoch 6 :\t Source MAE : 1.65  |  Testing MAE : 5.26\n",
      "Training: Epoch 7 :\t Source MAE : 1.46  |  Testing MAE : 5.43\n",
      "Training: Epoch 8 :\t Source MAE : 1.4  |  Testing MAE : 5.42\n",
      "Training: Epoch 9 :\t Source MAE : 1.35  |  Testing MAE : 5.53\n",
      "Training: Epoch 10 :\t Source MAE : 1.32  |  Testing MAE : 5.61\n",
      "Training: Epoch 11 :\t Source MAE : 1.28  |  Testing MAE : 5.75\n",
      "Training: Epoch 12 :\t Source MAE : 1.27  |  Testing MAE : 5.68\n",
      "Training: Epoch 13 :\t Source MAE : 1.23  |  Testing MAE : 5.74\n",
      "Training: Epoch 14 :\t Source MAE : 1.2  |  Testing MAE : 5.79\n",
      "Training: Epoch 15 :\t Source MAE : 1.17  |  Testing MAE : 5.78\n",
      "Training: Epoch 16 :\t Source MAE : 1.13  |  Testing MAE : 5.78\n",
      "Training: Epoch 17 :\t Source MAE : 1.1  |  Testing MAE : 5.92\n",
      "Training: Epoch 18 :\t Source MAE : 1.1  |  Testing MAE : 5.8\n",
      "Training: Epoch 19 :\t Source MAE : 1.05  |  Testing MAE : 5.85\n"
     ]
    }
   ],
   "source": [
    "# #Training\n",
    "train('source', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 0 :\t Source Accuracy : 0.000%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 1 :\t Source Accuracy : 0.000%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 2 :\t Source Accuracy : 0.000%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 3 :\t Source Accuracy : 0.000%  |  Target Domain Accuracy : 0.000%\n",
      "Training: Epoch 4 :\t Source Accuracy : 0.000%  |  Target Domain Accuracy : 0.000%\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "train('domain-adaptation', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dann/dense_1/BiasAdd:0' shape=(64, 1) dtype=float32>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic tf.Tensor (dann/dense_1/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49marray(asdf)\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:926\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    925\u001b[0m   \u001b[39mdel\u001b[39;00m dtype\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    927\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot convert a symbolic tf.Tensor (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m) to a numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    928\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m This error may indicate that you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre trying to pass a Tensor to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    929\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m a NumPy call, which is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot convert a symbolic tf.Tensor (dann/dense_1/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported."
     ]
    }
   ],
   "source": [
    "# np.array(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot Results\n",
    "# x_axis = [i for i in range(0, 20)]\n",
    "\n",
    "# plt.plot(x_axis, da_acc, label=\"source accuracy\")\n",
    "# plt.plot(x_axis, test_acc, label=\"testing accuracy\")\n",
    "# plt.plot(x_axis, test2_acc, label=\"target accuracy\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
