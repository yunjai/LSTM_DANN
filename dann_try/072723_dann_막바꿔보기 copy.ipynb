{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D, BatchNormalization, Dropout, Layer, LSTM, Input\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dann_data = pd.read_csv('dann_nelec_elec_weired.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dann_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  For Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dann_data.iloc[:,:-2]\n",
    "Y = dann_data.iloc[:,-2:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "scaled_x = scaler.transform(X)\n",
    "\n",
    "new_x = pd.DataFrame(scaled_x, index=X.index, columns=X.columns)\n",
    "new_dann_data = pd.concat([new_x, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = new_dann_data.iloc[:,:-2]\n",
    "source_nelec = new_dann_data[['N_ELEC']]\n",
    "target_elec = new_dann_data[['ELEC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source vs Target (Train_Test_Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_for_data_input = int(len(data_input)*0.7)\n",
    "data_input_trainX = data_input[:split_for_data_input]\n",
    "data_input_testX = data_input[split_for_data_input:]\n",
    "\n",
    "split_for_source_nelec = int(len(source_nelec)*0.7)\n",
    "source_nelec_trainX = source_nelec[:split_for_source_nelec]\n",
    "source_nelec_testX = source_nelec[split_for_source_nelec:]\n",
    "\n",
    "split_for_target_elec = int(len(target_elec)*0.7)\n",
    "target_elec_trainX = target_elec[:split_for_target_elec]\n",
    "target_elec_testX = target_elec[split_for_target_elec:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Rows, Window_Size, Column) 3차원으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_trainx, source_nelec_trainx = buildDataSet(data_input_trainX, source_nelec_trainX, 3)\n",
    "data_input_testx, source_nelec_testx = buildDataSet(data_input_testX, source_nelec_testX, 3)\n",
    "\n",
    "data_input_trainx, target_elec_trainx = buildDataSet(data_input_trainX, target_elec_trainX, 3)\n",
    "data_input_testx, target_elec_testx = buildDataSet(data_input_testX, target_elec_testX, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_input_trainx.shape)\n",
    "print(data_input_testx.shape)\n",
    "print(source_nelec_trainx.shape)\n",
    "print(source_nelec_testx.shape)\n",
    "print(target_elec_trainx.shape)\n",
    "print(target_elec_testx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Datasets\n",
    "BATCH_SIZE = 32\n",
    "source_dataset = tf.data.Dataset.from_tensor_slices((data_input_trainx, source_nelec_trainx)).batch(BATCH_SIZE*2, drop_remainder=True)\n",
    "#source_testset = tf.data.Dataset.from_tensor_slices((data_input_testx, source_nelec_testx)).batch(BATCH_SIZE*2, drop_remainder=True)\n",
    "da_dataset = tf.data.Dataset.from_tensor_slices((data_input_trainx, source_nelec_trainx, data_input_trainx, target_elec_trainx)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((data_input_testx, target_elec_testx)).batch(BATCH_SIZE*2, drop_remainder=True) #Test Dataset over Target Domain\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((data_input_trainx, target_elec_trainx)).batch(BATCH_SIZE*2, drop_remainder=True) #Test Dataset over Target (used for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def gradient_reverse(x, lamda=1.0):\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return lamda * -dy, None\n",
    "    \n",
    "    return y, grad\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self, x, lamda=1.0):\n",
    "        return gradient_reverse(x, lamda)\n",
    "    \n",
    "class DANN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature Extractor\n",
    "        self.feature_extractor_layer0 = LSTM(64, activation='swish', return_sequences=True)\n",
    "        self.feature_extractor_layer1 = Dropout(0.2)\n",
    "        self.feature_extractor_layer2 = LSTM(64, activation='swish', return_sequences=False)\n",
    "        \n",
    "        # Label regression\n",
    "        self.label_predcitor_layer0 = Dense(64, activation='relu')\n",
    "        self.label_predcitor_layer1 = Dense(1)\n",
    "        \n",
    "        # Domain Predictor\n",
    "        self.domain_predictor_layer0 = GradientReversalLayer()\n",
    "        self.domain_predictor_layer1 = Dense(64, activation='relu')\n",
    "        self.domain_predictor_layer2 = Dense(2)\n",
    "        \n",
    "    def call(self, x,train=False, source_train=True, lamda=1.0):\n",
    "        # Featrue Extractor\n",
    "        x = self.feature_extractor_layer0(x)\n",
    "        x = self.feature_extractor_layer1(x, training=train)\n",
    "        feature = self.feature_extractor_layer2(x)\n",
    "        \n",
    "        #feature = tf.reshape(x, [x.shape[0], -1]) ## shape 2차원으로 바꾸는 거\n",
    "        \n",
    "        # Label Predictor\n",
    "        if source_train is True:\n",
    "            feature_slice = feature\n",
    "        else:\n",
    "            feature_slice = tf.slice(feature, [0, 0], [feature.shape[0] // 2, -1])\n",
    "            \n",
    "        lp_x = self.label_predcitor_layer0(feature_slice)\n",
    "        l_logits = self.label_predcitor_layer1(lp_x)\n",
    "\n",
    "        # Domain Predictor\n",
    "        if source_train is True:\n",
    "            return l_logits\n",
    "        else:\n",
    "            dp_x = self.domain_predictor_layer0(feature, lamda) #GradientReversalLayer\n",
    "            dp_x = self.domain_predictor_layer1(dp_x)\n",
    "            d_logits = self.domain_predictor_layer2(dp_x)\n",
    "            return l_logits, d_logits\n",
    "\n",
    "model = DANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_mae_loss(true_consumption, pred_consumption):\n",
    "    #mae_loss = tf.reduce_mean(tf.keras.losses.MAE(true_consumption, pred_consumption))\n",
    "    return tf.reduce_mean(tf.keras.losses.MAE(true_consumption, pred_consumption))\n",
    "\n",
    "def domain_accucary(pred_domain, true_domain):\n",
    "    #domain_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_domain, labels=true_domain))\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_domain, labels=true_domain))\n",
    "\n",
    "def get_loss(true_consumption, pred_consumption, pred_domain=None, true_domain=None):\n",
    "    if pred_domain is None:\n",
    "        return label_mae_loss(true_consumption, pred_consumption)\n",
    "    else:\n",
    "        return  label_mae_loss(true_consumption, pred_consumption) + domain_accucary(pred_domain, true_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg_optimizer = tf.optimizers.Adamax(learning_rate=3e-4)\n",
    "model_cla_optimizer = tf.optimizers.Adam(learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_labels = np.vstack([np.tile([1., 0.], [BATCH_SIZE, 1]),\n",
    "                           np.tile([0., 1.], [BATCH_SIZE, 1])])\n",
    "domain_labels = domain_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_reg = tf.keras.metrics.MeanAbsoluteError()\n",
    "epoch_cla = tf.keras.metrics.BinaryAccuracy()\n",
    "source_acc = []  # Source Domain Accuracy while Source-only Training\n",
    "da_acc = []      # Source Domain Accuracy while DA-training\n",
    "test_acc = []    # Testing Dataset (Target Domain) Accuracy \n",
    "test2_acc = []   # Target Domain (used for Training) Accuracy\n",
    "EPOCH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_source(source_x, source_y, lamda=1.0):\n",
    "    x = source_x\n",
    "    y = source_y\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(x, train=True, source_train=True, lamda=lamda)\n",
    "        \n",
    "        model_loss = get_loss(output, y)\n",
    "        epoch_reg(output, y)\n",
    "        \n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_reg_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step_da(source_x, source_y, target_x=None, target_y=None, lamda=1.0):\n",
    "    cross_domain_x = tf.concat([source_x, target_x], 0)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(cross_domain_x, train=True, source_train=False, lamda=lamda)\n",
    "        l_logits, d_logits = output  # Output from the label predictor and domain predictor\n",
    "\n",
    "        model_loss = get_loss(source_y, l_logits, d_logits, domain_labels)\n",
    "        epoch_cla(d_logits, domain_labels)\n",
    "        #print('d_logits',d_logits.shape)\n",
    "        #print('domain_labels',domain_labels.shape)\n",
    "    gradients_mdan = tape.gradient(model_loss, model.trainable_variables)\n",
    "    model_cla_optimizer.apply_gradients(zip(gradients_mdan, model.trainable_variables))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(target_x, target_y):\n",
    "    x = target_x\n",
    "    y = target_y\n",
    "    \n",
    "    output = model(x, train=False, source_train=True)\n",
    "    epoch_reg(output, y)\n",
    "\n",
    "\n",
    "def train(train_mode, epochs=EPOCH):\n",
    "    \n",
    "    if train_mode == 'source':\n",
    "        dataset = source_dataset\n",
    "        train_func = train_step_source\n",
    "        acc_list = source_acc\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            p = float(epoch) / epochs\n",
    "            lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "            lamda = lamda.astype('float32')\n",
    "\n",
    "            for batch in dataset:\n",
    "                train_func(*batch, lamda=lamda)\n",
    "\n",
    "            print(\"Training: Epoch {} :\\t Source MAE : {:.3}\".format(epoch, epoch_reg.result()), end='  |  ')\n",
    "            acc_list.append(epoch_reg.result())\n",
    "            test(train_mode)\n",
    "            epoch_reg.reset_states()\n",
    "        \n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        dataset = da_dataset\n",
    "        train_func = train_step_da\n",
    "        acc_list = da_acc\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            p = float(epoch) / epochs\n",
    "            lamda = 2 / (1 + np.exp(-100 * p, dtype=np.float32)) - 1\n",
    "            lamda = lamda.astype('float32')\n",
    "\n",
    "            for batch in dataset:\n",
    "                train_func(*batch, lamda=lamda)\n",
    "\n",
    "            print(\"Training: Epoch {} :\\t Source Accuracy : {:.3%}\".format(epoch, epoch_cla.result()), end='  |  ')\n",
    "            acc_list.append(epoch_cla.result())\n",
    "            test(train_mode)\n",
    "            epoch_cla.reset_states()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training Mode\")\n",
    "\n",
    "def test(train_mode):\n",
    "    epoch_reg.reset_states()\n",
    "    epoch_cla.reset_states()\n",
    "    #Testing Dataset (Target Domain)\n",
    "    if train_mode == 'source':\n",
    "        for batch in test_dataset:\n",
    "            test_step(*batch)\n",
    "\n",
    "        print(\"Testing MAE : {:.3}\".format(epoch_reg.result()))\n",
    "        test_acc.append(epoch_reg.result())\n",
    "        epoch_reg.reset_states()\n",
    "    \n",
    "    #Target Domain (used for Training)\n",
    "    elif train_mode == 'domain-adaptation':\n",
    "        for batch in test_dataset2:\n",
    "            test_step(*batch)\n",
    "\n",
    "        print(\"Target Domain Accuracy : {:.3%}\".format(epoch_cla.result()))\n",
    "        test2_acc.append(epoch_cla.result())\n",
    "        epoch_cla.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training\n",
    "# train('source', EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "train('domain-adaptation', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot Results\n",
    "# x_axis = [i for i in range(0, 20)]\n",
    "\n",
    "# plt.plot(x_axis, da_acc[20:], label=\"source accuracy\")\n",
    "# plt.plot(x_axis, test_acc[20:], label=\"testing accuracy\")\n",
    "# plt.plot(x_axis, test2_acc[20:], label=\"target accuracy\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
